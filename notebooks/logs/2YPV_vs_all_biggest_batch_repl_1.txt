Epoch: 1/300 - Train loss: 0.7081496119499207, Validation loss: 0.7023907899856567
Epoch: 2/300 - Train loss: 0.7040188908576965, Validation loss: 0.698383629322052
Epoch: 3/300 - Train loss: 0.7000006437301636, Validation loss: 0.6942943334579468
Epoch: 4/300 - Train loss: 0.6960703730583191, Validation loss: 0.6909834146499634
Epoch: 5/300 - Train loss: 0.6921966075897217, Validation loss: 0.6869720220565796
Epoch: 6/300 - Train loss: 0.6883574724197388, Validation loss: 0.6834352016448975
Epoch: 7/300 - Train loss: 0.6845216751098633, Validation loss: 0.6797788739204407
Epoch: 8/300 - Train loss: 0.6806668639183044, Validation loss: 0.6759914755821228
Epoch: 9/300 - Train loss: 0.6767892837524414, Validation loss: 0.6721882820129395
Epoch: 10/300 - Train loss: 0.6728488206863403, Validation loss: 0.6683072447776794
Epoch: 11/300 - Train loss: 0.6688211560249329, Validation loss: 0.6643106341362
Epoch: 12/300 - Train loss: 0.6646822690963745, Validation loss: 0.6601452827453613
Epoch: 13/300 - Train loss: 0.6604083180427551, Validation loss: 0.6559208035469055
Epoch: 14/300 - Train loss: 0.6559913158416748, Validation loss: 0.6515451073646545
Epoch: 15/300 - Train loss: 0.6514235734939575, Validation loss: 0.6469845771789551
Epoch: 16/300 - Train loss: 0.646706759929657, Validation loss: 0.6422879695892334
Epoch: 17/300 - Train loss: 0.6418489217758179, Validation loss: 0.6374354958534241
Epoch: 18/300 - Train loss: 0.6368681192398071, Validation loss: 0.6325212717056274
Epoch: 19/300 - Train loss: 0.6317867040634155, Validation loss: 0.6274358034133911
Epoch: 20/300 - Train loss: 0.6266270279884338, Validation loss: 0.6223611235618591
Epoch: 21/300 - Train loss: 0.6214141845703125, Validation loss: 0.6172672510147095
Epoch: 22/300 - Train loss: 0.6161676645278931, Validation loss: 0.6120940446853638
Epoch: 23/300 - Train loss: 0.6108977198600769, Validation loss: 0.6070030927658081
Epoch: 24/300 - Train loss: 0.6056169867515564, Validation loss: 0.6019328236579895
Epoch: 25/300 - Train loss: 0.6003305315971375, Validation loss: 0.5965115427970886
Epoch: 26/300 - Train loss: 0.5950448513031006, Validation loss: 0.5915186405181885
Epoch: 27/300 - Train loss: 0.5897685289382935, Validation loss: 0.5862610340118408
Epoch: 28/300 - Train loss: 0.5845125913619995, Validation loss: 0.5810045003890991
Epoch: 29/300 - Train loss: 0.5792796611785889, Validation loss: 0.5761181116104126
Epoch: 30/300 - Train loss: 0.5740758776664734, Validation loss: 0.5707574486732483
Epoch: 31/300 - Train loss: 0.5689069032669067, Validation loss: 0.5658717155456543
Epoch: 32/300 - Train loss: 0.5637796521186829, Validation loss: 0.5605706572532654
Epoch: 33/300 - Train loss: 0.5586984753608704, Validation loss: 0.5557746291160583
Epoch: 34/300 - Train loss: 0.5536658763885498, Validation loss: 0.550737738609314
Epoch: 35/300 - Train loss: 0.5486851334571838, Validation loss: 0.5458346605300903
Epoch: 36/300 - Train loss: 0.5437604188919067, Validation loss: 0.5412608981132507
Epoch: 37/300 - Train loss: 0.5388936400413513, Validation loss: 0.5360810160636902
Epoch: 38/300 - Train loss: 0.5340876579284668, Validation loss: 0.531516969203949
Epoch: 39/300 - Train loss: 0.529343843460083, Validation loss: 0.5267608761787415
Epoch: 40/300 - Train loss: 0.5246637463569641, Validation loss: 0.5223343372344971
Epoch: 41/300 - Train loss: 0.5200490951538086, Validation loss: 0.5173519849777222
Epoch: 42/300 - Train loss: 0.5155001878738403, Validation loss: 0.5128921866416931
Epoch: 43/300 - Train loss: 0.5110186338424683, Validation loss: 0.5085426568984985
Epoch: 44/300 - Train loss: 0.5066052079200745, Validation loss: 0.5041301846504211
Epoch: 45/300 - Train loss: 0.5022604465484619, Validation loss: 0.49979931116104126
Epoch: 46/300 - Train loss: 0.4979853630065918, Validation loss: 0.4957018196582794
Epoch: 47/300 - Train loss: 0.49378064274787903, Validation loss: 0.4914683401584625
Epoch: 48/300 - Train loss: 0.48964664340019226, Validation loss: 0.48730745911598206
Epoch: 49/300 - Train loss: 0.48558393120765686, Validation loss: 0.48353835940361023
Epoch: 50/300 - Train loss: 0.4815917909145355, Validation loss: 0.4794306457042694
Epoch: 51/300 - Train loss: 0.47767025232315063, Validation loss: 0.4754164218902588
Epoch: 52/300 - Train loss: 0.47381991147994995, Validation loss: 0.47138434648513794
Epoch: 53/300 - Train loss: 0.4700409471988678, Validation loss: 0.46829283237457275
Epoch: 54/300 - Train loss: 0.4663325846195221, Validation loss: 0.46460285782814026
Epoch: 55/300 - Train loss: 0.4626947045326233, Validation loss: 0.4605046212673187
Epoch: 56/300 - Train loss: 0.45912662148475647, Validation loss: 0.4569195508956909
Epoch: 57/300 - Train loss: 0.4556284248828888, Validation loss: 0.45377376675605774
Epoch: 58/300 - Train loss: 0.45219969749450684, Validation loss: 0.4504041075706482
Epoch: 59/300 - Train loss: 0.44884029030799866, Validation loss: 0.44715818762779236
Epoch: 60/300 - Train loss: 0.44554877281188965, Validation loss: 0.4436562955379486
Epoch: 61/300 - Train loss: 0.4423251450061798, Validation loss: 0.4403492510318756
Epoch: 62/300 - Train loss: 0.43916845321655273, Validation loss: 0.4373454451560974
Epoch: 63/300 - Train loss: 0.43607795238494873, Validation loss: 0.4340936243534088
Epoch: 64/300 - Train loss: 0.43305283784866333, Validation loss: 0.431506484746933
Epoch: 65/300 - Train loss: 0.4300922751426697, Validation loss: 0.42835524678230286
Epoch: 66/300 - Train loss: 0.427195280790329, Validation loss: 0.4258020520210266
Epoch: 67/300 - Train loss: 0.4243606925010681, Validation loss: 0.42222368717193604
Epoch: 68/300 - Train loss: 0.4215869605541229, Validation loss: 0.4202827513217926
Epoch: 69/300 - Train loss: 0.4188728332519531, Validation loss: 0.41709089279174805
Epoch: 70/300 - Train loss: 0.41621777415275574, Validation loss: 0.4143988788127899
Epoch: 71/300 - Train loss: 0.4136204123497009, Validation loss: 0.41164523363113403
Epoch: 72/300 - Train loss: 0.41107892990112305, Validation loss: 0.40937933325767517
Epoch: 73/300 - Train loss: 0.408592164516449, Validation loss: 0.40676650404930115
Epoch: 74/300 - Train loss: 0.4061589539051056, Validation loss: 0.404293417930603
Epoch: 75/300 - Train loss: 0.40377792716026306, Validation loss: 0.4016285538673401
Epoch: 76/300 - Train loss: 0.40144819021224976, Validation loss: 0.39937058091163635
Epoch: 77/300 - Train loss: 0.3991680145263672, Validation loss: 0.3974969983100891
Epoch: 78/300 - Train loss: 0.39693593978881836, Validation loss: 0.3952212929725647
Epoch: 79/300 - Train loss: 0.39475107192993164, Validation loss: 0.393378883600235
Epoch: 80/300 - Train loss: 0.3926115036010742, Validation loss: 0.3911064565181732
Epoch: 81/300 - Train loss: 0.3905164301395416, Validation loss: 0.3889762759208679
Epoch: 82/300 - Train loss: 0.3884645104408264, Validation loss: 0.38631242513656616
Epoch: 83/300 - Train loss: 0.3864547908306122, Validation loss: 0.3848191499710083
Epoch: 84/300 - Train loss: 0.3844853937625885, Validation loss: 0.38220083713531494
Epoch: 85/300 - Train loss: 0.382554292678833, Validation loss: 0.3807072937488556
Epoch: 86/300 - Train loss: 0.38066160678863525, Validation loss: 0.37850016355514526
Epoch: 87/300 - Train loss: 0.3788069486618042, Validation loss: 0.3766608238220215
Epoch: 88/300 - Train loss: 0.3769885003566742, Validation loss: 0.3748391568660736
Epoch: 89/300 - Train loss: 0.3752050995826721, Validation loss: 0.3735259473323822
Epoch: 90/300 - Train loss: 0.3734559416770935, Validation loss: 0.37172582745552063
Epoch: 91/300 - Train loss: 0.3717406988143921, Validation loss: 0.37008771300315857
Epoch: 92/300 - Train loss: 0.3700575828552246, Validation loss: 0.36808261275291443
Epoch: 93/300 - Train loss: 0.36840495467185974, Validation loss: 0.36688292026519775
Epoch: 94/300 - Train loss: 0.3667829632759094, Validation loss: 0.3651019334793091
Epoch: 95/300 - Train loss: 0.36519092321395874, Validation loss: 0.36343449354171753
Epoch: 96/300 - Train loss: 0.36362800002098083, Validation loss: 0.36150309443473816
Epoch: 97/300 - Train loss: 0.36209312081336975, Validation loss: 0.36000218987464905
Epoch: 98/300 - Train loss: 0.3605857491493225, Validation loss: 0.3586301803588867
Epoch: 99/300 - Train loss: 0.35910478234291077, Validation loss: 0.35688960552215576
Epoch: 100/300 - Train loss: 0.357649028301239, Validation loss: 0.35588058829307556
Epoch: 101/300 - Train loss: 0.3562186658382416, Validation loss: 0.3543972373008728
Epoch: 102/300 - Train loss: 0.3548129200935364, Validation loss: 0.35294127464294434
Epoch: 103/300 - Train loss: 0.35343077778816223, Validation loss: 0.3512297570705414
Epoch: 104/300 - Train loss: 0.35207220911979675, Validation loss: 0.35031449794769287
Epoch: 105/300 - Train loss: 0.35073718428611755, Validation loss: 0.3489042818546295
Epoch: 106/300 - Train loss: 0.3494245111942291, Validation loss: 0.3472065329551697
Epoch: 107/300 - Train loss: 0.34813371300697327, Validation loss: 0.3459126353263855
Epoch: 108/300 - Train loss: 0.3468630611896515, Validation loss: 0.34481608867645264
Epoch: 109/300 - Train loss: 0.34561365842819214, Validation loss: 0.34350723028182983
Epoch: 110/300 - Train loss: 0.3443845510482788, Validation loss: 0.3422868847846985
Epoch: 111/300 - Train loss: 0.3431747257709503, Validation loss: 0.34185677766799927
Epoch: 112/300 - Train loss: 0.34198370575904846, Validation loss: 0.33972468972206116
Epoch: 113/300 - Train loss: 0.3408108055591583, Validation loss: 0.33854156732559204
Epoch: 114/300 - Train loss: 0.3396557569503784, Validation loss: 0.33814436197280884
Epoch: 115/300 - Train loss: 0.33851736783981323, Validation loss: 0.3368874490261078
Epoch: 116/300 - Train loss: 0.3373953700065613, Validation loss: 0.33555421233177185
Epoch: 117/300 - Train loss: 0.33629047870635986, Validation loss: 0.3346954882144928
Epoch: 118/300 - Train loss: 0.3352019190788269, Validation loss: 0.3334704041481018
Epoch: 119/300 - Train loss: 0.33412832021713257, Validation loss: 0.3320958614349365
Epoch: 120/300 - Train loss: 0.33306974172592163, Validation loss: 0.3309387266635895
Epoch: 121/300 - Train loss: 0.3320276439189911, Validation loss: 0.3299753665924072
Epoch: 122/300 - Train loss: 0.33100011944770813, Validation loss: 0.32880833745002747
Epoch: 123/300 - Train loss: 0.32998812198638916, Validation loss: 0.32828301191329956
Epoch: 124/300 - Train loss: 0.32899051904678345, Validation loss: 0.32712727785110474
Epoch: 125/300 - Train loss: 0.3280070722103119, Validation loss: 0.32631856203079224
Epoch: 126/300 - Train loss: 0.3270370364189148, Validation loss: 0.32516592741012573
Epoch: 127/300 - Train loss: 0.3260807693004608, Validation loss: 0.3241540789604187
Epoch: 128/300 - Train loss: 0.3251381814479828, Validation loss: 0.32345399260520935
Epoch: 129/300 - Train loss: 0.32420921325683594, Validation loss: 0.322361022233963
Epoch: 130/300 - Train loss: 0.3232921361923218, Validation loss: 0.3215022385120392
Epoch: 131/300 - Train loss: 0.3223872780799866, Validation loss: 0.32069262862205505
Epoch: 132/300 - Train loss: 0.3214937448501587, Validation loss: 0.3200853168964386
Epoch: 133/300 - Train loss: 0.32061243057250977, Validation loss: 0.31866854429244995
Epoch: 134/300 - Train loss: 0.31974321603775024, Validation loss: 0.3185320496559143
Epoch: 135/300 - Train loss: 0.3188847005367279, Validation loss: 0.31742945313453674
Epoch: 136/300 - Train loss: 0.3180367946624756, Validation loss: 0.31598958373069763
Epoch: 137/300 - Train loss: 0.3171994686126709, Validation loss: 0.3155653476715088
Epoch: 138/300 - Train loss: 0.31637269258499146, Validation loss: 0.3145609498023987
Epoch: 139/300 - Train loss: 0.3155558407306671, Validation loss: 0.3147420585155487
Epoch: 140/300 - Train loss: 0.3147500157356262, Validation loss: 0.3127434551715851
Epoch: 141/300 - Train loss: 0.31395381689071655, Validation loss: 0.3125501573085785
Epoch: 142/300 - Train loss: 0.3131667673587799, Validation loss: 0.31202414631843567
Epoch: 143/300 - Train loss: 0.3123883306980133, Validation loss: 0.31150567531585693
Epoch: 144/300 - Train loss: 0.3116194009780884, Validation loss: 0.31059086322784424
Epoch: 145/300 - Train loss: 0.3108593821525574, Validation loss: 0.3093673884868622
Epoch: 146/300 - Train loss: 0.3101085126399994, Validation loss: 0.3091815710067749
Epoch: 147/300 - Train loss: 0.3093658983707428, Validation loss: 0.3085433840751648
Epoch: 148/300 - Train loss: 0.3086312711238861, Validation loss: 0.30736398696899414
Epoch: 149/300 - Train loss: 0.30790549516677856, Validation loss: 0.30668139457702637
Epoch: 150/300 - Train loss: 0.3071889579296112, Validation loss: 0.305645227432251
Epoch: 151/300 - Train loss: 0.3064812421798706, Validation loss: 0.3062759339809418
Epoch: 152/300 - Train loss: 0.30578145384788513, Validation loss: 0.30468007922172546
Epoch: 153/300 - Train loss: 0.3050899803638458, Validation loss: 0.3035777509212494
Epoch: 154/300 - Train loss: 0.30440565943717957, Validation loss: 0.30323129892349243
Epoch: 155/300 - Train loss: 0.30372941493988037, Validation loss: 0.3024821877479553
Epoch: 156/300 - Train loss: 0.30306094884872437, Validation loss: 0.30231207609176636
Epoch: 157/300 - Train loss: 0.30239954590797424, Validation loss: 0.300919771194458
Epoch: 158/300 - Train loss: 0.3017440140247345, Validation loss: 0.3005017638206482
Epoch: 159/300 - Train loss: 0.30109548568725586, Validation loss: 0.3001832962036133
Epoch: 160/300 - Train loss: 0.3004535734653473, Validation loss: 0.2991628646850586
Epoch: 161/300 - Train loss: 0.2998177111148834, Validation loss: 0.298870712518692
Epoch: 162/300 - Train loss: 0.2991885542869568, Validation loss: 0.29835110902786255
Epoch: 163/300 - Train loss: 0.29856637120246887, Validation loss: 0.2976614534854889
Epoch: 164/300 - Train loss: 0.29795077443122864, Validation loss: 0.297290176153183
Epoch: 165/300 - Train loss: 0.29734209179878235, Validation loss: 0.2968149781227112
Epoch: 166/300 - Train loss: 0.2967394292354584, Validation loss: 0.29568278789520264
Epoch: 167/300 - Train loss: 0.2961427867412567, Validation loss: 0.2952776253223419
Epoch: 168/300 - Train loss: 0.2955528497695923, Validation loss: 0.29506683349609375
Epoch: 169/300 - Train loss: 0.2949686348438263, Validation loss: 0.29459261894226074
