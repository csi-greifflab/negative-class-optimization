Epoch: 1/200 - Train loss: 0.6936402320861816, Validation loss: 0.6925007104873657
Epoch: 2/200 - Train loss: 0.6873233318328857, Validation loss: 0.6819776892662048
Epoch: 3/200 - Train loss: 0.6739884614944458, Validation loss: 0.6672558784484863
Epoch: 4/200 - Train loss: 0.6581076979637146, Validation loss: 0.6507694125175476
Epoch: 5/200 - Train loss: 0.6401400566101074, Validation loss: 0.6323668956756592
Epoch: 6/200 - Train loss: 0.6207202076911926, Validation loss: 0.613030195236206
Epoch: 7/200 - Train loss: 0.6007781028747559, Validation loss: 0.5940021872520447
Epoch: 8/200 - Train loss: 0.5813608765602112, Validation loss: 0.5758423209190369
Epoch: 9/200 - Train loss: 0.5624998807907104, Validation loss: 0.5579549074172974
Epoch: 10/200 - Train loss: 0.5444541573524475, Validation loss: 0.5416537523269653
Epoch: 11/200 - Train loss: 0.5290769338607788, Validation loss: 0.5288316011428833
Epoch: 12/200 - Train loss: 0.5167951583862305, Validation loss: 0.5196275115013123
Epoch: 13/200 - Train loss: 0.5066653490066528, Validation loss: 0.5109588503837585
Epoch: 14/200 - Train loss: 0.4985175132751465, Validation loss: 0.5039467811584473
Epoch: 15/200 - Train loss: 0.49197033047676086, Validation loss: 0.49795854091644287
Epoch: 16/200 - Train loss: 0.4863432049751282, Validation loss: 0.49299633502960205
Epoch: 17/200 - Train loss: 0.48103177547454834, Validation loss: 0.48887014389038086
Epoch: 18/200 - Train loss: 0.4770006239414215, Validation loss: 0.48431915044784546
Epoch: 19/200 - Train loss: 0.4729527235031128, Validation loss: 0.4807969927787781
Epoch: 20/200 - Train loss: 0.46874621510505676, Validation loss: 0.4779996871948242
Epoch: 21/200 - Train loss: 0.4655367136001587, Validation loss: 0.4751914143562317
Epoch: 22/200 - Train loss: 0.46191900968551636, Validation loss: 0.4718415141105652
Epoch: 23/200 - Train loss: 0.45948535203933716, Validation loss: 0.4694342017173767
Epoch: 24/200 - Train loss: 0.4567094147205353, Validation loss: 0.46703630685806274
Epoch: 25/200 - Train loss: 0.45326587557792664, Validation loss: 0.4637635350227356
Epoch: 26/200 - Train loss: 0.45033520460128784, Validation loss: 0.46181949973106384
Epoch: 27/200 - Train loss: 0.4476175904273987, Validation loss: 0.45896413922309875
Epoch: 28/200 - Train loss: 0.44486069679260254, Validation loss: 0.45709121227264404
Epoch: 29/200 - Train loss: 0.4426857531070709, Validation loss: 0.4540479779243469
Epoch: 30/200 - Train loss: 0.44001829624176025, Validation loss: 0.45316046476364136
Epoch: 31/200 - Train loss: 0.43798568844795227, Validation loss: 0.45024052262306213
Epoch: 32/200 - Train loss: 0.43518200516700745, Validation loss: 0.4481410086154938
Epoch: 33/200 - Train loss: 0.43283146619796753, Validation loss: 0.4457677900791168
Epoch: 34/200 - Train loss: 0.43088990449905396, Validation loss: 0.4444662630558014
Epoch: 35/200 - Train loss: 0.42881473898887634, Validation loss: 0.44227275252342224
Epoch: 36/200 - Train loss: 0.4263356029987335, Validation loss: 0.44008389115333557
Epoch: 37/200 - Train loss: 0.4248557984828949, Validation loss: 0.4393931031227112
Epoch: 38/200 - Train loss: 0.42239484190940857, Validation loss: 0.4380935728549957
Epoch: 39/200 - Train loss: 0.4204898774623871, Validation loss: 0.43514516949653625
Epoch: 40/200 - Train loss: 0.4185464680194855, Validation loss: 0.4341426491737366
Epoch: 41/200 - Train loss: 0.4165741503238678, Validation loss: 0.43251046538352966
Epoch: 42/200 - Train loss: 0.4148658812046051, Validation loss: 0.43082860112190247
Epoch: 43/200 - Train loss: 0.41330528259277344, Validation loss: 0.42917779088020325
Epoch: 44/200 - Train loss: 0.41189098358154297, Validation loss: 0.428343266248703
Epoch: 45/200 - Train loss: 0.4097311496734619, Validation loss: 0.42677825689315796
Epoch: 46/200 - Train loss: 0.4083646237850189, Validation loss: 0.4247099757194519
Epoch: 47/200 - Train loss: 0.4067048132419586, Validation loss: 0.42328202724456787
Epoch: 48/200 - Train loss: 0.4051050841808319, Validation loss: 0.4228742718696594
Epoch: 49/200 - Train loss: 0.40361955761909485, Validation loss: 0.4217371642589569
Epoch: 50/200 - Train loss: 0.401677668094635, Validation loss: 0.4193607568740845
Epoch: 51/200 - Train loss: 0.40035924315452576, Validation loss: 0.41863930225372314
Epoch: 52/200 - Train loss: 0.39854639768600464, Validation loss: 0.4173200726509094
Epoch: 53/200 - Train loss: 0.39719539880752563, Validation loss: 0.41542941331863403
Epoch: 54/200 - Train loss: 0.3956061899662018, Validation loss: 0.4140346944332123
Epoch: 55/200 - Train loss: 0.394119530916214, Validation loss: 0.413818359375
Epoch: 56/200 - Train loss: 0.3932228684425354, Validation loss: 0.41224315762519836
Epoch: 57/200 - Train loss: 0.3911040723323822, Validation loss: 0.4112195670604706
Epoch: 58/200 - Train loss: 0.38966360688209534, Validation loss: 0.4092763066291809
Epoch: 59/200 - Train loss: 0.38843539357185364, Validation loss: 0.40816178917884827
Epoch: 60/200 - Train loss: 0.387119859457016, Validation loss: 0.40759438276290894
Epoch: 61/200 - Train loss: 0.38500460982322693, Validation loss: 0.4058210551738739
Epoch: 62/200 - Train loss: 0.38337618112564087, Validation loss: 0.4051460027694702
Epoch: 63/200 - Train loss: 0.38207560777664185, Validation loss: 0.40353864431381226
Epoch: 64/200 - Train loss: 0.38038673996925354, Validation loss: 0.403093546628952
Epoch: 65/200 - Train loss: 0.3787273168563843, Validation loss: 0.40107816457748413
Epoch: 66/200 - Train loss: 0.37733161449432373, Validation loss: 0.3996686339378357
Epoch: 67/200 - Train loss: 0.3761391341686249, Validation loss: 0.39853987097740173
Epoch: 68/200 - Train loss: 0.3742416799068451, Validation loss: 0.39730414748191833
Epoch: 69/200 - Train loss: 0.37305179238319397, Validation loss: 0.39580416679382324
Epoch: 70/200 - Train loss: 0.371551513671875, Validation loss: 0.3954593539237976
Epoch: 71/200 - Train loss: 0.3701607584953308, Validation loss: 0.3943532407283783
Epoch: 72/200 - Train loss: 0.36872830986976624, Validation loss: 0.39380091428756714
Epoch: 73/200 - Train loss: 0.3679299056529999, Validation loss: 0.3917939364910126
Epoch: 74/200 - Train loss: 0.36585843563079834, Validation loss: 0.390445739030838
Epoch: 75/200 - Train loss: 0.3643309473991394, Validation loss: 0.3894089162349701
Epoch: 76/200 - Train loss: 0.3628975749015808, Validation loss: 0.38815686106681824
Epoch: 77/200 - Train loss: 0.3617582619190216, Validation loss: 0.3869858980178833
Epoch: 78/200 - Train loss: 0.36077558994293213, Validation loss: 0.3855797350406647
Epoch: 79/200 - Train loss: 0.3592097759246826, Validation loss: 0.38477185368537903
Epoch: 80/200 - Train loss: 0.3581676185131073, Validation loss: 0.3841959238052368
Epoch: 81/200 - Train loss: 0.3566682040691376, Validation loss: 0.3836221694946289
Epoch: 82/200 - Train loss: 0.35535117983818054, Validation loss: 0.38220101594924927
Epoch: 83/200 - Train loss: 0.3544124960899353, Validation loss: 0.3814152181148529
Epoch: 84/200 - Train loss: 0.35301685333251953, Validation loss: 0.37993139028549194
Epoch: 85/200 - Train loss: 0.3515089452266693, Validation loss: 0.37970423698425293
Epoch: 86/200 - Train loss: 0.35008424520492554, Validation loss: 0.37836191058158875
Epoch: 87/200 - Train loss: 0.34912005066871643, Validation loss: 0.3774872124195099
Epoch: 88/200 - Train loss: 0.34780266880989075, Validation loss: 0.3759547770023346
Epoch: 89/200 - Train loss: 0.3465941250324249, Validation loss: 0.3763430416584015
Epoch: 90/200 - Train loss: 0.34562334418296814, Validation loss: 0.37520283460617065
Epoch: 91/200 - Train loss: 0.3443811237812042, Validation loss: 0.3742120862007141
Epoch: 92/200 - Train loss: 0.342977911233902, Validation loss: 0.3733963370323181
Epoch: 93/200 - Train loss: 0.3419773578643799, Validation loss: 0.37282413244247437
Epoch: 94/200 - Train loss: 0.3412698805332184, Validation loss: 0.37132173776626587
Epoch: 95/200 - Train loss: 0.3403671681880951, Validation loss: 0.3712487816810608
Epoch: 96/200 - Train loss: 0.33906447887420654, Validation loss: 0.37050193548202515
Epoch: 97/200 - Train loss: 0.33818671107292175, Validation loss: 0.3699507415294647
Epoch: 98/200 - Train loss: 0.33702412247657776, Validation loss: 0.36885708570480347
Epoch: 99/200 - Train loss: 0.33598893880844116, Validation loss: 0.3679198920726776
Epoch: 100/200 - Train loss: 0.33489757776260376, Validation loss: 0.36766499280929565
Epoch: 101/200 - Train loss: 0.3348572850227356, Validation loss: 0.3665437400341034
Epoch: 102/200 - Train loss: 0.33309146761894226, Validation loss: 0.36592578887939453
Epoch: 103/200 - Train loss: 0.33249321579933167, Validation loss: 0.36501380801200867
Epoch: 104/200 - Train loss: 0.3314971327781677, Validation loss: 0.3648749887943268
Epoch: 105/200 - Train loss: 0.3308514952659607, Validation loss: 0.3635425269603729
Epoch: 106/200 - Train loss: 0.33004575967788696, Validation loss: 0.36361926794052124
Epoch: 107/200 - Train loss: 0.32879960536956787, Validation loss: 0.36191174387931824
Epoch: 108/200 - Train loss: 0.3281978964805603, Validation loss: 0.36230725049972534
Epoch: 109/200 - Train loss: 0.3270111680030823, Validation loss: 0.36084532737731934
Epoch: 110/200 - Train loss: 0.3266758620738983, Validation loss: 0.3601236343383789
Epoch: 111/200 - Train loss: 0.3255484998226166, Validation loss: 0.36084213852882385
Epoch: 112/200 - Train loss: 0.32465773820877075, Validation loss: 0.359884113073349
Epoch: 113/200 - Train loss: 0.3242128789424896, Validation loss: 0.3604101240634918
