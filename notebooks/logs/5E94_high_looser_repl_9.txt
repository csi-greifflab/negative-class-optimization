Epoch: 1/200 - Train loss: 0.5820083618164062, Validation loss: 0.49586740136146545
Epoch: 2/200 - Train loss: 0.4294268786907196, Validation loss: 0.4062168598175049
Epoch: 3/200 - Train loss: 0.3650253415107727, Validation loss: 0.36271801590919495
Epoch: 4/200 - Train loss: 0.32677170634269714, Validation loss: 0.32790642976760864
Epoch: 5/200 - Train loss: 0.29505637288093567, Validation loss: 0.30130091309547424
Epoch: 6/200 - Train loss: 0.27134421467781067, Validation loss: 0.279349148273468
Epoch: 7/200 - Train loss: 0.2521791160106659, Validation loss: 0.26552456617355347
Epoch: 8/200 - Train loss: 0.2384309321641922, Validation loss: 0.2558009624481201
Epoch: 9/200 - Train loss: 0.22710898518562317, Validation loss: 0.2504923343658447
Epoch: 10/200 - Train loss: 0.2179962694644928, Validation loss: 0.24224574863910675
Epoch: 11/200 - Train loss: 0.21053417026996613, Validation loss: 0.23832392692565918
Epoch: 12/200 - Train loss: 0.2037641704082489, Validation loss: 0.2353009730577469
Epoch: 13/200 - Train loss: 0.19761264324188232, Validation loss: 0.23282021284103394
Epoch: 14/200 - Train loss: 0.19289875030517578, Validation loss: 0.22928370535373688
Epoch: 15/200 - Train loss: 0.18863648176193237, Validation loss: 0.22761176526546478
Epoch: 16/200 - Train loss: 0.18398912250995636, Validation loss: 0.22605256736278534
Epoch: 17/200 - Train loss: 0.18064111471176147, Validation loss: 0.22477379441261292
Epoch: 18/200 - Train loss: 0.17821289598941803, Validation loss: 0.23010528087615967
Epoch: 19/200 - Train loss: 0.174736887216568, Validation loss: 0.22540706396102905
Epoch: 20/200 - Train loss: 0.17251871526241302, Validation loss: 0.22505304217338562
Epoch: 21/200 - Train loss: 0.17001834511756897, Validation loss: 0.22151607275009155
Epoch: 22/200 - Train loss: 0.16779206693172455, Validation loss: 0.22397322952747345
Epoch: 23/200 - Train loss: 0.16545890271663666, Validation loss: 0.22220583260059357
Epoch: 24/200 - Train loss: 0.16367237269878387, Validation loss: 0.22758765518665314
Epoch: 25/200 - Train loss: 0.16251516342163086, Validation loss: 0.2203758805990219
Epoch: 26/200 - Train loss: 0.16004471480846405, Validation loss: 0.22546271979808807
Epoch: 27/200 - Train loss: 0.1595568209886551, Validation loss: 0.22360001504421234
Epoch: 28/200 - Train loss: 0.1573944389820099, Validation loss: 0.22205202281475067
Epoch: 29/200 - Train loss: 0.15649479627609253, Validation loss: 0.22238099575042725
Epoch: 30/200 - Train loss: 0.15433509647846222, Validation loss: 0.22232520580291748
Epoch: 31/200 - Train loss: 0.15338265895843506, Validation loss: 0.22389309108257294
Epoch: 32/200 - Train loss: 0.15201638638973236, Validation loss: 0.22481100261211395
Epoch: 33/200 - Train loss: 0.1514178216457367, Validation loss: 0.22162780165672302
Epoch: 34/200 - Train loss: 0.1504238396883011, Validation loss: 0.22560709714889526
Epoch: 35/200 - Train loss: 0.1495422124862671, Validation loss: 0.22776658833026886
Epoch: 36/200 - Train loss: 0.14823052287101746, Validation loss: 0.22608470916748047
Epoch: 37/200 - Train loss: 0.14774592220783234, Validation loss: 0.22561204433441162
Epoch: 38/200 - Train loss: 0.14692869782447815, Validation loss: 0.22530622780323029
Epoch: 39/200 - Train loss: 0.14598530530929565, Validation loss: 0.2274717390537262
Epoch: 40/200 - Train loss: 0.14489209651947021, Validation loss: 0.227128803730011
Epoch: 41/200 - Train loss: 0.14433713257312775, Validation loss: 0.23102723062038422
Epoch: 42/200 - Train loss: 0.14331498742103577, Validation loss: 0.22879838943481445
Epoch: 43/200 - Train loss: 0.1433042287826538, Validation loss: 0.23002766072750092
Epoch: 44/200 - Train loss: 0.1424814611673355, Validation loss: 0.2294272631406784
Epoch: 45/200 - Train loss: 0.14186306297779083, Validation loss: 0.23215016722679138
Epoch: 46/200 - Train loss: 0.1416236311197281, Validation loss: 0.22890149056911469
Epoch: 47/200 - Train loss: 0.14043322205543518, Validation loss: 0.23208114504814148
Epoch: 48/200 - Train loss: 0.14003244042396545, Validation loss: 0.23064680397510529
Epoch: 49/200 - Train loss: 0.13981275260448456, Validation loss: 0.22776567935943604
Epoch: 50/200 - Train loss: 0.13908934593200684, Validation loss: 0.23625096678733826
Epoch: 51/200 - Train loss: 0.13901720941066742, Validation loss: 0.23267106711864471
Epoch: 52/200 - Train loss: 0.13821421563625336, Validation loss: 0.23540571331977844
Epoch: 53/200 - Train loss: 0.13782310485839844, Validation loss: 0.23420754075050354
Epoch: 54/200 - Train loss: 0.13739241659641266, Validation loss: 0.2333863526582718
Epoch: 55/200 - Train loss: 0.13690324127674103, Validation loss: 0.22944967448711395
Epoch: 56/200 - Train loss: 0.13552215695381165, Validation loss: 0.23207946121692657
Epoch: 57/200 - Train loss: 0.1357300877571106, Validation loss: 0.2344416230916977
Epoch: 58/200 - Train loss: 0.13509748876094818, Validation loss: 0.23523938655853271
Epoch: 59/200 - Train loss: 0.13478270173072815, Validation loss: 0.23341631889343262
Epoch: 60/200 - Train loss: 0.13454829156398773, Validation loss: 0.23635637760162354
Epoch: 61/200 - Train loss: 0.13379597663879395, Validation loss: 0.23553867638111115
Epoch: 62/200 - Train loss: 0.13438300788402557, Validation loss: 0.23963789641857147
Epoch: 63/200 - Train loss: 0.13367070257663727, Validation loss: 0.23282064497470856
Epoch: 64/200 - Train loss: 0.13304832577705383, Validation loss: 0.23607555031776428
Epoch: 65/200 - Train loss: 0.13209328055381775, Validation loss: 0.23860453069210052
Epoch: 66/200 - Train loss: 0.13298314809799194, Validation loss: 0.23574118316173553
Epoch: 67/200 - Train loss: 0.13168580830097198, Validation loss: 0.2364201694726944
Epoch: 68/200 - Train loss: 0.13143296539783478, Validation loss: 0.2376980483531952
Epoch: 69/200 - Train loss: 0.13079187273979187, Validation loss: 0.23822875320911407
Epoch: 70/200 - Train loss: 0.13057446479797363, Validation loss: 0.2371702939271927
Epoch: 71/200 - Train loss: 0.13100214302539825, Validation loss: 0.2365182787179947
Epoch: 72/200 - Train loss: 0.13062699139118195, Validation loss: 0.2402513176202774
Epoch: 73/200 - Train loss: 0.12993302941322327, Validation loss: 0.24161891639232635
Epoch: 74/200 - Train loss: 0.12929950654506683, Validation loss: 0.23761484026908875
Epoch: 75/200 - Train loss: 0.12871134281158447, Validation loss: 0.24066346883773804
Epoch: 76/200 - Train loss: 0.12839560210704803, Validation loss: 0.24160951375961304
Epoch: 77/200 - Train loss: 0.12828245759010315, Validation loss: 0.2422821819782257
Epoch: 78/200 - Train loss: 0.12800921499729156, Validation loss: 0.24281153082847595
Epoch: 79/200 - Train loss: 0.12830513715744019, Validation loss: 0.24041470885276794
Epoch: 80/200 - Train loss: 0.12765741348266602, Validation loss: 0.2391909658908844
Epoch: 81/200 - Train loss: 0.12636926770210266, Validation loss: 0.24246710538864136
Epoch: 82/200 - Train loss: 0.1273634135723114, Validation loss: 0.2425008863210678
Epoch: 83/200 - Train loss: 0.12678855657577515, Validation loss: 0.2458806186914444
Epoch: 84/200 - Train loss: 0.12646332383155823, Validation loss: 0.24321617186069489
Epoch: 85/200 - Train loss: 0.12535782158374786, Validation loss: 0.24430565536022186
Epoch: 86/200 - Train loss: 0.12602156400680542, Validation loss: 0.24438415467739105
Epoch: 87/200 - Train loss: 0.1253637671470642, Validation loss: 0.24437041580677032
Epoch: 88/200 - Train loss: 0.12563207745552063, Validation loss: 0.24158300459384918
Epoch: 89/200 - Train loss: 0.12452422827482224, Validation loss: 0.24787814915180206
Epoch: 90/200 - Train loss: 0.12522737681865692, Validation loss: 0.24365797638893127
Epoch: 91/200 - Train loss: 0.12458883225917816, Validation loss: 0.24536466598510742
Epoch: 92/200 - Train loss: 0.12424993515014648, Validation loss: 0.24632973968982697
Epoch: 93/200 - Train loss: 0.12396825850009918, Validation loss: 0.24693724513053894
Epoch: 94/200 - Train loss: 0.12363146990537643, Validation loss: 0.24643652141094208
Epoch: 95/200 - Train loss: 0.1231919601559639, Validation loss: 0.2483140081167221
Epoch: 96/200 - Train loss: 0.12293433398008347, Validation loss: 0.2593355178833008
Epoch: 97/200 - Train loss: 0.12383400648832321, Validation loss: 0.24778608977794647
Epoch: 98/200 - Train loss: 0.12284831702709198, Validation loss: 0.2525160014629364
Epoch: 99/200 - Train loss: 0.12287131696939468, Validation loss: 0.2532464563846588
Epoch: 100/200 - Train loss: 0.12216059118509293, Validation loss: 0.24924325942993164
Epoch: 101/200 - Train loss: 0.12230546772480011, Validation loss: 0.2523580491542816
Epoch: 102/200 - Train loss: 0.12204589694738388, Validation loss: 0.2515197694301605
Epoch: 103/200 - Train loss: 0.12190433591604233, Validation loss: 0.25618329644203186
Epoch: 104/200 - Train loss: 0.12125660479068756, Validation loss: 0.24988047778606415
Epoch: 105/200 - Train loss: 0.12081712484359741, Validation loss: 0.251351535320282
Epoch: 106/200 - Train loss: 0.1216803640127182, Validation loss: 0.25366857647895813
Epoch: 107/200 - Train loss: 0.121429443359375, Validation loss: 0.253269761800766
Epoch: 108/200 - Train loss: 0.12113972008228302, Validation loss: 0.25363561511039734
Epoch: 109/200 - Train loss: 0.12044433504343033, Validation loss: 0.25354284048080444
Epoch: 110/200 - Train loss: 0.12072543054819107, Validation loss: 0.25372326374053955
Epoch: 111/200 - Train loss: 0.1201637014746666, Validation loss: 0.2561960220336914
Epoch: 112/200 - Train loss: 0.12054017186164856, Validation loss: 0.2532980740070343
Epoch: 113/200 - Train loss: 0.1203417032957077, Validation loss: 0.2621954083442688
Epoch: 114/200 - Train loss: 0.11985071003437042, Validation loss: 0.2601880729198456
Epoch: 115/200 - Train loss: 0.11920768022537231, Validation loss: 0.25773122906684875
Epoch: 116/200 - Train loss: 0.1203678771853447, Validation loss: 0.2582536041736603
Epoch: 117/200 - Train loss: 0.11900587379932404, Validation loss: 0.2557459771633148
Epoch: 118/200 - Train loss: 0.11973315477371216, Validation loss: 0.2563314139842987
Epoch: 119/200 - Train loss: 0.1197584792971611, Validation loss: 0.2592357099056244
Epoch: 120/200 - Train loss: 0.11942366510629654, Validation loss: 0.26625171303749084
Epoch: 121/200 - Train loss: 0.11839030683040619, Validation loss: 0.26053526997566223
Epoch: 122/200 - Train loss: 0.11844518780708313, Validation loss: 0.25883692502975464
Epoch: 123/200 - Train loss: 0.11821363866329193, Validation loss: 0.2624330520629883
Epoch: 124/200 - Train loss: 0.11809878051280975, Validation loss: 0.2627306282520294
Epoch: 125/200 - Train loss: 0.11861838400363922, Validation loss: 0.26201218366622925
Epoch: 126/200 - Train loss: 0.11760396510362625, Validation loss: 0.2601596713066101
Epoch: 127/200 - Train loss: 0.11771790683269501, Validation loss: 0.25937530398368835
Epoch: 128/200 - Train loss: 0.1172344833612442, Validation loss: 0.26093196868896484
Epoch: 129/200 - Train loss: 0.11694960296154022, Validation loss: 0.2651512622833252
Epoch: 130/200 - Train loss: 0.11717375367879868, Validation loss: 0.2639463543891907
Epoch: 131/200 - Train loss: 0.11783353984355927, Validation loss: 0.2628713846206665
Epoch: 132/200 - Train loss: 0.11662106961011887, Validation loss: 0.26261794567108154
Epoch: 133/200 - Train loss: 0.11675549298524857, Validation loss: 0.2634223401546478
Epoch: 134/200 - Train loss: 0.1162722259759903, Validation loss: 0.26905205845832825
Epoch: 135/200 - Train loss: 0.11682095378637314, Validation loss: 0.26628822088241577
Epoch: 136/200 - Train loss: 0.11719207465648651, Validation loss: 0.2652137279510498
Epoch: 137/200 - Train loss: 0.1160515546798706, Validation loss: 0.27120545506477356
Epoch: 138/200 - Train loss: 0.11707988381385803, Validation loss: 0.2684484124183655
Epoch: 139/200 - Train loss: 0.11559800803661346, Validation loss: 0.2673134207725525
Epoch: 140/200 - Train loss: 0.1160520538687706, Validation loss: 0.267691969871521
Epoch: 141/200 - Train loss: 0.1169145479798317, Validation loss: 0.2684304118156433
Epoch: 142/200 - Train loss: 0.11538522690534592, Validation loss: 0.2673603594303131
Epoch: 143/200 - Train loss: 0.11678602546453476, Validation loss: 0.2683534324169159
Epoch: 144/200 - Train loss: 0.11516851931810379, Validation loss: 0.26841607689857483
Epoch: 145/200 - Train loss: 0.11496295034885406, Validation loss: 0.27105697989463806
Epoch: 146/200 - Train loss: 0.11593471467494965, Validation loss: 0.2746316194534302
Epoch: 147/200 - Train loss: 0.11550214141607285, Validation loss: 0.2702547013759613
Epoch: 148/200 - Train loss: 0.1143869012594223, Validation loss: 0.2704262137413025
Epoch: 149/200 - Train loss: 0.11501622200012207, Validation loss: 0.271348237991333
Epoch: 150/200 - Train loss: 0.11480211466550827, Validation loss: 0.2719377279281616
Epoch: 151/200 - Train loss: 0.11562814563512802, Validation loss: 0.26960787177085876
Epoch: 152/200 - Train loss: 0.11433319747447968, Validation loss: 0.2754650413990021
Epoch: 153/200 - Train loss: 0.1148035004734993, Validation loss: 0.27175024151802063
Epoch: 154/200 - Train loss: 0.1138358861207962, Validation loss: 0.2761244475841522
Epoch: 155/200 - Train loss: 0.11420915275812149, Validation loss: 0.2749796211719513
Epoch: 156/200 - Train loss: 0.11382656544446945, Validation loss: 0.27893197536468506
Epoch: 157/200 - Train loss: 0.11412055045366287, Validation loss: 0.28382518887519836
Epoch: 158/200 - Train loss: 0.11405275017023087, Validation loss: 0.2758546769618988
Epoch: 159/200 - Train loss: 0.11380178481340408, Validation loss: 0.27889004349708557
Epoch: 160/200 - Train loss: 0.11363324522972107, Validation loss: 0.27659764885902405
Epoch: 161/200 - Train loss: 0.11311967670917511, Validation loss: 0.2794198989868164
Epoch: 162/200 - Train loss: 0.11427024006843567, Validation loss: 0.2781805992126465
Epoch: 163/200 - Train loss: 0.11433698982000351, Validation loss: 0.2772212624549866
Epoch: 164/200 - Train loss: 0.11407379060983658, Validation loss: 0.28127485513687134
Epoch: 165/200 - Train loss: 0.11292745918035507, Validation loss: 0.2790927588939667
Epoch: 166/200 - Train loss: 0.11330270767211914, Validation loss: 0.28532740473747253
Epoch: 167/200 - Train loss: 0.113058902323246, Validation loss: 0.28267401456832886
Epoch: 168/200 - Train loss: 0.11262429505586624, Validation loss: 0.2858448326587677
Epoch: 169/200 - Train loss: 0.11287383735179901, Validation loss: 0.28016987442970276
Epoch: 170/200 - Train loss: 0.11291645467281342, Validation loss: 0.2830747663974762
Epoch: 171/200 - Train loss: 0.11236352473497391, Validation loss: 0.2829132676124573
Epoch: 172/200 - Train loss: 0.11285941302776337, Validation loss: 0.28554317355155945
Epoch: 173/200 - Train loss: 0.1128702461719513, Validation loss: 0.28470414876937866
Epoch: 174/200 - Train loss: 0.11220996081829071, Validation loss: 0.2875881791114807
Epoch: 175/200 - Train loss: 0.11231274902820587, Validation loss: 0.2870791256427765
Epoch: 176/200 - Train loss: 0.11207824945449829, Validation loss: 0.2858772277832031
Epoch: 177/200 - Train loss: 0.11227688938379288, Validation loss: 0.284755140542984
Epoch: 178/200 - Train loss: 0.1114397719502449, Validation loss: 0.28206831216812134
Epoch: 179/200 - Train loss: 0.11226487904787064, Validation loss: 0.28489363193511963
Epoch: 180/200 - Train loss: 0.1119391992688179, Validation loss: 0.28409847617149353
Epoch: 181/200 - Train loss: 0.11291753500699997, Validation loss: 0.2874373495578766
Epoch: 182/200 - Train loss: 0.11178944259881973, Validation loss: 0.2836167514324188
Epoch: 183/200 - Train loss: 0.11101436614990234, Validation loss: 0.2919114828109741
Epoch: 184/200 - Train loss: 0.11102283000946045, Validation loss: 0.2864289879798889
Epoch: 185/200 - Train loss: 0.11165130883455276, Validation loss: 0.28529709577560425
Epoch: 186/200 - Train loss: 0.11207769811153412, Validation loss: 0.2881667912006378
Epoch: 187/200 - Train loss: 0.11051451414823532, Validation loss: 0.2924085259437561
Epoch: 188/200 - Train loss: 0.1108093410730362, Validation loss: 0.290138840675354
Epoch: 189/200 - Train loss: 0.11075471341609955, Validation loss: 0.28741681575775146
Epoch: 190/200 - Train loss: 0.11127413809299469, Validation loss: 0.2911384403705597
Epoch: 191/200 - Train loss: 0.11117144674062729, Validation loss: 0.28864923119544983
Epoch: 192/200 - Train loss: 0.11105801910161972, Validation loss: 0.29287126660346985
Epoch: 193/200 - Train loss: 0.10959386825561523, Validation loss: 0.29579058289527893
Epoch: 194/200 - Train loss: 0.11107472330331802, Validation loss: 0.2942684590816498
Epoch: 195/200 - Train loss: 0.11108903586864471, Validation loss: 0.2917096018791199
Epoch: 196/200 - Train loss: 0.11021231859922409, Validation loss: 0.29208606481552124
Epoch: 197/200 - Train loss: 0.11041823029518127, Validation loss: 0.29053500294685364
Epoch: 198/200 - Train loss: 0.11080390959978104, Validation loss: 0.2913312613964081
Epoch: 199/200 - Train loss: 0.11065082997083664, Validation loss: 0.2923072278499603
Epoch: 200/200 - Train loss: 0.10992241650819778, Validation loss: 0.29543420672416687
