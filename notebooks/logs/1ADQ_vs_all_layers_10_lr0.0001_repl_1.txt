Epoch: 1/200 - Train loss: 0.6896068453788757, Validation loss: 0.680060625076294
Epoch: 2/200 - Train loss: 0.6686813831329346, Validation loss: 0.6560430526733398
Epoch: 3/200 - Train loss: 0.6391960978507996, Validation loss: 0.6248300671577454
Epoch: 4/200 - Train loss: 0.6079419851303101, Validation loss: 0.5969235301017761
Epoch: 5/200 - Train loss: 0.581576943397522, Validation loss: 0.5744103193283081
Epoch: 6/200 - Train loss: 0.5605962872505188, Validation loss: 0.5575990080833435
Epoch: 7/200 - Train loss: 0.544607937335968, Validation loss: 0.5438269972801208
Epoch: 8/200 - Train loss: 0.5320274829864502, Validation loss: 0.5335183143615723
Epoch: 9/200 - Train loss: 0.5221673846244812, Validation loss: 0.5250606536865234
Epoch: 10/200 - Train loss: 0.5143462419509888, Validation loss: 0.5184482932090759
Epoch: 11/200 - Train loss: 0.5078941583633423, Validation loss: 0.513037383556366
Epoch: 12/200 - Train loss: 0.5021882653236389, Validation loss: 0.5080527067184448
Epoch: 13/200 - Train loss: 0.49737414717674255, Validation loss: 0.503412127494812
Epoch: 14/200 - Train loss: 0.4929542541503906, Validation loss: 0.5005901455879211
Epoch: 15/200 - Train loss: 0.4889283776283264, Validation loss: 0.4963837265968323
Epoch: 16/200 - Train loss: 0.48543938994407654, Validation loss: 0.49261415004730225
Epoch: 17/200 - Train loss: 0.4818752110004425, Validation loss: 0.48959919810295105
Epoch: 18/200 - Train loss: 0.4788464307785034, Validation loss: 0.4869701564311981
Epoch: 19/200 - Train loss: 0.4755854308605194, Validation loss: 0.48400789499282837
Epoch: 20/200 - Train loss: 0.472113698720932, Validation loss: 0.4812471568584442
Epoch: 21/200 - Train loss: 0.46954357624053955, Validation loss: 0.47823888063430786
Epoch: 22/200 - Train loss: 0.46652647852897644, Validation loss: 0.4758325517177582
Epoch: 23/200 - Train loss: 0.4642358720302582, Validation loss: 0.473004549741745
Epoch: 24/200 - Train loss: 0.46175143122673035, Validation loss: 0.4708213210105896
Epoch: 25/200 - Train loss: 0.4595882296562195, Validation loss: 0.4683903157711029
Epoch: 26/200 - Train loss: 0.4567010998725891, Validation loss: 0.4659363031387329
Epoch: 27/200 - Train loss: 0.45415905117988586, Validation loss: 0.46409890055656433
Epoch: 28/200 - Train loss: 0.4521820545196533, Validation loss: 0.4621056616306305
Epoch: 29/200 - Train loss: 0.44998735189437866, Validation loss: 0.4601256549358368
Epoch: 30/200 - Train loss: 0.4479866027832031, Validation loss: 0.45844706892967224
Epoch: 31/200 - Train loss: 0.4456142485141754, Validation loss: 0.45656758546829224
Epoch: 32/200 - Train loss: 0.44389116764068604, Validation loss: 0.45481744408607483
Epoch: 33/200 - Train loss: 0.44187891483306885, Validation loss: 0.4540332853794098
Epoch: 34/200 - Train loss: 0.43968465924263, Validation loss: 0.4510522186756134
Epoch: 35/200 - Train loss: 0.4377928674221039, Validation loss: 0.4493838846683502
Epoch: 36/200 - Train loss: 0.4358345866203308, Validation loss: 0.44719770550727844
Epoch: 37/200 - Train loss: 0.4339199364185333, Validation loss: 0.44606518745422363
Epoch: 38/200 - Train loss: 0.43229252099990845, Validation loss: 0.44511356949806213
Epoch: 39/200 - Train loss: 0.4298838973045349, Validation loss: 0.44266289472579956
Epoch: 40/200 - Train loss: 0.42775824666023254, Validation loss: 0.44183632731437683
Epoch: 41/200 - Train loss: 0.4260253310203552, Validation loss: 0.439336359500885
Epoch: 42/200 - Train loss: 0.4238014817237854, Validation loss: 0.4379327893257141
Epoch: 43/200 - Train loss: 0.4222399592399597, Validation loss: 0.43641310930252075
Epoch: 44/200 - Train loss: 0.4199926257133484, Validation loss: 0.4346481263637543
Epoch: 45/200 - Train loss: 0.4183659851551056, Validation loss: 0.4327618479728699
Epoch: 46/200 - Train loss: 0.4164150655269623, Validation loss: 0.431042343378067
Epoch: 47/200 - Train loss: 0.4146065413951874, Validation loss: 0.4291929602622986
Epoch: 48/200 - Train loss: 0.4125674068927765, Validation loss: 0.4282453656196594
Epoch: 49/200 - Train loss: 0.4107799828052521, Validation loss: 0.4258125126361847
Epoch: 50/200 - Train loss: 0.40857526659965515, Validation loss: 0.4247855544090271
Epoch: 51/200 - Train loss: 0.40669047832489014, Validation loss: 0.4227216839790344
Epoch: 52/200 - Train loss: 0.40489163994789124, Validation loss: 0.42057865858078003
Epoch: 53/200 - Train loss: 0.4027138948440552, Validation loss: 0.41859832406044006
Epoch: 54/200 - Train loss: 0.4007798731327057, Validation loss: 0.41653531789779663
Epoch: 55/200 - Train loss: 0.39870938658714294, Validation loss: 0.4157133400440216
Epoch: 56/200 - Train loss: 0.39670348167419434, Validation loss: 0.4132902920246124
Epoch: 57/200 - Train loss: 0.39473018050193787, Validation loss: 0.4117770791053772
Epoch: 58/200 - Train loss: 0.3930472731590271, Validation loss: 0.4099973142147064
Epoch: 59/200 - Train loss: 0.39119768142700195, Validation loss: 0.4081786572933197
Epoch: 60/200 - Train loss: 0.38925349712371826, Validation loss: 0.4074547588825226
Epoch: 61/200 - Train loss: 0.38680005073547363, Validation loss: 0.4050787687301636
Epoch: 62/200 - Train loss: 0.38540327548980713, Validation loss: 0.4029923677444458
Epoch: 63/200 - Train loss: 0.3830564320087433, Validation loss: 0.40036287903785706
Epoch: 64/200 - Train loss: 0.3811390697956085, Validation loss: 0.39968085289001465
Epoch: 65/200 - Train loss: 0.37940698862075806, Validation loss: 0.39753666520118713
Epoch: 66/200 - Train loss: 0.3774906098842621, Validation loss: 0.39599165320396423
Epoch: 67/200 - Train loss: 0.3756018579006195, Validation loss: 0.3945949971675873
Epoch: 68/200 - Train loss: 0.37380552291870117, Validation loss: 0.3923187255859375
Epoch: 69/200 - Train loss: 0.3714831471443176, Validation loss: 0.3906538486480713
Epoch: 70/200 - Train loss: 0.36976251006126404, Validation loss: 0.38928207755088806
Epoch: 71/200 - Train loss: 0.3675515353679657, Validation loss: 0.3878936171531677
Epoch: 72/200 - Train loss: 0.3658921420574188, Validation loss: 0.3859608769416809
Epoch: 73/200 - Train loss: 0.3641003370285034, Validation loss: 0.38422954082489014
Epoch: 74/200 - Train loss: 0.3618074357509613, Validation loss: 0.38367950916290283
Epoch: 75/200 - Train loss: 0.3601115942001343, Validation loss: 0.3802734613418579
Epoch: 76/200 - Train loss: 0.3581937551498413, Validation loss: 0.37947311997413635
Epoch: 77/200 - Train loss: 0.35678359866142273, Validation loss: 0.3777400851249695
Epoch: 78/200 - Train loss: 0.3547694683074951, Validation loss: 0.37603601813316345
Epoch: 79/200 - Train loss: 0.3528876006603241, Validation loss: 0.37419021129608154
Epoch: 80/200 - Train loss: 0.35117635130882263, Validation loss: 0.3736908733844757
Epoch: 81/200 - Train loss: 0.3496483862400055, Validation loss: 0.37154486775398254
Epoch: 82/200 - Train loss: 0.3477334976196289, Validation loss: 0.3700272738933563
Epoch: 83/200 - Train loss: 0.3460419774055481, Validation loss: 0.3686968982219696
Epoch: 84/200 - Train loss: 0.34448298811912537, Validation loss: 0.3673191964626312
Epoch: 85/200 - Train loss: 0.34266629815101624, Validation loss: 0.36666691303253174
Epoch: 86/200 - Train loss: 0.3411243259906769, Validation loss: 0.3640063405036926
Epoch: 87/200 - Train loss: 0.3393290042877197, Validation loss: 0.3626095652580261
Epoch: 88/200 - Train loss: 0.3376735746860504, Validation loss: 0.3611978590488434
Epoch: 89/200 - Train loss: 0.33609363436698914, Validation loss: 0.3606413006782532
Epoch: 90/200 - Train loss: 0.3342685103416443, Validation loss: 0.35907837748527527
Epoch: 91/200 - Train loss: 0.3328186571598053, Validation loss: 0.35714465379714966
Epoch: 92/200 - Train loss: 0.33114519715309143, Validation loss: 0.3557156026363373
Epoch: 93/200 - Train loss: 0.330161452293396, Validation loss: 0.3544858396053314
Epoch: 94/200 - Train loss: 0.32822757959365845, Validation loss: 0.35296863317489624
Epoch: 95/200 - Train loss: 0.3270091712474823, Validation loss: 0.352334201335907
Epoch: 96/200 - Train loss: 0.32522329688072205, Validation loss: 0.35072606801986694
Epoch: 97/200 - Train loss: 0.3238607943058014, Validation loss: 0.3496388792991638
Epoch: 98/200 - Train loss: 0.32233235239982605, Validation loss: 0.34824085235595703
Epoch: 99/200 - Train loss: 0.32073649764060974, Validation loss: 0.34628087282180786
Epoch: 100/200 - Train loss: 0.3194328248500824, Validation loss: 0.3457236588001251
Epoch: 101/200 - Train loss: 0.31769782304763794, Validation loss: 0.3449030816555023
Epoch: 102/200 - Train loss: 0.3165421783924103, Validation loss: 0.3427075147628784
Epoch: 103/200 - Train loss: 0.3153672516345978, Validation loss: 0.3417649269104004
Epoch: 104/200 - Train loss: 0.31403452157974243, Validation loss: 0.3405502438545227
Epoch: 105/200 - Train loss: 0.3126984238624573, Validation loss: 0.34005939960479736
Epoch: 106/200 - Train loss: 0.3110456168651581, Validation loss: 0.3392504155635834
Epoch: 107/200 - Train loss: 0.30998411774635315, Validation loss: 0.33741283416748047
Epoch: 108/200 - Train loss: 0.30804622173309326, Validation loss: 0.3367030918598175
Epoch: 109/200 - Train loss: 0.3069716691970825, Validation loss: 0.33510804176330566
Epoch: 110/200 - Train loss: 0.305808961391449, Validation loss: 0.3345819413661957
Epoch: 111/200 - Train loss: 0.3045765161514282, Validation loss: 0.33315426111221313
Epoch: 112/200 - Train loss: 0.30309900641441345, Validation loss: 0.33237868547439575
Epoch: 113/200 - Train loss: 0.30193302035331726, Validation loss: 0.33203890919685364
Epoch: 114/200 - Train loss: 0.30065497756004333, Validation loss: 0.33025455474853516
Epoch: 115/200 - Train loss: 0.29938021302223206, Validation loss: 0.3295834958553314
Epoch: 116/200 - Train loss: 0.29840049147605896, Validation loss: 0.3281680643558502
Epoch: 117/200 - Train loss: 0.2971813678741455, Validation loss: 0.32719752192497253
Epoch: 118/200 - Train loss: 0.2960268259048462, Validation loss: 0.3268074691295624
Epoch: 119/200 - Train loss: 0.29462823271751404, Validation loss: 0.325559139251709
Epoch: 120/200 - Train loss: 0.29370802640914917, Validation loss: 0.3247683346271515
Epoch: 121/200 - Train loss: 0.29273757338523865, Validation loss: 0.3237960636615753
Epoch: 122/200 - Train loss: 0.291492760181427, Validation loss: 0.3232068717479706
Epoch: 123/200 - Train loss: 0.2903977334499359, Validation loss: 0.3226568400859833
Epoch: 124/200 - Train loss: 0.28919726610183716, Validation loss: 0.32107192277908325
Epoch: 125/200 - Train loss: 0.28788456320762634, Validation loss: 0.3202258348464966
Epoch: 126/200 - Train loss: 0.2873508036136627, Validation loss: 0.3192531168460846
Epoch: 127/200 - Train loss: 0.2861410975456238, Validation loss: 0.31971800327301025
Epoch: 128/200 - Train loss: 0.28517207503318787, Validation loss: 0.31794536113739014
Epoch: 129/200 - Train loss: 0.28423067927360535, Validation loss: 0.31678974628448486
Epoch: 130/200 - Train loss: 0.2826313376426697, Validation loss: 0.31667521595954895
Epoch: 131/200 - Train loss: 0.2822011411190033, Validation loss: 0.31500300765037537
Epoch: 132/200 - Train loss: 0.28074929118156433, Validation loss: 0.3156152069568634
Epoch: 133/200 - Train loss: 0.2801149785518646, Validation loss: 0.3135366141796112
Epoch: 134/200 - Train loss: 0.2793816030025482, Validation loss: 0.3139146864414215
Epoch: 135/200 - Train loss: 0.2781527638435364, Validation loss: 0.313412606716156
Epoch: 136/200 - Train loss: 0.2772088944911957, Validation loss: 0.3112289309501648
Epoch: 137/200 - Train loss: 0.2764692008495331, Validation loss: 0.31115129590034485
Epoch: 138/200 - Train loss: 0.2752573788166046, Validation loss: 0.30998778343200684
Epoch: 139/200 - Train loss: 0.2743409276008606, Validation loss: 0.30941852927207947
Epoch: 140/200 - Train loss: 0.2734091579914093, Validation loss: 0.30894041061401367
Epoch: 141/200 - Train loss: 0.2724122703075409, Validation loss: 0.3084912896156311
Epoch: 142/200 - Train loss: 0.27163395285606384, Validation loss: 0.3084622621536255
Epoch: 143/200 - Train loss: 0.27111902832984924, Validation loss: 0.30653902888298035
Epoch: 144/200 - Train loss: 0.2698782682418823, Validation loss: 0.306145042181015
Epoch: 145/200 - Train loss: 0.2691933512687683, Validation loss: 0.3069307208061218
Epoch: 146/200 - Train loss: 0.26809731125831604, Validation loss: 0.3055914640426636
Epoch: 147/200 - Train loss: 0.2675006687641144, Validation loss: 0.3043898940086365
Epoch: 148/200 - Train loss: 0.2670954465866089, Validation loss: 0.3042594790458679
Epoch: 149/200 - Train loss: 0.2659668028354645, Validation loss: 0.30303576588630676
Epoch: 150/200 - Train loss: 0.26512545347213745, Validation loss: 0.30262622237205505
Epoch: 151/200 - Train loss: 0.26436394453048706, Validation loss: 0.30288392305374146
Epoch: 152/200 - Train loss: 0.2635497450828552, Validation loss: 0.30160340666770935
Epoch: 153/200 - Train loss: 0.26270419359207153, Validation loss: 0.3010414242744446
Epoch: 154/200 - Train loss: 0.2626052498817444, Validation loss: 0.2994214594364166
Epoch: 155/200 - Train loss: 0.26145845651626587, Validation loss: 0.29997071623802185
Epoch: 156/200 - Train loss: 0.2605443596839905, Validation loss: 0.29844269156455994
Epoch: 157/200 - Train loss: 0.2597595751285553, Validation loss: 0.2985394597053528
Epoch: 158/200 - Train loss: 0.2592174708843231, Validation loss: 0.2976042926311493
Epoch: 159/200 - Train loss: 0.258771687746048, Validation loss: 0.2977718710899353
Epoch: 160/200 - Train loss: 0.25780510902404785, Validation loss: 0.29742810130119324
Epoch: 161/200 - Train loss: 0.25745877623558044, Validation loss: 0.2970792055130005
