Epoch: 1/300 - Train loss: 0.6951013803482056, Validation loss: 0.692954421043396
Epoch: 2/300 - Train loss: 0.6938424110412598, Validation loss: 0.6916821599006653
Epoch: 3/300 - Train loss: 0.692611813545227, Validation loss: 0.6904574632644653
Epoch: 4/300 - Train loss: 0.6913984417915344, Validation loss: 0.6892727017402649
Epoch: 5/300 - Train loss: 0.6901885271072388, Validation loss: 0.6880692839622498
Epoch: 6/300 - Train loss: 0.6889733672142029, Validation loss: 0.6868715882301331
Epoch: 7/300 - Train loss: 0.6877493262290955, Validation loss: 0.6855533719062805
Epoch: 8/300 - Train loss: 0.6865090131759644, Validation loss: 0.6843070387840271
Epoch: 9/300 - Train loss: 0.685246467590332, Validation loss: 0.682935357093811
Epoch: 10/300 - Train loss: 0.683955192565918, Validation loss: 0.6816238760948181
Epoch: 11/300 - Train loss: 0.682632565498352, Validation loss: 0.6802533864974976
Epoch: 12/300 - Train loss: 0.6812744140625, Validation loss: 0.6788085103034973
Epoch: 13/300 - Train loss: 0.6798736453056335, Validation loss: 0.6773871183395386
Epoch: 14/300 - Train loss: 0.6784291863441467, Validation loss: 0.6757673621177673
Epoch: 15/300 - Train loss: 0.6769387125968933, Validation loss: 0.674214780330658
Epoch: 16/300 - Train loss: 0.6753994226455688, Validation loss: 0.6726865768432617
Epoch: 17/300 - Train loss: 0.6738086342811584, Validation loss: 0.6708909273147583
Epoch: 18/300 - Train loss: 0.6721636056900024, Validation loss: 0.6691427826881409
Epoch: 19/300 - Train loss: 0.6704632043838501, Validation loss: 0.6673669815063477
Epoch: 20/300 - Train loss: 0.6687059998512268, Validation loss: 0.6655113697052002
Epoch: 21/300 - Train loss: 0.6668910980224609, Validation loss: 0.6634819507598877
Epoch: 22/300 - Train loss: 0.665021538734436, Validation loss: 0.6615447402000427
Epoch: 23/300 - Train loss: 0.6630971431732178, Validation loss: 0.6595166921615601
Epoch: 24/300 - Train loss: 0.6611171364784241, Validation loss: 0.6574763059616089
Epoch: 25/300 - Train loss: 0.6590830087661743, Validation loss: 0.6553751826286316
Epoch: 26/300 - Train loss: 0.6569965481758118, Validation loss: 0.6531587839126587
Epoch: 27/300 - Train loss: 0.654859185218811, Validation loss: 0.6509013772010803
Epoch: 28/300 - Train loss: 0.6526743769645691, Validation loss: 0.6486668586730957
Epoch: 29/300 - Train loss: 0.6504485607147217, Validation loss: 0.6464017629623413
Epoch: 30/300 - Train loss: 0.648181676864624, Validation loss: 0.6439724564552307
Epoch: 31/300 - Train loss: 0.6458747982978821, Validation loss: 0.6415600776672363
Epoch: 32/300 - Train loss: 0.6435304880142212, Validation loss: 0.6389882564544678
Epoch: 33/300 - Train loss: 0.6411535143852234, Validation loss: 0.6366504430770874
Epoch: 34/300 - Train loss: 0.6387462615966797, Validation loss: 0.6341921091079712
Epoch: 35/300 - Train loss: 0.6363122463226318, Validation loss: 0.6316490769386292
Epoch: 36/300 - Train loss: 0.6338542699813843, Validation loss: 0.6291075348854065
Epoch: 37/300 - Train loss: 0.6313775181770325, Validation loss: 0.6266923546791077
Epoch: 38/300 - Train loss: 0.6288835406303406, Validation loss: 0.6242350935935974
Epoch: 39/300 - Train loss: 0.6263724565505981, Validation loss: 0.621661365032196
Epoch: 40/300 - Train loss: 0.6238502860069275, Validation loss: 0.61907958984375
Epoch: 41/300 - Train loss: 0.6213207840919495, Validation loss: 0.6165332794189453
Epoch: 42/300 - Train loss: 0.6187889575958252, Validation loss: 0.6139970421791077
Epoch: 43/300 - Train loss: 0.6162580251693726, Validation loss: 0.6113003492355347
Epoch: 44/300 - Train loss: 0.6137299537658691, Validation loss: 0.6086861491203308
Epoch: 45/300 - Train loss: 0.6112064123153687, Validation loss: 0.6064077615737915
Epoch: 46/300 - Train loss: 0.6086900234222412, Validation loss: 0.6041197776794434
Epoch: 47/300 - Train loss: 0.6061866283416748, Validation loss: 0.6013492941856384
Epoch: 48/300 - Train loss: 0.6036975979804993, Validation loss: 0.5986400842666626
Epoch: 49/300 - Train loss: 0.6012284755706787, Validation loss: 0.5963040590286255
Epoch: 50/300 - Train loss: 0.5987804532051086, Validation loss: 0.593788743019104
Epoch: 51/300 - Train loss: 0.5963545441627502, Validation loss: 0.5913326144218445
Epoch: 52/300 - Train loss: 0.5939546227455139, Validation loss: 0.589262068271637
Epoch: 53/300 - Train loss: 0.5915819406509399, Validation loss: 0.5868551135063171
Epoch: 54/300 - Train loss: 0.5892379879951477, Validation loss: 0.5844038128852844
Epoch: 55/300 - Train loss: 0.5869247913360596, Validation loss: 0.5820634961128235
Epoch: 56/300 - Train loss: 0.5846441388130188, Validation loss: 0.5799530744552612
Epoch: 57/300 - Train loss: 0.5823974013328552, Validation loss: 0.5779110789299011
Epoch: 58/300 - Train loss: 0.5801855325698853, Validation loss: 0.5758348703384399
Epoch: 59/300 - Train loss: 0.5780090689659119, Validation loss: 0.573133647441864
Epoch: 60/300 - Train loss: 0.5758686661720276, Validation loss: 0.5712307691574097
Epoch: 61/300 - Train loss: 0.5737630724906921, Validation loss: 0.5695034265518188
Epoch: 62/300 - Train loss: 0.5716935396194458, Validation loss: 0.5673756003379822
Epoch: 63/300 - Train loss: 0.5696587562561035, Validation loss: 0.5659657716751099
Epoch: 64/300 - Train loss: 0.5676593780517578, Validation loss: 0.5634937286376953
Epoch: 65/300 - Train loss: 0.5656948089599609, Validation loss: 0.5616622567176819
Epoch: 66/300 - Train loss: 0.5637650489807129, Validation loss: 0.5601825714111328
Epoch: 67/300 - Train loss: 0.5618696212768555, Validation loss: 0.5583877563476562
Epoch: 68/300 - Train loss: 0.5600075125694275, Validation loss: 0.5565270781517029
Epoch: 69/300 - Train loss: 0.5581771731376648, Validation loss: 0.5548299551010132
Epoch: 70/300 - Train loss: 0.5563774704933167, Validation loss: 0.5531176328659058
Epoch: 71/300 - Train loss: 0.5546081066131592, Validation loss: 0.5514468550682068
Epoch: 72/300 - Train loss: 0.5528671145439148, Validation loss: 0.5499914288520813
Epoch: 73/300 - Train loss: 0.5511537194252014, Validation loss: 0.5479961633682251
Epoch: 74/300 - Train loss: 0.549466609954834, Validation loss: 0.5463917851448059
Epoch: 75/300 - Train loss: 0.5478044152259827, Validation loss: 0.5450399518013
Epoch: 76/300 - Train loss: 0.5461663007736206, Validation loss: 0.5436307191848755
Epoch: 77/300 - Train loss: 0.5445517897605896, Validation loss: 0.5419567227363586
Epoch: 78/300 - Train loss: 0.5429601073265076, Validation loss: 0.5406218767166138
Epoch: 79/300 - Train loss: 0.5413895845413208, Validation loss: 0.5392436981201172
Epoch: 80/300 - Train loss: 0.5398396253585815, Validation loss: 0.5377547144889832
Epoch: 81/300 - Train loss: 0.5383086800575256, Validation loss: 0.5364627242088318
Epoch: 82/300 - Train loss: 0.5367961525917053, Validation loss: 0.5346910357475281
Epoch: 83/300 - Train loss: 0.5353001952171326, Validation loss: 0.533728301525116
Epoch: 84/300 - Train loss: 0.5338205695152283, Validation loss: 0.5322542786598206
Epoch: 85/300 - Train loss: 0.5323573350906372, Validation loss: 0.5307219624519348
Epoch: 86/300 - Train loss: 0.5309096574783325, Validation loss: 0.5292131900787354
Epoch: 87/300 - Train loss: 0.5294767618179321, Validation loss: 0.5278925895690918
Epoch: 88/300 - Train loss: 0.5280583500862122, Validation loss: 0.5268934965133667
Epoch: 89/300 - Train loss: 0.5266533493995667, Validation loss: 0.5255621075630188
Epoch: 90/300 - Train loss: 0.5252620577812195, Validation loss: 0.5245046615600586
Epoch: 91/300 - Train loss: 0.5238837599754333, Validation loss: 0.5232692956924438
Epoch: 92/300 - Train loss: 0.5225178003311157, Validation loss: 0.5219157934188843
Epoch: 93/300 - Train loss: 0.5211642384529114, Validation loss: 0.521060585975647
Epoch: 94/300 - Train loss: 0.5198216438293457, Validation loss: 0.5189734697341919
Epoch: 95/300 - Train loss: 0.5184901356697083, Validation loss: 0.5178074240684509
Epoch: 96/300 - Train loss: 0.517167866230011, Validation loss: 0.5167403221130371
Epoch: 97/300 - Train loss: 0.5158534646034241, Validation loss: 0.5157146453857422
Epoch: 98/300 - Train loss: 0.5145465731620789, Validation loss: 0.5145408511161804
Epoch: 99/300 - Train loss: 0.5132474303245544, Validation loss: 0.5125934481620789
Epoch: 100/300 - Train loss: 0.5119573473930359, Validation loss: 0.5118424296379089
Epoch: 101/300 - Train loss: 0.510676383972168, Validation loss: 0.5108798742294312
Epoch: 102/300 - Train loss: 0.5094026327133179, Validation loss: 0.5093731880187988
Epoch: 103/300 - Train loss: 0.5081360340118408, Validation loss: 0.508107602596283
Epoch: 104/300 - Train loss: 0.5068768858909607, Validation loss: 0.5071437954902649
Epoch: 105/300 - Train loss: 0.5056247711181641, Validation loss: 0.5055516958236694
Epoch: 106/300 - Train loss: 0.5043790936470032, Validation loss: 0.5047860741615295
Epoch: 107/300 - Train loss: 0.5031407475471497, Validation loss: 0.5030912756919861
Epoch: 108/300 - Train loss: 0.5019089579582214, Validation loss: 0.5025115013122559
Epoch: 109/300 - Train loss: 0.5006828904151917, Validation loss: 0.5011795163154602
Epoch: 110/300 - Train loss: 0.49946245551109314, Validation loss: 0.5005850791931152
Epoch: 111/300 - Train loss: 0.4982466697692871, Validation loss: 0.4988969564437866
Epoch: 112/300 - Train loss: 0.49703580141067505, Validation loss: 0.49810007214546204
Epoch: 113/300 - Train loss: 0.4958310127258301, Validation loss: 0.49660027027130127
Epoch: 114/300 - Train loss: 0.4946305453777313, Validation loss: 0.4953058660030365
Epoch: 115/300 - Train loss: 0.4934351146221161, Validation loss: 0.49442118406295776
Epoch: 116/300 - Train loss: 0.49224406480789185, Validation loss: 0.49297836422920227
Epoch: 117/300 - Train loss: 0.49105870723724365, Validation loss: 0.4920680820941925
Epoch: 118/300 - Train loss: 0.4898775815963745, Validation loss: 0.4911041855812073
Epoch: 119/300 - Train loss: 0.48869943618774414, Validation loss: 0.4903947710990906
Epoch: 120/300 - Train loss: 0.4875265955924988, Validation loss: 0.48837772011756897
Epoch: 121/300 - Train loss: 0.48635926842689514, Validation loss: 0.4877891540527344
Epoch: 122/300 - Train loss: 0.48519712686538696, Validation loss: 0.48689740896224976
Epoch: 123/300 - Train loss: 0.4840392470359802, Validation loss: 0.48546743392944336
Epoch: 124/300 - Train loss: 0.48288601636886597, Validation loss: 0.48450684547424316
Epoch: 125/300 - Train loss: 0.48173826932907104, Validation loss: 0.48330992460250854
Epoch: 126/300 - Train loss: 0.48059627413749695, Validation loss: 0.4821491539478302
Epoch: 127/300 - Train loss: 0.4794590175151825, Validation loss: 0.4811058044433594
Epoch: 128/300 - Train loss: 0.4783267080783844, Validation loss: 0.4798443615436554
Epoch: 129/300 - Train loss: 0.4771995544433594, Validation loss: 0.4788808524608612
Epoch: 130/300 - Train loss: 0.4760776460170746, Validation loss: 0.4781838655471802
Epoch: 131/300 - Train loss: 0.4749603271484375, Validation loss: 0.47640591859817505
Epoch: 132/300 - Train loss: 0.4738475978374481, Validation loss: 0.475669801235199
Epoch: 133/300 - Train loss: 0.47273996472358704, Validation loss: 0.4743545651435852
Epoch: 134/300 - Train loss: 0.47163769602775574, Validation loss: 0.4737829566001892
Epoch: 135/300 - Train loss: 0.47053995728492737, Validation loss: 0.47258055210113525
Epoch: 136/300 - Train loss: 0.4694475829601288, Validation loss: 0.4715314209461212
Epoch: 137/300 - Train loss: 0.4683615267276764, Validation loss: 0.47005635499954224
Epoch: 138/300 - Train loss: 0.46728095412254333, Validation loss: 0.4695345461368561
Epoch: 139/300 - Train loss: 0.46620529890060425, Validation loss: 0.4688485860824585
Epoch: 140/300 - Train loss: 0.46513426303863525, Validation loss: 0.4675122797489166
Epoch: 141/300 - Train loss: 0.46406829357147217, Validation loss: 0.4672377109527588
Epoch: 142/300 - Train loss: 0.46300724148750305, Validation loss: 0.4655092656612396
Epoch: 143/300 - Train loss: 0.4619508683681488, Validation loss: 0.4640691876411438
Epoch: 144/300 - Train loss: 0.4608991742134094, Validation loss: 0.46338218450546265
Epoch: 145/300 - Train loss: 0.45985251665115356, Validation loss: 0.4623578190803528
Epoch: 146/300 - Train loss: 0.4588111340999603, Validation loss: 0.4613282084465027
Epoch: 147/300 - Train loss: 0.4577748775482178, Validation loss: 0.46017950773239136
Epoch: 148/300 - Train loss: 0.45674386620521545, Validation loss: 0.4594847857952118
Epoch: 149/300 - Train loss: 0.4557179808616638, Validation loss: 0.4582233130931854
Epoch: 150/300 - Train loss: 0.45469775795936584, Validation loss: 0.4577074646949768
Epoch: 151/300 - Train loss: 0.45368409156799316, Validation loss: 0.45698603987693787
Epoch: 152/300 - Train loss: 0.4526757299900055, Validation loss: 0.455763041973114
Epoch: 153/300 - Train loss: 0.4516727030277252, Validation loss: 0.45518285036087036
Epoch: 154/300 - Train loss: 0.4506748914718628, Validation loss: 0.45327287912368774
Epoch: 155/300 - Train loss: 0.4496821463108063, Validation loss: 0.4524613916873932
Epoch: 156/300 - Train loss: 0.44869616627693176, Validation loss: 0.4521072506904602
Epoch: 157/300 - Train loss: 0.44771629571914673, Validation loss: 0.45067551732063293
Epoch: 158/300 - Train loss: 0.446743369102478, Validation loss: 0.44964271783828735
Epoch: 159/300 - Train loss: 0.4457772970199585, Validation loss: 0.4492124617099762
Epoch: 160/300 - Train loss: 0.4448170065879822, Validation loss: 0.4476931393146515
Epoch: 161/300 - Train loss: 0.44386252760887146, Validation loss: 0.4471277892589569
Epoch: 162/300 - Train loss: 0.44291454553604126, Validation loss: 0.4464884400367737
Epoch: 163/300 - Train loss: 0.44197341799736023, Validation loss: 0.445085346698761
Epoch: 164/300 - Train loss: 0.4410388171672821, Validation loss: 0.44457486271858215
Epoch: 165/300 - Train loss: 0.4401109218597412, Validation loss: 0.44342347979545593
Epoch: 166/300 - Train loss: 0.4391877353191376, Validation loss: 0.4430038332939148
Epoch: 167/300 - Train loss: 0.4382690489292145, Validation loss: 0.4414496123790741
Epoch: 168/300 - Train loss: 0.43735525012016296, Validation loss: 0.44094178080558777
Epoch: 169/300 - Train loss: 0.43644627928733826, Validation loss: 0.43980997800827026
Epoch: 170/300 - Train loss: 0.4355428218841553, Validation loss: 0.43920081853866577
Epoch: 171/300 - Train loss: 0.4346444308757782, Validation loss: 0.4381612539291382
Epoch: 172/300 - Train loss: 0.4337512254714966, Validation loss: 0.4376063942909241
Epoch: 173/300 - Train loss: 0.4328632950782776, Validation loss: 0.4367378056049347
Epoch: 174/300 - Train loss: 0.43198075890541077, Validation loss: 0.43600013852119446
Epoch: 175/300 - Train loss: 0.431103378534317, Validation loss: 0.4347792863845825
Epoch: 176/300 - Train loss: 0.4302308261394501, Validation loss: 0.4341427981853485
Epoch: 177/300 - Train loss: 0.4293632507324219, Validation loss: 0.4332350492477417
Epoch: 178/300 - Train loss: 0.42850080132484436, Validation loss: 0.4321746230125427
Epoch: 179/300 - Train loss: 0.42764267325401306, Validation loss: 0.4322042167186737
Epoch: 180/300 - Train loss: 0.42679014801979065, Validation loss: 0.4310084879398346
Epoch: 181/300 - Train loss: 0.42594316601753235, Validation loss: 0.43094974756240845
Epoch: 182/300 - Train loss: 0.42510169744491577, Validation loss: 0.42954525351524353
Epoch: 183/300 - Train loss: 0.4242657721042633, Validation loss: 0.4285357594490051
Epoch: 184/300 - Train loss: 0.4234357476234436, Validation loss: 0.4276067614555359
Epoch: 185/300 - Train loss: 0.42261210083961487, Validation loss: 0.42760398983955383
Epoch: 186/300 - Train loss: 0.4217943251132965, Validation loss: 0.4264076352119446
Epoch: 187/300 - Train loss: 0.4209824502468109, Validation loss: 0.4259819984436035
Epoch: 188/300 - Train loss: 0.4201759397983551, Validation loss: 0.4246039092540741
Epoch: 189/300 - Train loss: 0.419374942779541, Validation loss: 0.4235205352306366
Epoch: 190/300 - Train loss: 0.4185803234577179, Validation loss: 0.42253315448760986
Epoch: 191/300 - Train loss: 0.41779235005378723, Validation loss: 0.42238184809684753
Epoch: 192/300 - Train loss: 0.4170090854167938, Validation loss: 0.4214300215244293
Epoch: 193/300 - Train loss: 0.41623103618621826, Validation loss: 0.42072540521621704
Epoch: 194/300 - Train loss: 0.4154576063156128, Validation loss: 0.419889360666275
Epoch: 195/300 - Train loss: 0.4146895408630371, Validation loss: 0.4197891056537628
Epoch: 196/300 - Train loss: 0.41392678022384644, Validation loss: 0.41896045207977295
Epoch: 197/300 - Train loss: 0.41316914558410645, Validation loss: 0.4174201190471649
Epoch: 198/300 - Train loss: 0.41241705417633057, Validation loss: 0.41705024242401123
Epoch: 199/300 - Train loss: 0.41167014837265015, Validation loss: 0.4165879786014557
Epoch: 200/300 - Train loss: 0.410928875207901, Validation loss: 0.4159758985042572
Epoch: 201/300 - Train loss: 0.41019293665885925, Validation loss: 0.41486090421676636
Epoch: 202/300 - Train loss: 0.40946194529533386, Validation loss: 0.41425126791000366
Epoch: 203/300 - Train loss: 0.4087357819080353, Validation loss: 0.4136669337749481
Epoch: 204/300 - Train loss: 0.4080136716365814, Validation loss: 0.4133684039115906
Epoch: 205/300 - Train loss: 0.40729621052742004, Validation loss: 0.41247522830963135
Epoch: 206/300 - Train loss: 0.4065839946269989, Validation loss: 0.4122140407562256
Epoch: 207/300 - Train loss: 0.40587612986564636, Validation loss: 0.4106104075908661
Epoch: 208/300 - Train loss: 0.40517255663871765, Validation loss: 0.4102031886577606
Epoch: 209/300 - Train loss: 0.4044741690158844, Validation loss: 0.4092155992984772
Epoch: 210/300 - Train loss: 0.4037811756134033, Validation loss: 0.40885791182518005
Epoch: 211/300 - Train loss: 0.40309301018714905, Validation loss: 0.40859246253967285
Epoch: 212/300 - Train loss: 0.40240925550460815, Validation loss: 0.4083499610424042
Epoch: 213/300 - Train loss: 0.40172988176345825, Validation loss: 0.4071803092956543
Epoch: 214/300 - Train loss: 0.40105506777763367, Validation loss: 0.40627583861351013
Epoch: 215/300 - Train loss: 0.4003850817680359, Validation loss: 0.40606409311294556
Epoch: 216/300 - Train loss: 0.39972007274627686, Validation loss: 0.40538230538368225
Epoch: 217/300 - Train loss: 0.3990590572357178, Validation loss: 0.40489086508750916
Epoch: 218/300 - Train loss: 0.39840248227119446, Validation loss: 0.4042425751686096
