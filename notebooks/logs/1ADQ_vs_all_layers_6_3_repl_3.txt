Epoch: 1/200 - Train loss: 0.5850863456726074, Validation loss: 0.4848760962486267
Epoch: 2/200 - Train loss: 0.4572160840034485, Validation loss: 0.4482821226119995
Epoch: 3/200 - Train loss: 0.4261977970600128, Validation loss: 0.4259577989578247
Epoch: 4/200 - Train loss: 0.40097472071647644, Validation loss: 0.40215539932250977
Epoch: 5/200 - Train loss: 0.3770939111709595, Validation loss: 0.3800588548183441
Epoch: 6/200 - Train loss: 0.35542136430740356, Validation loss: 0.3634284436702728
Epoch: 7/200 - Train loss: 0.3347412049770355, Validation loss: 0.34725484251976013
Epoch: 8/200 - Train loss: 0.31755074858665466, Validation loss: 0.3333323001861572
Epoch: 9/200 - Train loss: 0.30054065585136414, Validation loss: 0.32516175508499146
Epoch: 10/200 - Train loss: 0.2879100441932678, Validation loss: 0.3132537305355072
Epoch: 11/200 - Train loss: 0.27817997336387634, Validation loss: 0.309764564037323
Epoch: 12/200 - Train loss: 0.2709723114967346, Validation loss: 0.3035257160663605
Epoch: 13/200 - Train loss: 0.2656983435153961, Validation loss: 0.301787406206131
Epoch: 14/200 - Train loss: 0.260186105966568, Validation loss: 0.30260783433914185
Epoch: 15/200 - Train loss: 0.2574058771133423, Validation loss: 0.29801130294799805
Epoch: 16/200 - Train loss: 0.254140704870224, Validation loss: 0.3003820478916168
Epoch: 17/200 - Train loss: 0.2509050667285919, Validation loss: 0.29835230112075806
Epoch: 18/200 - Train loss: 0.24841003119945526, Validation loss: 0.2956050634384155
Epoch: 19/200 - Train loss: 0.2467343956232071, Validation loss: 0.29499733448028564
Epoch: 20/200 - Train loss: 0.2450312376022339, Validation loss: 0.2933996319770813
Epoch: 21/200 - Train loss: 0.24374045431613922, Validation loss: 0.29697856307029724
Epoch: 22/200 - Train loss: 0.2412947714328766, Validation loss: 0.2950606048107147
Epoch: 23/200 - Train loss: 0.2411481887102127, Validation loss: 0.29355382919311523
Epoch: 24/200 - Train loss: 0.23898765444755554, Validation loss: 0.29808148741722107
Epoch: 25/200 - Train loss: 0.23790210485458374, Validation loss: 0.2951361835002899
Epoch: 26/200 - Train loss: 0.23666584491729736, Validation loss: 0.2947021722793579
Epoch: 1/200 - Train loss: 0.5928544402122498, Validation loss: 0.5238646864891052
Epoch: 2/200 - Train loss: 0.49805697798728943, Validation loss: 0.48614606261253357
Epoch: 3/200 - Train loss: 0.44487714767456055, Validation loss: 0.4400606155395508
Epoch: 4/200 - Train loss: 0.40679624676704407, Validation loss: 0.4071621596813202
Epoch: 5/200 - Train loss: 0.3778159022331238, Validation loss: 0.38830262422561646
Epoch: 6/200 - Train loss: 0.35809987783432007, Validation loss: 0.374657541513443
Epoch: 7/200 - Train loss: 0.3435492515563965, Validation loss: 0.3615053594112396
Epoch: 8/200 - Train loss: 0.33249208331108093, Validation loss: 0.3540171682834625
Epoch: 9/200 - Train loss: 0.32300156354904175, Validation loss: 0.34574997425079346
Epoch: 10/200 - Train loss: 0.3164155185222626, Validation loss: 0.34045276045799255
Epoch: 11/200 - Train loss: 0.31062665581703186, Validation loss: 0.33370786905288696
Epoch: 12/200 - Train loss: 0.30339908599853516, Validation loss: 0.33271437883377075
Epoch: 13/200 - Train loss: 0.29756951332092285, Validation loss: 0.32857733964920044
Epoch: 14/200 - Train loss: 0.29335492849349976, Validation loss: 0.3237845301628113
Epoch: 15/200 - Train loss: 0.28853145241737366, Validation loss: 0.32076990604400635
Epoch: 16/200 - Train loss: 0.2837625741958618, Validation loss: 0.3156019449234009
Epoch: 17/200 - Train loss: 0.2799973487854004, Validation loss: 0.31492429971694946
Epoch: 18/200 - Train loss: 0.2755359411239624, Validation loss: 0.31251758337020874
Epoch: 19/200 - Train loss: 0.2734297215938568, Validation loss: 0.3121420443058014
Epoch: 20/200 - Train loss: 0.268763929605484, Validation loss: 0.3099847733974457
Epoch: 21/200 - Train loss: 0.2673133909702301, Validation loss: 0.30590036511421204
Epoch: 22/200 - Train loss: 0.263286828994751, Validation loss: 0.3099988102912903
Epoch: 23/200 - Train loss: 0.2609861493110657, Validation loss: 0.3091023862361908
Epoch: 24/200 - Train loss: 0.2593730390071869, Validation loss: 0.3034219741821289
Epoch: 25/200 - Train loss: 0.25858786702156067, Validation loss: 0.3068363666534424
Epoch: 26/200 - Train loss: 0.25615793466567993, Validation loss: 0.30487218499183655
Epoch: 27/200 - Train loss: 0.2544550895690918, Validation loss: 0.3097192049026489
Epoch: 28/200 - Train loss: 0.2528517544269562, Validation loss: 0.30125799775123596
Epoch: 29/200 - Train loss: 0.2512912154197693, Validation loss: 0.3089454174041748
Epoch: 30/200 - Train loss: 0.2523327171802521, Validation loss: 0.3020671606063843
Epoch: 31/200 - Train loss: 0.25079792737960815, Validation loss: 0.30125096440315247
Epoch: 32/200 - Train loss: 0.24889348447322845, Validation loss: 0.3010541498661041
Epoch: 33/200 - Train loss: 0.2474120855331421, Validation loss: 0.3015861511230469
Epoch: 34/200 - Train loss: 0.2472742199897766, Validation loss: 0.2964261472225189
Epoch: 35/200 - Train loss: 0.24636203050613403, Validation loss: 0.2984197735786438
Epoch: 36/200 - Train loss: 0.24586081504821777, Validation loss: 0.2982979118824005
Epoch: 37/200 - Train loss: 0.24383805692195892, Validation loss: 0.29154250025749207
Epoch: 38/200 - Train loss: 0.24395279586315155, Validation loss: 0.29854729771614075
Epoch: 39/200 - Train loss: 0.24296167492866516, Validation loss: 0.29461196064949036
Epoch: 40/200 - Train loss: 0.24290721118450165, Validation loss: 0.2912740707397461
Epoch: 41/200 - Train loss: 0.24241229891777039, Validation loss: 0.2907165288925171
Epoch: 42/200 - Train loss: 0.24098247289657593, Validation loss: 0.2882261872291565
Epoch: 43/200 - Train loss: 0.2397644817829132, Validation loss: 0.2887970507144928
Epoch: 44/200 - Train loss: 0.24010714888572693, Validation loss: 0.2901401221752167
Epoch: 45/200 - Train loss: 0.23847877979278564, Validation loss: 0.28944820165634155
Epoch: 46/200 - Train loss: 0.23804998397827148, Validation loss: 0.2854389548301697
Epoch: 47/200 - Train loss: 0.23701299726963043, Validation loss: 0.2861643135547638
Epoch: 48/200 - Train loss: 0.23629222810268402, Validation loss: 0.2858516275882721
Epoch: 49/200 - Train loss: 0.23614749312400818, Validation loss: 0.286165714263916
Epoch: 50/200 - Train loss: 0.23673215508460999, Validation loss: 0.2841945290565491
Epoch: 51/200 - Train loss: 0.2347181886434555, Validation loss: 0.2840501666069031
Epoch: 52/200 - Train loss: 0.23424884676933289, Validation loss: 0.283124178647995
Epoch: 53/200 - Train loss: 0.23321175575256348, Validation loss: 0.2863827645778656
Epoch: 54/200 - Train loss: 0.23327864706516266, Validation loss: 0.2880769670009613
Epoch: 55/200 - Train loss: 0.2318514883518219, Validation loss: 0.27936863899230957
Epoch: 56/200 - Train loss: 0.23230533301830292, Validation loss: 0.28226345777511597
Epoch: 57/200 - Train loss: 0.23158907890319824, Validation loss: 0.2800232470035553
Epoch: 58/200 - Train loss: 0.23170654475688934, Validation loss: 0.2772026062011719
Epoch: 59/200 - Train loss: 0.22989223897457123, Validation loss: 0.2787267565727234
Epoch: 60/200 - Train loss: 0.23000970482826233, Validation loss: 0.282298743724823
Epoch: 61/200 - Train loss: 0.22952039539813995, Validation loss: 0.28044167160987854
Epoch: 62/200 - Train loss: 0.22876723110675812, Validation loss: 0.2835119366645813
Epoch: 63/200 - Train loss: 0.22947096824645996, Validation loss: 0.2788642644882202
Epoch: 64/200 - Train loss: 0.2305159717798233, Validation loss: 0.28571516275405884
Epoch: 65/200 - Train loss: 0.22884513437747955, Validation loss: 0.2745876908302307
Epoch: 66/200 - Train loss: 0.2267627865076065, Validation loss: 0.2744816541671753
Epoch: 67/200 - Train loss: 0.22591519355773926, Validation loss: 0.27902284264564514
Epoch: 68/200 - Train loss: 0.22605638206005096, Validation loss: 0.27687859535217285
Epoch: 69/200 - Train loss: 0.22647473216056824, Validation loss: 0.27711352705955505
Epoch: 70/200 - Train loss: 0.22590097784996033, Validation loss: 0.2765173316001892
Epoch: 71/200 - Train loss: 0.22603438794612885, Validation loss: 0.2779092490673065
Epoch: 72/200 - Train loss: 0.22537322342395782, Validation loss: 0.2763729691505432
Epoch: 73/200 - Train loss: 0.2242475003004074, Validation loss: 0.2770616412162781
Epoch: 74/200 - Train loss: 0.22485791146755219, Validation loss: 0.2818848490715027
Epoch: 75/200 - Train loss: 0.22257980704307556, Validation loss: 0.2742792069911957
Epoch: 76/200 - Train loss: 0.22369590401649475, Validation loss: 0.2757123112678528
Epoch: 77/200 - Train loss: 0.2237408608198166, Validation loss: 0.2716670632362366
Epoch: 78/200 - Train loss: 0.2235998511314392, Validation loss: 0.2769319415092468
Epoch: 79/200 - Train loss: 0.2229365110397339, Validation loss: 0.27620241045951843
Epoch: 80/200 - Train loss: 0.22226077318191528, Validation loss: 0.27414390444755554
Epoch: 81/200 - Train loss: 0.2217341512441635, Validation loss: 0.2807077169418335
Epoch: 82/200 - Train loss: 0.22046388685703278, Validation loss: 0.27485835552215576
Epoch: 83/200 - Train loss: 0.22027894854545593, Validation loss: 0.2717756927013397
Epoch: 84/200 - Train loss: 0.22112254798412323, Validation loss: 0.27180591225624084
Epoch: 85/200 - Train loss: 0.22087500989437103, Validation loss: 0.2827448546886444
Epoch: 86/200 - Train loss: 0.22084712982177734, Validation loss: 0.27745795249938965
Epoch: 87/200 - Train loss: 0.2209145873785019, Validation loss: 0.2736586630344391
Epoch: 88/200 - Train loss: 0.21933503448963165, Validation loss: 0.27466845512390137
Epoch: 89/200 - Train loss: 0.2184925675392151, Validation loss: 0.2804800271987915
Epoch: 90/200 - Train loss: 0.2183264195919037, Validation loss: 0.2745109498500824
Epoch: 91/200 - Train loss: 0.219579815864563, Validation loss: 0.2722780704498291
Epoch: 92/200 - Train loss: 0.2177158147096634, Validation loss: 0.2803976833820343
Epoch: 93/200 - Train loss: 0.21758320927619934, Validation loss: 0.27363261580467224
Epoch: 94/200 - Train loss: 0.2181389480829239, Validation loss: 0.27114054560661316
Epoch: 95/200 - Train loss: 0.2172423005104065, Validation loss: 0.27182531356811523
Epoch: 96/200 - Train loss: 0.21770890057086945, Validation loss: 0.2721605598926544
Epoch: 97/200 - Train loss: 0.2176002413034439, Validation loss: 0.27379876375198364
Epoch: 98/200 - Train loss: 0.2162371277809143, Validation loss: 0.27437207102775574
Epoch: 99/200 - Train loss: 0.21673648059368134, Validation loss: 0.2713223993778229
Epoch: 100/200 - Train loss: 0.2168409526348114, Validation loss: 0.28233277797698975
Epoch: 101/200 - Train loss: 0.2163229137659073, Validation loss: 0.2739156186580658
Epoch: 102/200 - Train loss: 0.21559558808803558, Validation loss: 0.27183279395103455
Epoch: 103/200 - Train loss: 0.21513421833515167, Validation loss: 0.27636465430259705
Epoch: 104/200 - Train loss: 0.21556060016155243, Validation loss: 0.27407628297805786
Epoch: 105/200 - Train loss: 0.21627813577651978, Validation loss: 0.2769140899181366
Epoch: 106/200 - Train loss: 0.21591360867023468, Validation loss: 0.2748303711414337
Epoch: 107/200 - Train loss: 0.21476353704929352, Validation loss: 0.2733980119228363
Epoch: 108/200 - Train loss: 0.21464380621910095, Validation loss: 0.27487677335739136
Epoch: 109/200 - Train loss: 0.2144204080104828, Validation loss: 0.2727442979812622
Epoch: 110/200 - Train loss: 0.21660101413726807, Validation loss: 0.2727438509464264
Epoch: 111/200 - Train loss: 0.2143959403038025, Validation loss: 0.2774311304092407
Epoch: 112/200 - Train loss: 0.2139834314584732, Validation loss: 0.2726117968559265
