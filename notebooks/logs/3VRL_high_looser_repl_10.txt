Epoch: 1/200 - Train loss: 0.5894851684570312, Validation loss: 0.4896266758441925
Epoch: 2/200 - Train loss: 0.4010891318321228, Validation loss: 0.3576832413673401
Epoch: 3/200 - Train loss: 0.3138202428817749, Validation loss: 0.30542677640914917
Epoch: 4/200 - Train loss: 0.270773321390152, Validation loss: 0.279545396566391
Epoch: 5/200 - Train loss: 0.2455919235944748, Validation loss: 0.24925588071346283
Epoch: 6/200 - Train loss: 0.22517408430576324, Validation loss: 0.2360248565673828
Epoch: 7/200 - Train loss: 0.21006391942501068, Validation loss: 0.22296775877475739
Epoch: 8/200 - Train loss: 0.20175126194953918, Validation loss: 0.21178528666496277
Epoch: 9/200 - Train loss: 0.19251927733421326, Validation loss: 0.217400461435318
Epoch: 10/200 - Train loss: 0.18573759496212006, Validation loss: 0.21219751238822937
Epoch: 11/200 - Train loss: 0.18298348784446716, Validation loss: 0.21030202507972717
Epoch: 12/200 - Train loss: 0.17822593450546265, Validation loss: 0.2038782238960266
Epoch: 13/200 - Train loss: 0.18000714480876923, Validation loss: 0.20092718303203583
Epoch: 14/200 - Train loss: 0.1764041781425476, Validation loss: 0.19656553864479065
Epoch: 15/200 - Train loss: 0.1724311262369156, Validation loss: 0.19635428488254547
Epoch: 16/200 - Train loss: 0.16931715607643127, Validation loss: 0.19538143277168274
Epoch: 17/200 - Train loss: 0.16749528050422668, Validation loss: 0.19167910516262054
Epoch: 18/200 - Train loss: 0.16562137007713318, Validation loss: 0.20227377116680145
Epoch: 19/200 - Train loss: 0.16200387477874756, Validation loss: 0.18821342289447784
Epoch: 20/200 - Train loss: 0.16351687908172607, Validation loss: 0.20222622156143188
Epoch: 21/200 - Train loss: 0.16183671355247498, Validation loss: 0.2004661113023758
Epoch: 22/200 - Train loss: 0.15943972766399384, Validation loss: 0.20239503681659698
Epoch: 23/200 - Train loss: 0.15858478844165802, Validation loss: 0.20854449272155762
Epoch: 24/200 - Train loss: 0.15695205330848694, Validation loss: 0.20857977867126465
Epoch: 25/200 - Train loss: 0.15537171065807343, Validation loss: 0.21009159088134766
Epoch: 26/200 - Train loss: 0.15327483415603638, Validation loss: 0.209867924451828
Epoch: 27/200 - Train loss: 0.15560227632522583, Validation loss: 0.20862583816051483
Epoch: 28/200 - Train loss: 0.15160344541072845, Validation loss: 0.21141710877418518
Epoch: 29/200 - Train loss: 0.1526268571615219, Validation loss: 0.2196952849626541
Epoch: 30/200 - Train loss: 0.1489308625459671, Validation loss: 0.22030134499073029
Epoch: 31/200 - Train loss: 0.14759767055511475, Validation loss: 0.21736250817775726
Epoch: 32/200 - Train loss: 0.15082791447639465, Validation loss: 0.22029562294483185
Epoch: 33/200 - Train loss: 0.1462574154138565, Validation loss: 0.22073747217655182
Epoch: 34/200 - Train loss: 0.14833056926727295, Validation loss: 0.20925258100032806
Epoch: 35/200 - Train loss: 0.1472500115633011, Validation loss: 0.22241972386837006
Epoch: 36/200 - Train loss: 0.14604389667510986, Validation loss: 0.2204575389623642
Epoch: 37/200 - Train loss: 0.1459183692932129, Validation loss: 0.21982736885547638
Epoch: 38/200 - Train loss: 0.14456452429294586, Validation loss: 0.22055020928382874
Epoch: 39/200 - Train loss: 0.1429256647825241, Validation loss: 0.22139307856559753
Epoch: 40/200 - Train loss: 0.14285169541835785, Validation loss: 0.21915873885154724
Epoch: 41/200 - Train loss: 0.14239491522312164, Validation loss: 0.23027612268924713
Epoch: 42/200 - Train loss: 0.1408887654542923, Validation loss: 0.22415857017040253
Epoch: 43/200 - Train loss: 0.1410004198551178, Validation loss: 0.2275937795639038
Epoch: 44/200 - Train loss: 0.14115072786808014, Validation loss: 0.2200324386358261
Epoch: 45/200 - Train loss: 0.1389438360929489, Validation loss: 0.2186736762523651
Epoch: 46/200 - Train loss: 0.1416817456483841, Validation loss: 0.2208600491285324
Epoch: 47/200 - Train loss: 0.1374022662639618, Validation loss: 0.22119194269180298
Epoch: 48/200 - Train loss: 0.13988877832889557, Validation loss: 0.22024083137512207
Epoch: 49/200 - Train loss: 0.13948947191238403, Validation loss: 0.22386394441127777
Epoch: 50/200 - Train loss: 0.13985112309455872, Validation loss: 0.22075118124485016
Epoch: 51/200 - Train loss: 0.135944664478302, Validation loss: 0.2357616126537323
Epoch: 52/200 - Train loss: 0.13835442066192627, Validation loss: 0.23512975871562958
Epoch: 53/200 - Train loss: 0.1377135068178177, Validation loss: 0.22141310572624207
Epoch: 54/200 - Train loss: 0.13711132109165192, Validation loss: 0.23308508098125458
Epoch: 55/200 - Train loss: 0.13658805191516876, Validation loss: 0.23503565788269043
Epoch: 56/200 - Train loss: 0.13632290065288544, Validation loss: 0.23295298218727112
Epoch: 57/200 - Train loss: 0.13571421802043915, Validation loss: 0.2352098971605301
Epoch: 58/200 - Train loss: 0.1355409175157547, Validation loss: 0.22154314815998077
Epoch: 59/200 - Train loss: 0.13444779813289642, Validation loss: 0.22125090658664703
Epoch: 60/200 - Train loss: 0.13392426073551178, Validation loss: 0.23149995505809784
Epoch: 61/200 - Train loss: 0.13379471004009247, Validation loss: 0.23654326796531677
Epoch: 62/200 - Train loss: 0.13347889482975006, Validation loss: 0.23466762900352478
Epoch: 63/200 - Train loss: 0.13346141576766968, Validation loss: 0.23594960570335388
Epoch: 64/200 - Train loss: 0.13262727856636047, Validation loss: 0.24629344046115875
Epoch: 65/200 - Train loss: 0.1323837786912918, Validation loss: 0.23456650972366333
Epoch: 66/200 - Train loss: 0.13262708485126495, Validation loss: 0.25130948424339294
Epoch: 67/200 - Train loss: 0.1313502937555313, Validation loss: 0.23679426312446594
Epoch: 68/200 - Train loss: 0.131318137049675, Validation loss: 0.25006407499313354
Epoch: 69/200 - Train loss: 0.13028910756111145, Validation loss: 0.23776309192180634
Epoch: 70/200 - Train loss: 0.13032664358615875, Validation loss: 0.24623265862464905
Epoch: 71/200 - Train loss: 0.12957438826560974, Validation loss: 0.23832473158836365
Epoch: 72/200 - Train loss: 0.1292867213487625, Validation loss: 0.2468780279159546
Epoch: 73/200 - Train loss: 0.12952622771263123, Validation loss: 0.2501087188720703
Epoch: 74/200 - Train loss: 0.12898296117782593, Validation loss: 0.26055479049682617
Epoch: 75/200 - Train loss: 0.12840501964092255, Validation loss: 0.2397894710302353
Epoch: 76/200 - Train loss: 0.12860040366649628, Validation loss: 0.2470993995666504
Epoch: 77/200 - Train loss: 0.12705382704734802, Validation loss: 0.2381601184606552
Epoch: 78/200 - Train loss: 0.1275864690542221, Validation loss: 0.23729564249515533
Epoch: 79/200 - Train loss: 0.1277126520872116, Validation loss: 0.23721899092197418
Epoch: 80/200 - Train loss: 0.1274394690990448, Validation loss: 0.2490045577287674
Epoch: 81/200 - Train loss: 0.12626652419567108, Validation loss: 0.24784870445728302
Epoch: 82/200 - Train loss: 0.12674584984779358, Validation loss: 0.26326799392700195
Epoch: 83/200 - Train loss: 0.12668584287166595, Validation loss: 0.26192790269851685
Epoch: 84/200 - Train loss: 0.1259281486272812, Validation loss: 0.24989202618598938
Epoch: 85/200 - Train loss: 0.12510450184345245, Validation loss: 0.24692010879516602
Epoch: 86/200 - Train loss: 0.12492689490318298, Validation loss: 0.263441264629364
Epoch: 87/200 - Train loss: 0.12580695748329163, Validation loss: 0.2633396089076996
Epoch: 88/200 - Train loss: 0.12434803694486618, Validation loss: 0.2512665092945099
Epoch: 89/200 - Train loss: 0.12419820576906204, Validation loss: 0.24934843182563782
Epoch: 90/200 - Train loss: 0.12477440387010574, Validation loss: 0.2550378143787384
Epoch: 91/200 - Train loss: 0.12462314963340759, Validation loss: 0.2527488172054291
Epoch: 92/200 - Train loss: 0.12411735206842422, Validation loss: 0.2663627564907074
Epoch: 93/200 - Train loss: 0.1244186982512474, Validation loss: 0.268520325422287
Epoch: 94/200 - Train loss: 0.12470198422670364, Validation loss: 0.27306273579597473
Epoch: 95/200 - Train loss: 0.1228286474943161, Validation loss: 0.2646505832672119
Epoch: 96/200 - Train loss: 0.12242813408374786, Validation loss: 0.2649257481098175
Epoch: 97/200 - Train loss: 0.12790779769420624, Validation loss: 0.2538096010684967
Epoch: 98/200 - Train loss: 0.12151621282100677, Validation loss: 0.2535722553730011
Epoch: 99/200 - Train loss: 0.12176663428544998, Validation loss: 0.26860836148262024
Epoch: 100/200 - Train loss: 0.1214127317070961, Validation loss: 0.24659638106822968
Epoch: 101/200 - Train loss: 0.12124843150377274, Validation loss: 0.2695252299308777
Epoch: 102/200 - Train loss: 0.12062574923038483, Validation loss: 0.27128198742866516
Epoch: 103/200 - Train loss: 0.12699168920516968, Validation loss: 0.28342682123184204
Epoch: 104/200 - Train loss: 0.12134622782468796, Validation loss: 0.252589613199234
Epoch: 105/200 - Train loss: 0.1202155202627182, Validation loss: 0.26503056287765503
Epoch: 106/200 - Train loss: 0.1209079846739769, Validation loss: 0.26738405227661133
Epoch: 107/200 - Train loss: 0.12044824659824371, Validation loss: 0.2665789723396301
Epoch: 108/200 - Train loss: 0.12346549332141876, Validation loss: 0.26838600635528564
Epoch: 109/200 - Train loss: 0.12050778418779373, Validation loss: 0.27040427923202515
Epoch: 110/200 - Train loss: 0.11939361691474915, Validation loss: 0.2695220410823822
Epoch: 111/200 - Train loss: 0.12010973691940308, Validation loss: 0.26707392930984497
Epoch: 112/200 - Train loss: 0.12037985026836395, Validation loss: 0.25766587257385254
Epoch: 113/200 - Train loss: 0.11955784261226654, Validation loss: 0.2666832506656647
Epoch: 114/200 - Train loss: 0.11908169835805893, Validation loss: 0.27144986391067505
Epoch: 115/200 - Train loss: 0.11882767826318741, Validation loss: 0.2558137774467468
Epoch: 116/200 - Train loss: 0.11909276992082596, Validation loss: 0.25614747405052185
Epoch: 117/200 - Train loss: 0.11802977323532104, Validation loss: 0.27061358094215393
Epoch: 118/200 - Train loss: 0.11848732829093933, Validation loss: 0.2704090178012848
Epoch: 119/200 - Train loss: 0.11818458884954453, Validation loss: 0.2790903151035309
Epoch: 120/200 - Train loss: 0.11940199136734009, Validation loss: 0.2692949175834656
Epoch: 121/200 - Train loss: 0.1186821386218071, Validation loss: 0.2712657153606415
Epoch: 122/200 - Train loss: 0.11828790605068207, Validation loss: 0.2632668614387512
Epoch: 123/200 - Train loss: 0.11705179512500763, Validation loss: 0.27225571870803833
Epoch: 124/200 - Train loss: 0.11770538240671158, Validation loss: 0.2749427258968353
Epoch: 125/200 - Train loss: 0.11829603463411331, Validation loss: 0.27351632714271545
Epoch: 126/200 - Train loss: 0.11771970987319946, Validation loss: 0.270401269197464
Epoch: 127/200 - Train loss: 0.1177903488278389, Validation loss: 0.27552124857902527
Epoch: 128/200 - Train loss: 0.11763308197259903, Validation loss: 0.27602604031562805
Epoch: 129/200 - Train loss: 0.11709419637918472, Validation loss: 0.27374497056007385
Epoch: 130/200 - Train loss: 0.11743830144405365, Validation loss: 0.2748313248157501
Epoch: 131/200 - Train loss: 0.11645635217428207, Validation loss: 0.27622535824775696
Epoch: 132/200 - Train loss: 0.11929179728031158, Validation loss: 0.26212677359580994
Epoch: 133/200 - Train loss: 0.11662717163562775, Validation loss: 0.2725849747657776
Epoch: 134/200 - Train loss: 0.11611374467611313, Validation loss: 0.27463486790657043
Epoch: 135/200 - Train loss: 0.11530082672834396, Validation loss: 0.2779347002506256
Epoch: 136/200 - Train loss: 0.11894869804382324, Validation loss: 0.277871698141098
Epoch: 137/200 - Train loss: 0.11655900627374649, Validation loss: 0.27346694469451904
Epoch: 138/200 - Train loss: 0.11609036475419998, Validation loss: 0.2735402584075928
Epoch: 139/200 - Train loss: 0.11637376248836517, Validation loss: 0.27433711290359497
Epoch: 140/200 - Train loss: 0.11510005593299866, Validation loss: 0.2763289213180542
Epoch: 141/200 - Train loss: 0.11532740294933319, Validation loss: 0.270871639251709
Epoch: 142/200 - Train loss: 0.11856656521558762, Validation loss: 0.2859915494918823
Epoch: 143/200 - Train loss: 0.11841142177581787, Validation loss: 0.2763713300228119
Epoch: 144/200 - Train loss: 0.11483484506607056, Validation loss: 0.27535688877105713
Epoch: 145/200 - Train loss: 0.11769797652959824, Validation loss: 0.27416667342185974
Epoch: 146/200 - Train loss: 0.11479142308235168, Validation loss: 0.2731286287307739
Epoch: 147/200 - Train loss: 0.11690796166658401, Validation loss: 0.26455309987068176
Epoch: 148/200 - Train loss: 0.11787644773721695, Validation loss: 0.27967458963394165
Epoch: 149/200 - Train loss: 0.11481588333845139, Validation loss: 0.27448946237564087
Epoch: 150/200 - Train loss: 0.11375387758016586, Validation loss: 0.2786579430103302
Epoch: 151/200 - Train loss: 0.1169886663556099, Validation loss: 0.26580625772476196
Epoch: 152/200 - Train loss: 0.11427070945501328, Validation loss: 0.29893916845321655
Epoch: 153/200 - Train loss: 0.11458585411310196, Validation loss: 0.2838512063026428
Epoch: 154/200 - Train loss: 0.11733964085578918, Validation loss: 0.27632656693458557
Epoch: 155/200 - Train loss: 0.11426498740911484, Validation loss: 0.2649365961551666
Epoch: 156/200 - Train loss: 0.11388319730758667, Validation loss: 0.2759886384010315
Epoch: 157/200 - Train loss: 0.11360494047403336, Validation loss: 0.2801112234592438
Epoch: 158/200 - Train loss: 0.11353537440299988, Validation loss: 0.2643611431121826
Epoch: 159/200 - Train loss: 0.11636865884065628, Validation loss: 0.2778985798358917
Epoch: 160/200 - Train loss: 0.11278285086154938, Validation loss: 0.2646971046924591
Epoch: 161/200 - Train loss: 0.11321301013231277, Validation loss: 0.27854496240615845
Epoch: 162/200 - Train loss: 0.11612588167190552, Validation loss: 0.2766464054584503
Epoch: 163/200 - Train loss: 0.11206993460655212, Validation loss: 0.2700224816799164
Epoch: 164/200 - Train loss: 0.1160966232419014, Validation loss: 0.2676447033882141
Epoch: 165/200 - Train loss: 0.11247903853654861, Validation loss: 0.2920171618461609
Epoch: 166/200 - Train loss: 0.11217135190963745, Validation loss: 0.2796648144721985
Epoch: 167/200 - Train loss: 0.11617864668369293, Validation loss: 0.26674404740333557
Epoch: 168/200 - Train loss: 0.1133565604686737, Validation loss: 0.2711925804615021
Epoch: 169/200 - Train loss: 0.11549396812915802, Validation loss: 0.2918955087661743
Epoch: 170/200 - Train loss: 0.11497689038515091, Validation loss: 0.27010369300842285
Epoch: 171/200 - Train loss: 0.11508418619632721, Validation loss: 0.2948712110519409
Epoch: 172/200 - Train loss: 0.11474728584289551, Validation loss: 0.274300754070282
Epoch: 173/200 - Train loss: 0.11518562585115433, Validation loss: 0.26993051171302795
Epoch: 174/200 - Train loss: 0.111717090010643, Validation loss: 0.2812654376029968
Epoch: 175/200 - Train loss: 0.11518798768520355, Validation loss: 0.26727014780044556
Epoch: 176/200 - Train loss: 0.11120948940515518, Validation loss: 0.2672162652015686
Epoch: 177/200 - Train loss: 0.11462780833244324, Validation loss: 0.2684227228164673
Epoch: 178/200 - Train loss: 0.11520979553461075, Validation loss: 0.26595157384872437
Epoch: 179/200 - Train loss: 0.11431462317705154, Validation loss: 0.2934626638889313
Epoch: 180/200 - Train loss: 0.11381787061691284, Validation loss: 0.2888326644897461
Epoch: 181/200 - Train loss: 0.11741205304861069, Validation loss: 0.2699609398841858
Epoch: 182/200 - Train loss: 0.11116886883974075, Validation loss: 0.2679431140422821
Epoch: 183/200 - Train loss: 0.11370537430047989, Validation loss: 0.2884312570095062
Epoch: 184/200 - Train loss: 0.11105415225028992, Validation loss: 0.27059122920036316
Epoch: 185/200 - Train loss: 0.11350937932729721, Validation loss: 0.2806285321712494
Epoch: 186/200 - Train loss: 0.11676033586263657, Validation loss: 0.27165788412094116
Epoch: 187/200 - Train loss: 0.11381786316633224, Validation loss: 0.272811621427536
Epoch: 188/200 - Train loss: 0.11269243061542511, Validation loss: 0.2856615483760834
Epoch: 189/200 - Train loss: 0.11327225714921951, Validation loss: 0.27700138092041016
Epoch: 190/200 - Train loss: 0.11326999962329865, Validation loss: 0.28756988048553467
Epoch: 191/200 - Train loss: 0.11292210221290588, Validation loss: 0.28587010502815247
Epoch: 192/200 - Train loss: 0.11307255923748016, Validation loss: 0.2854042947292328
Epoch: 193/200 - Train loss: 0.11332116276025772, Validation loss: 0.2714744210243225
Epoch: 194/200 - Train loss: 0.11311091482639313, Validation loss: 0.2724628448486328
Epoch: 195/200 - Train loss: 0.11312665790319443, Validation loss: 0.28398630023002625
Epoch: 196/200 - Train loss: 0.112854965031147, Validation loss: 0.28617650270462036
Epoch: 197/200 - Train loss: 0.11007369309663773, Validation loss: 0.2976521849632263
Epoch: 198/200 - Train loss: 0.11250868439674377, Validation loss: 0.2872580885887146
Epoch: 199/200 - Train loss: 0.11344777792692184, Validation loss: 0.2724025547504425
Epoch: 200/200 - Train loss: 0.11314572393894196, Validation loss: 0.2743622064590454
