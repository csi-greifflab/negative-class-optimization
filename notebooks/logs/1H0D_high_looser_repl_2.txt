Epoch: 1/200 - Train loss: 0.6351298689842224, Validation loss: 0.557443380355835
Epoch: 2/200 - Train loss: 0.510599672794342, Validation loss: 0.47557520866394043
Epoch: 3/200 - Train loss: 0.45115596055984497, Validation loss: 0.4303227663040161
Epoch: 4/200 - Train loss: 0.4100571274757385, Validation loss: 0.400532603263855
Epoch: 5/200 - Train loss: 0.3817094564437866, Validation loss: 0.3764123022556305
Epoch: 6/200 - Train loss: 0.36020469665527344, Validation loss: 0.3598574101924896
Epoch: 7/200 - Train loss: 0.3443048298358917, Validation loss: 0.3466801941394806
Epoch: 8/200 - Train loss: 0.3303011953830719, Validation loss: 0.3415609300136566
Epoch: 9/200 - Train loss: 0.3174407184123993, Validation loss: 0.3311339020729065
Epoch: 10/200 - Train loss: 0.3073125183582306, Validation loss: 0.3226046562194824
Epoch: 11/200 - Train loss: 0.29908883571624756, Validation loss: 0.3188536465167999
Epoch: 12/200 - Train loss: 0.2902277708053589, Validation loss: 0.312127023935318
Epoch: 13/200 - Train loss: 0.28243669867515564, Validation loss: 0.3101457953453064
Epoch: 14/200 - Train loss: 0.2756296694278717, Validation loss: 0.30774804949760437
Epoch: 15/200 - Train loss: 0.26967981457710266, Validation loss: 0.30242720246315
Epoch: 16/200 - Train loss: 0.2641320526599884, Validation loss: 0.29708385467529297
Epoch: 17/200 - Train loss: 0.2590677738189697, Validation loss: 0.29774343967437744
Epoch: 18/200 - Train loss: 0.254024475812912, Validation loss: 0.2921527922153473
Epoch: 19/200 - Train loss: 0.24953940510749817, Validation loss: 0.2858763337135315
Epoch: 20/200 - Train loss: 0.24547219276428223, Validation loss: 0.28223055601119995
Epoch: 21/200 - Train loss: 0.24226489663124084, Validation loss: 0.28346529603004456
Epoch: 22/200 - Train loss: 0.23897893726825714, Validation loss: 0.2826787233352661
Epoch: 23/200 - Train loss: 0.23531579971313477, Validation loss: 0.2816079258918762
Epoch: 24/200 - Train loss: 0.23223252594470978, Validation loss: 0.2838844656944275
Epoch: 25/200 - Train loss: 0.22938312590122223, Validation loss: 0.27606597542762756
Epoch: 26/200 - Train loss: 0.2268347144126892, Validation loss: 0.2773922383785248
Epoch: 27/200 - Train loss: 0.22450926899909973, Validation loss: 0.2763223350048065
Epoch: 28/200 - Train loss: 0.22341981530189514, Validation loss: 0.2784040868282318
Epoch: 29/200 - Train loss: 0.2206261157989502, Validation loss: 0.27716636657714844
Epoch: 30/200 - Train loss: 0.2193371206521988, Validation loss: 0.2772105038166046
Epoch: 31/200 - Train loss: 0.2169199436903, Validation loss: 0.2749771177768707
Epoch: 32/200 - Train loss: 0.21587270498275757, Validation loss: 0.2781509459018707
Epoch: 33/200 - Train loss: 0.21484416723251343, Validation loss: 0.28021296858787537
Epoch: 34/200 - Train loss: 0.2123306542634964, Validation loss: 0.27835479378700256
Epoch: 35/200 - Train loss: 0.21042999625205994, Validation loss: 0.27473706007003784
Epoch: 36/200 - Train loss: 0.21046386659145355, Validation loss: 0.277545690536499
Epoch: 37/200 - Train loss: 0.2087274193763733, Validation loss: 0.2745347023010254
Epoch: 38/200 - Train loss: 0.2069457769393921, Validation loss: 0.27484187483787537
Epoch: 39/200 - Train loss: 0.20665982365608215, Validation loss: 0.2767285704612732
Epoch: 40/200 - Train loss: 0.20605410635471344, Validation loss: 0.27781563997268677
Epoch: 41/200 - Train loss: 0.20382465422153473, Validation loss: 0.2762511670589447
Epoch: 42/200 - Train loss: 0.20321287214756012, Validation loss: 0.27377137541770935
Epoch: 43/200 - Train loss: 0.20248185098171234, Validation loss: 0.27640655636787415
