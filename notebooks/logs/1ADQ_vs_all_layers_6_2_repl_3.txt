Epoch: 1/200 - Train loss: 0.7075514793395996, Validation loss: 0.6950241923332214
Epoch: 2/200 - Train loss: 0.6940537095069885, Validation loss: 0.6930910348892212
Epoch: 3/200 - Train loss: 0.6931785345077515, Validation loss: 0.6932520270347595
Epoch: 4/200 - Train loss: 0.6931957006454468, Validation loss: 0.6931802034378052
Epoch: 5/200 - Train loss: 0.6931732296943665, Validation loss: 0.6931957602500916
Epoch: 6/200 - Train loss: 0.6931867003440857, Validation loss: 0.6931458711624146
Epoch: 7/200 - Train loss: 0.6931893825531006, Validation loss: 0.6931734681129456
Epoch: 8/200 - Train loss: 0.6931931376457214, Validation loss: 0.6931671500205994
Epoch: 9/200 - Train loss: 0.6931683421134949, Validation loss: 0.6932727098464966
Epoch: 10/200 - Train loss: 0.693190336227417, Validation loss: 0.6933026909828186
Epoch: 1/200 - Train loss: 0.6899062395095825, Validation loss: 0.5999830365180969
Epoch: 2/200 - Train loss: 0.5584273338317871, Validation loss: 0.5259022116661072
Epoch: 3/200 - Train loss: 0.496288001537323, Validation loss: 0.48129719495773315
Epoch: 4/200 - Train loss: 0.4568350613117218, Validation loss: 0.4527374804019928
Epoch: 5/200 - Train loss: 0.4288334846496582, Validation loss: 0.42570894956588745
Epoch: 6/200 - Train loss: 0.40161067247390747, Validation loss: 0.4041615128517151
Epoch: 7/200 - Train loss: 0.37261107563972473, Validation loss: 0.3722679018974304
Epoch: 8/200 - Train loss: 0.3427521288394928, Validation loss: 0.3513833284378052
Epoch: 9/200 - Train loss: 0.31692954897880554, Validation loss: 0.3314304053783417
Epoch: 10/200 - Train loss: 0.2966715395450592, Validation loss: 0.3142232894897461
Epoch: 11/200 - Train loss: 0.2827054560184479, Validation loss: 0.3061332106590271
Epoch: 12/200 - Train loss: 0.2733624279499054, Validation loss: 0.3017919361591339
Epoch: 13/200 - Train loss: 0.26509249210357666, Validation loss: 0.29709720611572266
Epoch: 14/200 - Train loss: 0.25842198729515076, Validation loss: 0.295231431722641
Epoch: 15/200 - Train loss: 0.2547203004360199, Validation loss: 0.29825136065483093
Epoch: 16/200 - Train loss: 0.24991244077682495, Validation loss: 0.29013383388519287
Epoch: 17/200 - Train loss: 0.24761535227298737, Validation loss: 0.2886701822280884
Epoch: 18/200 - Train loss: 0.2439289391040802, Validation loss: 0.286917120218277
Epoch: 19/200 - Train loss: 0.24171847105026245, Validation loss: 0.2837483584880829
Epoch: 20/200 - Train loss: 0.23982493579387665, Validation loss: 0.29027923941612244
Epoch: 21/200 - Train loss: 0.2374296635389328, Validation loss: 0.2813989520072937
Epoch: 22/200 - Train loss: 0.23520763218402863, Validation loss: 0.2805737257003784
Epoch: 23/200 - Train loss: 0.23577450215816498, Validation loss: 0.2797384262084961
Epoch: 24/200 - Train loss: 0.23343095183372498, Validation loss: 0.2826361358165741
Epoch: 25/200 - Train loss: 0.23275789618492126, Validation loss: 0.2815990149974823
Epoch: 26/200 - Train loss: 0.22981205582618713, Validation loss: 0.2775384485721588
Epoch: 27/200 - Train loss: 0.22896727919578552, Validation loss: 0.2784509062767029
Epoch: 28/200 - Train loss: 0.22863376140594482, Validation loss: 0.2813293933868408
Epoch: 29/200 - Train loss: 0.22808580100536346, Validation loss: 0.2825498878955841
Epoch: 30/200 - Train loss: 0.22644853591918945, Validation loss: 0.2792840600013733
Epoch: 31/200 - Train loss: 0.22572821378707886, Validation loss: 0.27407869696617126
Epoch: 32/200 - Train loss: 0.22631120681762695, Validation loss: 0.27864477038383484
Epoch: 33/200 - Train loss: 0.22469110786914825, Validation loss: 0.27706408500671387
Epoch: 34/200 - Train loss: 0.22387072443962097, Validation loss: 0.2768348157405853
Epoch: 35/200 - Train loss: 0.22325299680233002, Validation loss: 0.2812422811985016
Epoch: 36/200 - Train loss: 0.22491838037967682, Validation loss: 0.27504274249076843
Epoch: 37/200 - Train loss: 0.22266270220279694, Validation loss: 0.2794501483440399
Epoch: 38/200 - Train loss: 0.22303414344787598, Validation loss: 0.27802008390426636
Epoch: 39/200 - Train loss: 0.22176136076450348, Validation loss: 0.2776931822299957
Epoch: 40/200 - Train loss: 0.2218339741230011, Validation loss: 0.2758074402809143
Epoch: 41/200 - Train loss: 0.2218797355890274, Validation loss: 0.27839142084121704
Epoch: 42/200 - Train loss: 0.22110481560230255, Validation loss: 0.2785724997520447
Epoch: 43/200 - Train loss: 0.22145409882068634, Validation loss: 0.2734355628490448
Epoch: 44/200 - Train loss: 0.22077350318431854, Validation loss: 0.27535876631736755
Epoch: 45/200 - Train loss: 0.21969756484031677, Validation loss: 0.28196972608566284
Epoch: 46/200 - Train loss: 0.2190961241722107, Validation loss: 0.28181686997413635
Epoch: 47/200 - Train loss: 0.21878595650196075, Validation loss: 0.2736312448978424
Epoch: 48/200 - Train loss: 0.21869224309921265, Validation loss: 0.27853450179100037
Epoch: 49/200 - Train loss: 0.21952912211418152, Validation loss: 0.28123193979263306
Epoch: 50/200 - Train loss: 0.2189706414937973, Validation loss: 0.2871561050415039
Epoch: 51/200 - Train loss: 0.218015655875206, Validation loss: 0.27945375442504883
Epoch: 52/200 - Train loss: 0.2180415242910385, Validation loss: 0.27488112449645996
Epoch: 53/200 - Train loss: 0.21805858612060547, Validation loss: 0.2756587862968445
Epoch: 54/200 - Train loss: 0.21863405406475067, Validation loss: 0.27832409739494324
Epoch: 55/200 - Train loss: 0.21830449998378754, Validation loss: 0.281828910112381
Epoch: 56/200 - Train loss: 0.21823528409004211, Validation loss: 0.2784583568572998
Epoch: 57/200 - Train loss: 0.21703742444515228, Validation loss: 0.28134214878082275
Epoch: 58/200 - Train loss: 0.21662184596061707, Validation loss: 0.2770182192325592
Epoch: 59/200 - Train loss: 0.21783466637134552, Validation loss: 0.27957916259765625
Epoch: 60/200 - Train loss: 0.21680514514446259, Validation loss: 0.27822551131248474
Epoch: 61/200 - Train loss: 0.21590198576450348, Validation loss: 0.2776681184768677
Epoch: 62/200 - Train loss: 0.21574771404266357, Validation loss: 0.27688926458358765
Epoch: 63/200 - Train loss: 0.21611995995044708, Validation loss: 0.2766401469707489
Epoch: 64/200 - Train loss: 0.2167622298002243, Validation loss: 0.27693408727645874
Epoch: 65/200 - Train loss: 0.2150551825761795, Validation loss: 0.27359822392463684
Epoch: 66/200 - Train loss: 0.21491077542304993, Validation loss: 0.27739760279655457
Epoch: 67/200 - Train loss: 0.21468280255794525, Validation loss: 0.2796317934989929
Epoch: 68/200 - Train loss: 0.21509107947349548, Validation loss: 0.27496078610420227
Epoch: 69/200 - Train loss: 0.21461959183216095, Validation loss: 0.27452948689460754
Epoch: 70/200 - Train loss: 0.2140737920999527, Validation loss: 0.27588924765586853
Epoch: 71/200 - Train loss: 0.21488924324512482, Validation loss: 0.2766752243041992
Epoch: 72/200 - Train loss: 0.21374015510082245, Validation loss: 0.2827450931072235
Epoch: 73/200 - Train loss: 0.21403956413269043, Validation loss: 0.27766966819763184
Epoch: 74/200 - Train loss: 0.2140188068151474, Validation loss: 0.27699390053749084
Epoch: 75/200 - Train loss: 0.21449892222881317, Validation loss: 0.27614033222198486
Epoch: 76/200 - Train loss: 0.21346774697303772, Validation loss: 0.27548837661743164
Epoch: 77/200 - Train loss: 0.21279874444007874, Validation loss: 0.27401599287986755
Epoch: 78/200 - Train loss: 0.21476083993911743, Validation loss: 0.27282750606536865
Epoch: 79/200 - Train loss: 0.21485334634780884, Validation loss: 0.27526047825813293
Epoch: 80/200 - Train loss: 0.21400433778762817, Validation loss: 0.27618318796157837
Epoch: 81/200 - Train loss: 0.21328559517860413, Validation loss: 0.2758466899394989
Epoch: 82/200 - Train loss: 0.21285434067249298, Validation loss: 0.27621325850486755
