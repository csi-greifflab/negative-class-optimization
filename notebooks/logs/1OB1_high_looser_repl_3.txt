Epoch: 1/200 - Train loss: 0.6108359694480896, Validation loss: 0.5416053533554077
Epoch: 2/200 - Train loss: 0.49832242727279663, Validation loss: 0.47950008511543274
Epoch: 3/200 - Train loss: 0.44602665305137634, Validation loss: 0.43962571024894714
Epoch: 4/200 - Train loss: 0.41454246640205383, Validation loss: 0.4142288863658905
Epoch: 5/200 - Train loss: 0.387828528881073, Validation loss: 0.3944457471370697
Epoch: 6/200 - Train loss: 0.36686819791793823, Validation loss: 0.3750155568122864
Epoch: 7/200 - Train loss: 0.34943950176239014, Validation loss: 0.36068853735923767
Epoch: 8/200 - Train loss: 0.3327252268791199, Validation loss: 0.3453683853149414
Epoch: 9/200 - Train loss: 0.31829211115837097, Validation loss: 0.33052629232406616
Epoch: 10/200 - Train loss: 0.30389177799224854, Validation loss: 0.3183635175228119
Epoch: 11/200 - Train loss: 0.29008302092552185, Validation loss: 0.30537816882133484
Epoch: 12/200 - Train loss: 0.27866142988204956, Validation loss: 0.29578152298927307
Epoch: 13/200 - Train loss: 0.26759201288223267, Validation loss: 0.2900853157043457
Epoch: 14/200 - Train loss: 0.2584554851055145, Validation loss: 0.2804360091686249
Epoch: 15/200 - Train loss: 0.25044554471969604, Validation loss: 0.2753094732761383
Epoch: 16/200 - Train loss: 0.24398864805698395, Validation loss: 0.26989415287971497
Epoch: 17/200 - Train loss: 0.23825658857822418, Validation loss: 0.2730633318424225
Epoch: 18/200 - Train loss: 0.2326994240283966, Validation loss: 0.2646787762641907
Epoch: 19/200 - Train loss: 0.2279205322265625, Validation loss: 0.2580333948135376
Epoch: 20/200 - Train loss: 0.223992720246315, Validation loss: 0.26080459356307983
Epoch: 21/200 - Train loss: 0.22015172243118286, Validation loss: 0.25882160663604736
Epoch: 22/200 - Train loss: 0.21626146137714386, Validation loss: 0.25407469272613525
Epoch: 23/200 - Train loss: 0.21442864835262299, Validation loss: 0.2532769739627838
Epoch: 24/200 - Train loss: 0.2101912647485733, Validation loss: 0.24959664046764374
Epoch: 25/200 - Train loss: 0.20842452347278595, Validation loss: 0.24692001938819885
Epoch: 26/200 - Train loss: 0.20589807629585266, Validation loss: 0.24955733120441437
Epoch: 27/200 - Train loss: 0.20316486060619354, Validation loss: 0.24742378294467926
Epoch: 28/200 - Train loss: 0.2007289081811905, Validation loss: 0.24648146331310272
Epoch: 29/200 - Train loss: 0.1990690976381302, Validation loss: 0.24385491013526917
Epoch: 30/200 - Train loss: 0.19730301201343536, Validation loss: 0.24468792974948883
Epoch: 31/200 - Train loss: 0.19516046345233917, Validation loss: 0.24237008392810822
Epoch: 32/200 - Train loss: 0.1936272382736206, Validation loss: 0.24363702535629272
Epoch: 33/200 - Train loss: 0.1923104077577591, Validation loss: 0.24019651114940643
Epoch: 34/200 - Train loss: 0.18987706303596497, Validation loss: 0.23823055624961853
Epoch: 35/200 - Train loss: 0.18754541873931885, Validation loss: 0.24139437079429626
Epoch: 36/200 - Train loss: 0.18671150505542755, Validation loss: 0.24037784337997437
Epoch: 37/200 - Train loss: 0.1852630376815796, Validation loss: 0.2408289909362793
Epoch: 38/200 - Train loss: 0.1842799335718155, Validation loss: 0.23951873183250427
Epoch: 39/200 - Train loss: 0.18277348577976227, Validation loss: 0.23833662271499634
Epoch: 40/200 - Train loss: 0.18187865614891052, Validation loss: 0.24212297797203064
Epoch: 41/200 - Train loss: 0.1802070289850235, Validation loss: 0.24129530787467957
Epoch: 42/200 - Train loss: 0.1788424700498581, Validation loss: 0.23596401512622833
Epoch: 43/200 - Train loss: 0.17795412242412567, Validation loss: 0.23959551751613617
Epoch: 44/200 - Train loss: 0.1773378700017929, Validation loss: 0.23602883517742157
Epoch: 45/200 - Train loss: 0.17585639655590057, Validation loss: 0.2358981966972351
Epoch: 46/200 - Train loss: 0.17491669952869415, Validation loss: 0.23389868438243866
Epoch: 47/200 - Train loss: 0.17399147152900696, Validation loss: 0.23762008547782898
Epoch: 48/200 - Train loss: 0.1729213148355484, Validation loss: 0.23831014335155487
Epoch: 49/200 - Train loss: 0.17193478345870972, Validation loss: 0.23987874388694763
Epoch: 50/200 - Train loss: 0.17089903354644775, Validation loss: 0.2416718304157257
Epoch: 51/200 - Train loss: 0.17017632722854614, Validation loss: 0.2358136922121048
Epoch: 52/200 - Train loss: 0.16980823874473572, Validation loss: 0.23981653153896332
Epoch: 53/200 - Train loss: 0.1691429167985916, Validation loss: 0.23436225950717926
Epoch: 54/200 - Train loss: 0.16761870682239532, Validation loss: 0.23884424567222595
Epoch: 55/200 - Train loss: 0.16754178702831268, Validation loss: 0.23774558305740356
Epoch: 56/200 - Train loss: 0.16720043122768402, Validation loss: 0.2383909374475479
Epoch: 57/200 - Train loss: 0.1661035120487213, Validation loss: 0.23591090738773346
Epoch: 58/200 - Train loss: 0.16514115035533905, Validation loss: 0.2409505397081375
Epoch: 59/200 - Train loss: 0.16437982022762299, Validation loss: 0.2427639663219452
Epoch: 60/200 - Train loss: 0.16461032629013062, Validation loss: 0.23781253397464752
Epoch: 61/200 - Train loss: 0.16355788707733154, Validation loss: 0.2391834259033203
Epoch: 62/200 - Train loss: 0.16226695477962494, Validation loss: 0.23980151116847992
Epoch: 63/200 - Train loss: 0.16273654997348785, Validation loss: 0.24005557596683502
Epoch: 64/200 - Train loss: 0.1618805229663849, Validation loss: 0.2398897260427475
Epoch: 65/200 - Train loss: 0.16114811599254608, Validation loss: 0.23878155648708344
Epoch: 66/200 - Train loss: 0.16173982620239258, Validation loss: 0.24192075431346893
Epoch: 67/200 - Train loss: 0.16027739644050598, Validation loss: 0.24283337593078613
Epoch: 68/200 - Train loss: 0.1602395623922348, Validation loss: 0.24209348857402802
Epoch: 69/200 - Train loss: 0.15945832431316376, Validation loss: 0.2383493334054947
Epoch: 70/200 - Train loss: 0.1600879728794098, Validation loss: 0.2442319244146347
Epoch: 71/200 - Train loss: 0.15909428894519806, Validation loss: 0.24223822355270386
Epoch: 72/200 - Train loss: 0.15954437851905823, Validation loss: 0.24744993448257446
Epoch: 73/200 - Train loss: 0.15732300281524658, Validation loss: 0.24158892035484314
Epoch: 74/200 - Train loss: 0.15757954120635986, Validation loss: 0.24904745817184448
Epoch: 75/200 - Train loss: 0.15767276287078857, Validation loss: 0.2413461059331894
Epoch: 76/200 - Train loss: 0.15626877546310425, Validation loss: 0.24387401342391968
Epoch: 77/200 - Train loss: 0.15633614361286163, Validation loss: 0.2425689697265625
Epoch: 78/200 - Train loss: 0.1562333106994629, Validation loss: 0.24692098796367645
Epoch: 79/200 - Train loss: 0.1559913009405136, Validation loss: 0.24528005719184875
Epoch: 80/200 - Train loss: 0.15626393258571625, Validation loss: 0.24490121006965637
Epoch: 81/200 - Train loss: 0.15629012882709503, Validation loss: 0.2476484328508377
Epoch: 82/200 - Train loss: 0.1551850289106369, Validation loss: 0.247173473238945
Epoch: 83/200 - Train loss: 0.15413427352905273, Validation loss: 0.25009098649024963
Epoch: 84/200 - Train loss: 0.15477849543094635, Validation loss: 0.2494020313024521
Epoch: 85/200 - Train loss: 0.15411995351314545, Validation loss: 0.24851378798484802
Epoch: 86/200 - Train loss: 0.15346986055374146, Validation loss: 0.24865567684173584
Epoch: 87/200 - Train loss: 0.1544177532196045, Validation loss: 0.24835939705371857
Epoch: 88/200 - Train loss: 0.15350688993930817, Validation loss: 0.24643480777740479
Epoch: 89/200 - Train loss: 0.15320943295955658, Validation loss: 0.25052782893180847
Epoch: 90/200 - Train loss: 0.15257011353969574, Validation loss: 0.25098466873168945
