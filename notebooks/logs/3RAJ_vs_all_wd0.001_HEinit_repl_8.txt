Epoch: 1/300 - Train loss: 0.7251175045967102, Validation loss: 0.7233681082725525
Epoch: 2/300 - Train loss: 0.720465362071991, Validation loss: 0.7184642553329468
Epoch: 3/300 - Train loss: 0.7162172794342041, Validation loss: 0.7147064805030823
Epoch: 4/300 - Train loss: 0.7123476266860962, Validation loss: 0.7110709547996521
Epoch: 5/300 - Train loss: 0.7088313102722168, Validation loss: 0.7073871493339539
Epoch: 6/300 - Train loss: 0.705630898475647, Validation loss: 0.7045451402664185
Epoch: 7/300 - Train loss: 0.7027127742767334, Validation loss: 0.7018513679504395
Epoch: 8/300 - Train loss: 0.7000415325164795, Validation loss: 0.6993945837020874
Epoch: 9/300 - Train loss: 0.6975861191749573, Validation loss: 0.6968225836753845
Epoch: 10/300 - Train loss: 0.6953136324882507, Validation loss: 0.6946483254432678
Epoch: 11/300 - Train loss: 0.6931973695755005, Validation loss: 0.6925941109657288
Epoch: 12/300 - Train loss: 0.6912127733230591, Validation loss: 0.6906934380531311
Epoch: 13/300 - Train loss: 0.6893365383148193, Validation loss: 0.6889933943748474
Epoch: 14/300 - Train loss: 0.6875420808792114, Validation loss: 0.6870178580284119
Epoch: 15/300 - Train loss: 0.6858119368553162, Validation loss: 0.685420572757721
Epoch: 16/300 - Train loss: 0.6841293573379517, Validation loss: 0.6836130619049072
Epoch: 17/300 - Train loss: 0.6824762225151062, Validation loss: 0.6817625164985657
Epoch: 18/300 - Train loss: 0.6808390021324158, Validation loss: 0.6799672245979309
Epoch: 19/300 - Train loss: 0.6792057752609253, Validation loss: 0.6783040165901184
Epoch: 20/300 - Train loss: 0.6775675415992737, Validation loss: 0.676693856716156
Epoch: 21/300 - Train loss: 0.6759128570556641, Validation loss: 0.6749637722969055
Epoch: 22/300 - Train loss: 0.6742358803749084, Validation loss: 0.673241376876831
Epoch: 23/300 - Train loss: 0.6725270748138428, Validation loss: 0.6712750792503357
Epoch: 24/300 - Train loss: 0.6707838177680969, Validation loss: 0.6696230173110962
Epoch: 25/300 - Train loss: 0.6690038442611694, Validation loss: 0.6675557494163513
Epoch: 26/300 - Train loss: 0.6671865582466125, Validation loss: 0.6658738255500793
Epoch: 27/300 - Train loss: 0.6653292179107666, Validation loss: 0.663794994354248
Epoch: 28/300 - Train loss: 0.6634320616722107, Validation loss: 0.6618368625640869
Epoch: 29/300 - Train loss: 0.6614929437637329, Validation loss: 0.6600714325904846
Epoch: 30/300 - Train loss: 0.6595156788825989, Validation loss: 0.65802401304245
Epoch: 31/300 - Train loss: 0.657504141330719, Validation loss: 0.6556733846664429
Epoch: 32/300 - Train loss: 0.6554589867591858, Validation loss: 0.6538807153701782
Epoch: 33/300 - Train loss: 0.6533879041671753, Validation loss: 0.6514630913734436
Epoch: 34/300 - Train loss: 0.6512961387634277, Validation loss: 0.6495658159255981
Epoch: 35/300 - Train loss: 0.6491838693618774, Validation loss: 0.6473252773284912
Epoch: 36/300 - Train loss: 0.647057294845581, Validation loss: 0.645202100276947
Epoch: 37/300 - Train loss: 0.6449243426322937, Validation loss: 0.6429532170295715
Epoch: 38/300 - Train loss: 0.6427871584892273, Validation loss: 0.6406341791152954
Epoch: 39/300 - Train loss: 0.64064621925354, Validation loss: 0.6386638283729553
Epoch: 40/300 - Train loss: 0.6385010480880737, Validation loss: 0.6364056468009949
Epoch: 41/300 - Train loss: 0.6363537311553955, Validation loss: 0.6340776085853577
Epoch: 42/300 - Train loss: 0.6342048048973083, Validation loss: 0.6319360136985779
Epoch: 43/300 - Train loss: 0.6320541501045227, Validation loss: 0.6301112174987793
Epoch: 44/300 - Train loss: 0.6299005150794983, Validation loss: 0.6276354789733887
Epoch: 45/300 - Train loss: 0.6277427673339844, Validation loss: 0.6252428889274597
Epoch: 46/300 - Train loss: 0.6255795955657959, Validation loss: 0.6231286525726318
Epoch: 47/300 - Train loss: 0.6234110593795776, Validation loss: 0.6212142705917358
Epoch: 48/300 - Train loss: 0.6212393641471863, Validation loss: 0.6189776062965393
Epoch: 49/300 - Train loss: 0.6190629005432129, Validation loss: 0.6166763305664062
Epoch: 50/300 - Train loss: 0.6168811321258545, Validation loss: 0.6142677664756775
Epoch: 51/300 - Train loss: 0.6146942973136902, Validation loss: 0.6120498776435852
Epoch: 52/300 - Train loss: 0.6125079989433289, Validation loss: 0.6101268529891968
Epoch: 53/300 - Train loss: 0.6103225946426392, Validation loss: 0.6076797246932983
Epoch: 54/300 - Train loss: 0.6081412434577942, Validation loss: 0.6059937477111816
Epoch: 55/300 - Train loss: 0.605965793132782, Validation loss: 0.6034692525863647
Epoch: 56/300 - Train loss: 0.6037968397140503, Validation loss: 0.6013139486312866
Epoch: 57/300 - Train loss: 0.6016328930854797, Validation loss: 0.5991086959838867
Epoch: 58/300 - Train loss: 0.5994760990142822, Validation loss: 0.5971601009368896
Epoch: 59/300 - Train loss: 0.5973247289657593, Validation loss: 0.5948768854141235
Epoch: 60/300 - Train loss: 0.595177948474884, Validation loss: 0.5928249359130859
Epoch: 61/300 - Train loss: 0.5930336713790894, Validation loss: 0.5906718969345093
Epoch: 62/300 - Train loss: 0.5908917188644409, Validation loss: 0.5885078310966492
Epoch: 63/300 - Train loss: 0.5887527465820312, Validation loss: 0.5863185524940491
Epoch: 64/300 - Train loss: 0.586617112159729, Validation loss: 0.5845785737037659
Epoch: 65/300 - Train loss: 0.584486186504364, Validation loss: 0.5821585655212402
Epoch: 66/300 - Train loss: 0.5823604464530945, Validation loss: 0.5800057053565979
Epoch: 67/300 - Train loss: 0.5802399516105652, Validation loss: 0.5780948996543884
Epoch: 68/300 - Train loss: 0.5781252980232239, Validation loss: 0.5761280059814453
Epoch: 69/300 - Train loss: 0.5760175585746765, Validation loss: 0.5737794637680054
Epoch: 70/300 - Train loss: 0.5739161372184753, Validation loss: 0.571622371673584
Epoch: 71/300 - Train loss: 0.5718198418617249, Validation loss: 0.5698273181915283
Epoch: 72/300 - Train loss: 0.5697306394577026, Validation loss: 0.5673947334289551
Epoch: 73/300 - Train loss: 0.5676455497741699, Validation loss: 0.5654876232147217
Epoch: 74/300 - Train loss: 0.5655664205551147, Validation loss: 0.5637431740760803
Epoch: 75/300 - Train loss: 0.5634909272193909, Validation loss: 0.5613270998001099
Epoch: 76/300 - Train loss: 0.5614180564880371, Validation loss: 0.5590616464614868
Epoch: 77/300 - Train loss: 0.5593497157096863, Validation loss: 0.5572319626808167
Epoch: 78/300 - Train loss: 0.5572859644889832, Validation loss: 0.5552359819412231
Epoch: 79/300 - Train loss: 0.5552300810813904, Validation loss: 0.5530025959014893
Epoch: 80/300 - Train loss: 0.5531833171844482, Validation loss: 0.5509687066078186
Epoch: 81/300 - Train loss: 0.5511428713798523, Validation loss: 0.5490288734436035
Epoch: 82/300 - Train loss: 0.5491085648536682, Validation loss: 0.5469485521316528
Epoch: 83/300 - Train loss: 0.5470808148384094, Validation loss: 0.5453028678894043
Epoch: 84/300 - Train loss: 0.5450602769851685, Validation loss: 0.5429546236991882
Epoch: 85/300 - Train loss: 0.5430463552474976, Validation loss: 0.5410548448562622
Epoch: 86/300 - Train loss: 0.5410388708114624, Validation loss: 0.5394114851951599
Epoch: 87/300 - Train loss: 0.5390385985374451, Validation loss: 0.5369641780853271
Epoch: 88/300 - Train loss: 0.537046492099762, Validation loss: 0.5352572798728943
Epoch: 89/300 - Train loss: 0.5350635647773743, Validation loss: 0.5334208011627197
Epoch: 90/300 - Train loss: 0.533089816570282, Validation loss: 0.5311965942382812
Epoch: 91/300 - Train loss: 0.5311235189437866, Validation loss: 0.5295579433441162
Epoch: 92/300 - Train loss: 0.5291650295257568, Validation loss: 0.5276837944984436
Epoch: 93/300 - Train loss: 0.5272151231765747, Validation loss: 0.5256333947181702
Epoch: 94/300 - Train loss: 0.5252739787101746, Validation loss: 0.5237225890159607
Epoch: 95/300 - Train loss: 0.5233401656150818, Validation loss: 0.5218394994735718
Epoch: 96/300 - Train loss: 0.5214141011238098, Validation loss: 0.5199687480926514
Epoch: 97/300 - Train loss: 0.5194952487945557, Validation loss: 0.5178135633468628
Epoch: 98/300 - Train loss: 0.5175854563713074, Validation loss: 0.5158824920654297
Epoch: 99/300 - Train loss: 0.5156850814819336, Validation loss: 0.5136569142341614
Epoch: 100/300 - Train loss: 0.513793408870697, Validation loss: 0.5121433734893799
Epoch: 101/300 - Train loss: 0.5119103789329529, Validation loss: 0.510365903377533
Epoch: 102/300 - Train loss: 0.5100365877151489, Validation loss: 0.5086929798126221
Epoch: 103/300 - Train loss: 0.5081723928451538, Validation loss: 0.5071521401405334
Epoch: 104/300 - Train loss: 0.5063182711601257, Validation loss: 0.5044561624526978
Epoch: 105/300 - Train loss: 0.5044731497764587, Validation loss: 0.5032884478569031
Epoch: 106/300 - Train loss: 0.5026383399963379, Validation loss: 0.5012363195419312
Epoch: 107/300 - Train loss: 0.5008150935173035, Validation loss: 0.4995034337043762
Epoch: 108/300 - Train loss: 0.4990037977695465, Validation loss: 0.4971771240234375
Epoch: 109/300 - Train loss: 0.49720385670661926, Validation loss: 0.4960639476776123
Epoch: 110/300 - Train loss: 0.4954155683517456, Validation loss: 0.49424588680267334
Epoch: 111/300 - Train loss: 0.4936390221118927, Validation loss: 0.4926600158214569
Epoch: 112/300 - Train loss: 0.49187570810317993, Validation loss: 0.4909633696079254
Epoch: 113/300 - Train loss: 0.49012503027915955, Validation loss: 0.4894380569458008
Epoch: 114/300 - Train loss: 0.48838672041893005, Validation loss: 0.48740094900131226
Epoch: 115/300 - Train loss: 0.4866609275341034, Validation loss: 0.4858332574367523
Epoch: 116/300 - Train loss: 0.4849487841129303, Validation loss: 0.4849280118942261
Epoch: 117/300 - Train loss: 0.48325085639953613, Validation loss: 0.4823109805583954
Epoch: 118/300 - Train loss: 0.4815659821033478, Validation loss: 0.4809371829032898
Epoch: 119/300 - Train loss: 0.4798937737941742, Validation loss: 0.47889432311058044
Epoch: 120/300 - Train loss: 0.47823458909988403, Validation loss: 0.4772092401981354
Epoch: 121/300 - Train loss: 0.4765898287296295, Validation loss: 0.47589245438575745
Epoch: 122/300 - Train loss: 0.4749595522880554, Validation loss: 0.47443240880966187
Epoch: 123/300 - Train loss: 0.4733427166938782, Validation loss: 0.4724721610546112
Epoch: 124/300 - Train loss: 0.4717402160167694, Validation loss: 0.4711115062236786
Epoch: 125/300 - Train loss: 0.4701516926288605, Validation loss: 0.4700017273426056
Epoch: 126/300 - Train loss: 0.46857765316963196, Validation loss: 0.4688018560409546
Epoch: 127/300 - Train loss: 0.46701785922050476, Validation loss: 0.4667665362358093
Epoch: 128/300 - Train loss: 0.46547192335128784, Validation loss: 0.4649842381477356
Epoch: 129/300 - Train loss: 0.46394017338752747, Validation loss: 0.46433213353157043
Epoch: 130/300 - Train loss: 0.46242207288742065, Validation loss: 0.46278080344200134
Epoch: 131/300 - Train loss: 0.4609185457229614, Validation loss: 0.4613111913204193
Epoch: 132/300 - Train loss: 0.45943009853363037, Validation loss: 0.45959237217903137
Epoch: 133/300 - Train loss: 0.45795738697052, Validation loss: 0.4587761461734772
Epoch: 134/300 - Train loss: 0.45649978518486023, Validation loss: 0.45675984025001526
Epoch: 135/300 - Train loss: 0.4550568163394928, Validation loss: 0.45561158657073975
Epoch: 136/300 - Train loss: 0.45362943410873413, Validation loss: 0.45410746335983276
Epoch: 137/300 - Train loss: 0.452216774225235, Validation loss: 0.45256292819976807
Epoch: 138/300 - Train loss: 0.45081886649131775, Validation loss: 0.4510258138179779
Epoch: 139/300 - Train loss: 0.4494363069534302, Validation loss: 0.4498768746852875
Epoch: 140/300 - Train loss: 0.44806984066963196, Validation loss: 0.447994589805603
Epoch: 141/300 - Train loss: 0.4467194378376007, Validation loss: 0.44750553369522095
Epoch: 142/300 - Train loss: 0.4453846514225006, Validation loss: 0.4462931454181671
Epoch: 143/300 - Train loss: 0.4440653324127197, Validation loss: 0.4451982378959656
Epoch: 144/300 - Train loss: 0.4427611529827118, Validation loss: 0.44401848316192627
Epoch: 145/300 - Train loss: 0.44147253036499023, Validation loss: 0.44201838970184326
Epoch: 146/300 - Train loss: 0.4401996433734894, Validation loss: 0.4417441785335541
Epoch: 147/300 - Train loss: 0.43894249200820923, Validation loss: 0.4398898482322693
Epoch: 148/300 - Train loss: 0.43770015239715576, Validation loss: 0.43871259689331055
Epoch: 149/300 - Train loss: 0.43647292256355286, Validation loss: 0.4377714991569519
Epoch: 150/300 - Train loss: 0.4352605640888214, Validation loss: 0.4370806813240051
Epoch: 151/300 - Train loss: 0.43406346440315247, Validation loss: 0.43526509404182434
Epoch: 152/300 - Train loss: 0.4328809678554535, Validation loss: 0.43437787890434265
Epoch: 153/300 - Train loss: 0.431712806224823, Validation loss: 0.433608740568161
Epoch: 154/300 - Train loss: 0.4305589199066162, Validation loss: 0.4325680434703827
Epoch: 155/300 - Train loss: 0.4294193983078003, Validation loss: 0.4311099350452423
Epoch: 156/300 - Train loss: 0.42829298973083496, Validation loss: 0.429654598236084
Epoch: 157/300 - Train loss: 0.42718029022216797, Validation loss: 0.4290510416030884
Epoch: 158/300 - Train loss: 0.4260813891887665, Validation loss: 0.42772772908210754
Epoch: 159/300 - Train loss: 0.42499589920043945, Validation loss: 0.4260818064212799
Epoch: 160/300 - Train loss: 0.42392367124557495, Validation loss: 0.4258333146572113
Epoch: 161/300 - Train loss: 0.422865092754364, Validation loss: 0.4242250323295593
Epoch: 162/300 - Train loss: 0.42181944847106934, Validation loss: 0.4241665005683899
Epoch: 163/300 - Train loss: 0.42078638076782227, Validation loss: 0.422690749168396
Epoch: 164/300 - Train loss: 0.41976603865623474, Validation loss: 0.42160511016845703
Epoch: 165/300 - Train loss: 0.41875872015953064, Validation loss: 0.4211193025112152
Epoch: 166/300 - Train loss: 0.41776418685913086, Validation loss: 0.4198722839355469
Epoch: 167/300 - Train loss: 0.41678187251091003, Validation loss: 0.4186362624168396
Epoch: 168/300 - Train loss: 0.4158118963241577, Validation loss: 0.41852349042892456
Epoch: 169/300 - Train loss: 0.4148538410663605, Validation loss: 0.4170251786708832
Epoch: 170/300 - Train loss: 0.4139069616794586, Validation loss: 0.4159466624259949
Epoch: 171/300 - Train loss: 0.412971556186676, Validation loss: 0.4151020050048828
Epoch: 172/300 - Train loss: 0.41204747557640076, Validation loss: 0.41404807567596436
Epoch: 173/300 - Train loss: 0.41113516688346863, Validation loss: 0.4138447344303131
Epoch: 174/300 - Train loss: 0.4102340340614319, Validation loss: 0.4127202332019806
Epoch: 175/300 - Train loss: 0.40934377908706665, Validation loss: 0.4113917946815491
Epoch: 176/300 - Train loss: 0.4084647595882416, Validation loss: 0.4112457036972046
Epoch: 177/300 - Train loss: 0.4075968563556671, Validation loss: 0.4102797210216522
Epoch: 178/300 - Train loss: 0.4067399799823761, Validation loss: 0.40943509340286255
Epoch: 179/300 - Train loss: 0.40589433908462524, Validation loss: 0.40871143341064453
Epoch: 180/300 - Train loss: 0.40505942702293396, Validation loss: 0.4075033962726593
Epoch: 181/300 - Train loss: 0.40423476696014404, Validation loss: 0.4068160951137543
Epoch: 182/300 - Train loss: 0.403420090675354, Validation loss: 0.4062158465385437
Epoch: 183/300 - Train loss: 0.402614563703537, Validation loss: 0.40519702434539795
Epoch: 184/300 - Train loss: 0.40181782841682434, Validation loss: 0.4040278494358063
Epoch: 185/300 - Train loss: 0.40103012323379517, Validation loss: 0.4037606418132782
Epoch: 186/300 - Train loss: 0.4002515971660614, Validation loss: 0.4031567871570587
Epoch: 187/300 - Train loss: 0.399482399225235, Validation loss: 0.40217116475105286
Epoch: 188/300 - Train loss: 0.39872246980667114, Validation loss: 0.40150314569473267
Epoch: 189/300 - Train loss: 0.3979697823524475, Validation loss: 0.40041157603263855
Epoch: 190/300 - Train loss: 0.3972254991531372, Validation loss: 0.40022677183151245
Epoch: 191/300 - Train loss: 0.39649003744125366, Validation loss: 0.39899444580078125
Epoch: 192/300 - Train loss: 0.3957631587982178, Validation loss: 0.3988759219646454
Epoch: 193/300 - Train loss: 0.39504358172416687, Validation loss: 0.3983886241912842
Epoch: 194/300 - Train loss: 0.39433130621910095, Validation loss: 0.39733636379241943
Epoch: 195/300 - Train loss: 0.39362633228302, Validation loss: 0.3968632221221924
Epoch: 196/300 - Train loss: 0.39292871952056885, Validation loss: 0.3961068093776703
Epoch: 197/300 - Train loss: 0.3922383785247803, Validation loss: 0.3957751989364624
Epoch: 198/300 - Train loss: 0.39155569672584534, Validation loss: 0.39449572563171387
Epoch: 199/300 - Train loss: 0.39087986946105957, Validation loss: 0.3938484787940979
Epoch: 200/300 - Train loss: 0.3902116119861603, Validation loss: 0.3934401869773865
Epoch: 201/300 - Train loss: 0.3895506262779236, Validation loss: 0.3928013741970062
Epoch: 202/300 - Train loss: 0.38889601826667786, Validation loss: 0.3918249309062958
Epoch: 203/300 - Train loss: 0.388247549533844, Validation loss: 0.391292005777359
Epoch: 204/300 - Train loss: 0.3876062333583832, Validation loss: 0.39071711897850037
Epoch: 205/300 - Train loss: 0.38697096705436707, Validation loss: 0.39001041650772095
Epoch: 206/300 - Train loss: 0.3863421082496643, Validation loss: 0.38976722955703735
Epoch: 207/300 - Train loss: 0.3857196271419525, Validation loss: 0.3891325294971466
Epoch: 208/300 - Train loss: 0.385104238986969, Validation loss: 0.38828206062316895
Epoch: 209/300 - Train loss: 0.3844940662384033, Validation loss: 0.3878944516181946
Epoch: 210/300 - Train loss: 0.38388925790786743, Validation loss: 0.38775166869163513
Epoch: 211/300 - Train loss: 0.3832907974720001, Validation loss: 0.38772764801979065
