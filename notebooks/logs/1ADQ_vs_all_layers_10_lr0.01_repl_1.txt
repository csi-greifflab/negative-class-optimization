Epoch: 1/200 - Train loss: 0.4545285999774933, Validation loss: 0.36512643098831177
Epoch: 2/200 - Train loss: 0.32388395071029663, Validation loss: 0.3096483647823334
Epoch: 3/200 - Train loss: 0.2737569808959961, Validation loss: 0.28564655780792236
Epoch: 4/200 - Train loss: 0.24743294715881348, Validation loss: 0.2731482684612274
Epoch: 5/200 - Train loss: 0.23133154213428497, Validation loss: 0.2672079801559448
Epoch: 6/200 - Train loss: 0.21935369074344635, Validation loss: 0.25736990571022034
Epoch: 7/200 - Train loss: 0.20992349088191986, Validation loss: 0.253373384475708
Epoch: 8/200 - Train loss: 0.20415766537189484, Validation loss: 0.2505047023296356
Epoch: 9/200 - Train loss: 0.19676601886749268, Validation loss: 0.2442900836467743
Epoch: 10/200 - Train loss: 0.1923365443944931, Validation loss: 0.24443189799785614
Epoch: 11/200 - Train loss: 0.186457097530365, Validation loss: 0.2422003597021103
Epoch: 12/200 - Train loss: 0.1835843026638031, Validation loss: 0.23408369719982147
Epoch: 13/200 - Train loss: 0.17799754440784454, Validation loss: 0.23424087464809418
Epoch: 14/200 - Train loss: 0.17494063079357147, Validation loss: 0.245399609208107
Epoch: 15/200 - Train loss: 0.172601118683815, Validation loss: 0.2365579456090927
Epoch: 16/200 - Train loss: 0.166190043091774, Validation loss: 0.23390844464302063
Epoch: 17/200 - Train loss: 0.1639314889907837, Validation loss: 0.24322310090065002
Epoch: 18/200 - Train loss: 0.1617850512266159, Validation loss: 0.23499198257923126
Epoch: 19/200 - Train loss: 0.15969929099082947, Validation loss: 0.23321588337421417
Epoch: 20/200 - Train loss: 0.15717889368534088, Validation loss: 0.23611615598201752
Epoch: 21/200 - Train loss: 0.1553007960319519, Validation loss: 0.25343912839889526
Epoch: 22/200 - Train loss: 0.15523964166641235, Validation loss: 0.23719392716884613
Epoch: 23/200 - Train loss: 0.15191669762134552, Validation loss: 0.23596243560314178
Epoch: 24/200 - Train loss: 0.15087470412254333, Validation loss: 0.23950593173503876
Epoch: 25/200 - Train loss: 0.14975513517856598, Validation loss: 0.2427530139684677
Epoch: 26/200 - Train loss: 0.14950266480445862, Validation loss: 0.2515624463558197
Epoch: 27/200 - Train loss: 0.1492195576429367, Validation loss: 0.23815394937992096
Epoch: 28/200 - Train loss: 0.14608514308929443, Validation loss: 0.2541397213935852
Epoch: 29/200 - Train loss: 0.14485326409339905, Validation loss: 0.23453643918037415
Epoch: 30/200 - Train loss: 0.14568482339382172, Validation loss: 0.23884165287017822
Epoch: 31/200 - Train loss: 0.14384129643440247, Validation loss: 0.24073684215545654
Epoch: 32/200 - Train loss: 0.1426905393600464, Validation loss: 0.23640532791614532
Epoch: 33/200 - Train loss: 0.1420608013868332, Validation loss: 0.2367110252380371
Epoch: 34/200 - Train loss: 0.14044755697250366, Validation loss: 0.2417580634355545
Epoch: 35/200 - Train loss: 0.14049886167049408, Validation loss: 0.2422304004430771
Epoch: 36/200 - Train loss: 0.137923002243042, Validation loss: 0.24549196660518646
Epoch: 37/200 - Train loss: 0.1387055665254593, Validation loss: 0.2558671832084656
Epoch: 38/200 - Train loss: 0.1389225274324417, Validation loss: 0.25361162424087524
Epoch: 39/200 - Train loss: 0.13718661665916443, Validation loss: 0.25420069694519043
Epoch: 40/200 - Train loss: 0.1379520297050476, Validation loss: 0.25241410732269287
Epoch: 41/200 - Train loss: 0.13666504621505737, Validation loss: 0.2544403672218323
Epoch: 42/200 - Train loss: 0.1368849277496338, Validation loss: 0.2544058561325073
Epoch: 43/200 - Train loss: 0.1373114138841629, Validation loss: 0.25270283222198486
Epoch: 44/200 - Train loss: 0.13669659197330475, Validation loss: 0.26229915022850037
Epoch: 45/200 - Train loss: 0.13492275774478912, Validation loss: 0.2539156675338745
Epoch: 46/200 - Train loss: 0.1334475725889206, Validation loss: 0.2568887174129486
Epoch: 47/200 - Train loss: 0.1345566362142563, Validation loss: 0.25857973098754883
Epoch: 48/200 - Train loss: 0.13433632254600525, Validation loss: 0.26369473338127136
Epoch: 49/200 - Train loss: 0.1336294412612915, Validation loss: 0.2639364004135132
Epoch: 50/200 - Train loss: 0.1338423788547516, Validation loss: 0.26485610008239746
Epoch: 51/200 - Train loss: 0.13285578787326813, Validation loss: 0.25822293758392334
Epoch: 52/200 - Train loss: 0.13281287252902985, Validation loss: 0.25829237699508667
Epoch: 53/200 - Train loss: 0.13155050575733185, Validation loss: 0.24987666308879852
Epoch: 54/200 - Train loss: 0.13196730613708496, Validation loss: 0.26462996006011963
Epoch: 55/200 - Train loss: 0.13087283074855804, Validation loss: 0.259034126996994
Epoch: 56/200 - Train loss: 0.13210059702396393, Validation loss: 0.2626936137676239
Epoch: 57/200 - Train loss: 0.13062067329883575, Validation loss: 0.2610824406147003
Epoch: 58/200 - Train loss: 0.13063818216323853, Validation loss: 0.2608017027378082
Epoch: 59/200 - Train loss: 0.13055729866027832, Validation loss: 0.26310062408447266
Epoch: 60/200 - Train loss: 0.13019785284996033, Validation loss: 0.2599128782749176
Epoch: 61/200 - Train loss: 0.12928050756454468, Validation loss: 0.26368385553359985
Epoch: 62/200 - Train loss: 0.12995468080043793, Validation loss: 0.26278772950172424
Epoch: 63/200 - Train loss: 0.12936291098594666, Validation loss: 0.26842841506004333
Epoch: 64/200 - Train loss: 0.12899863719940186, Validation loss: 0.2663324475288391
Epoch: 65/200 - Train loss: 0.12904782593250275, Validation loss: 0.2658962607383728
Epoch: 66/200 - Train loss: 0.1284204125404358, Validation loss: 0.2607918977737427
Epoch: 67/200 - Train loss: 0.12770432233810425, Validation loss: 0.2588302195072174
Epoch: 68/200 - Train loss: 0.1291341930627823, Validation loss: 0.26453062891960144
Epoch: 69/200 - Train loss: 0.12847091257572174, Validation loss: 0.2640269100666046
Epoch: 70/200 - Train loss: 0.12930795550346375, Validation loss: 0.2646746337413788
Epoch: 71/200 - Train loss: 0.12685398757457733, Validation loss: 0.2663794159889221
Epoch: 72/200 - Train loss: 0.12846793234348297, Validation loss: 0.26280826330184937
Epoch: 73/200 - Train loss: 0.1270107626914978, Validation loss: 0.26524630188941956
Epoch: 74/200 - Train loss: 0.12691329419612885, Validation loss: 0.2665771245956421
Epoch: 75/200 - Train loss: 0.12616480886936188, Validation loss: 0.26783835887908936
Epoch: 76/200 - Train loss: 0.1264898180961609, Validation loss: 0.26340022683143616
Epoch: 77/200 - Train loss: 0.1264381855726242, Validation loss: 0.26574137806892395
Epoch: 78/200 - Train loss: 0.12556245923042297, Validation loss: 0.2728223502635956
Epoch: 79/200 - Train loss: 0.12576806545257568, Validation loss: 0.27065593004226685
Epoch: 80/200 - Train loss: 0.12511666119098663, Validation loss: 0.2661151885986328
Epoch: 81/200 - Train loss: 0.12458895891904831, Validation loss: 0.26833391189575195
Epoch: 82/200 - Train loss: 0.1257573962211609, Validation loss: 0.2651550769805908
Epoch: 83/200 - Train loss: 0.1253013163805008, Validation loss: 0.27188920974731445
Epoch: 84/200 - Train loss: 0.1250692456960678, Validation loss: 0.267521470785141
Epoch: 85/200 - Train loss: 0.1256130486726761, Validation loss: 0.27089858055114746
Epoch: 86/200 - Train loss: 0.12612606585025787, Validation loss: 0.26931655406951904
Epoch: 87/200 - Train loss: 0.1253128945827484, Validation loss: 0.26536864042282104
Epoch: 88/200 - Train loss: 0.12369073927402496, Validation loss: 0.27079346776008606
Epoch: 89/200 - Train loss: 0.12441694736480713, Validation loss: 0.2684992253780365
Epoch: 90/200 - Train loss: 0.12443380057811737, Validation loss: 0.27119845151901245
Epoch: 91/200 - Train loss: 0.1250651329755783, Validation loss: 0.2738771438598633
Epoch: 92/200 - Train loss: 0.1250835657119751, Validation loss: 0.27044689655303955
Epoch: 93/200 - Train loss: 0.12315944582223892, Validation loss: 0.27639466524124146
Epoch: 94/200 - Train loss: 0.12400390952825546, Validation loss: 0.2707250714302063
Epoch: 95/200 - Train loss: 0.12366610765457153, Validation loss: 0.2711111009120941
Epoch: 96/200 - Train loss: 0.1243782564997673, Validation loss: 0.2734277844429016
Epoch: 97/200 - Train loss: 0.12308470159769058, Validation loss: 0.2691778838634491
Epoch: 98/200 - Train loss: 0.12419389188289642, Validation loss: 0.274141401052475
Epoch: 99/200 - Train loss: 0.12392079085111618, Validation loss: 0.2682434320449829
Epoch: 100/200 - Train loss: 0.12343913316726685, Validation loss: 0.280040979385376
Epoch: 101/200 - Train loss: 0.12396430224180222, Validation loss: 0.2752806842327118
Epoch: 102/200 - Train loss: 0.123601034283638, Validation loss: 0.27570462226867676
Epoch: 103/200 - Train loss: 0.12312447279691696, Validation loss: 0.275539755821228
Epoch: 104/200 - Train loss: 0.12253320962190628, Validation loss: 0.28177759051322937
Epoch: 105/200 - Train loss: 0.12252259999513626, Validation loss: 0.2743133306503296
Epoch: 106/200 - Train loss: 0.12226270884275436, Validation loss: 0.2711646854877472
Epoch: 107/200 - Train loss: 0.1219133660197258, Validation loss: 0.2777596712112427
Epoch: 108/200 - Train loss: 0.12234745174646378, Validation loss: 0.2816910445690155
Epoch: 109/200 - Train loss: 0.12167683243751526, Validation loss: 0.2761598527431488
Epoch: 110/200 - Train loss: 0.12214154005050659, Validation loss: 0.2838872969150543
Epoch: 111/200 - Train loss: 0.12078069150447845, Validation loss: 0.28114303946495056
Epoch: 112/200 - Train loss: 0.1220083087682724, Validation loss: 0.2803180515766144
Epoch: 113/200 - Train loss: 0.12224997580051422, Validation loss: 0.27855774760246277
Epoch: 114/200 - Train loss: 0.12280263751745224, Validation loss: 0.2798704206943512
Epoch: 115/200 - Train loss: 0.1218278631567955, Validation loss: 0.27909746766090393
Epoch: 116/200 - Train loss: 0.12092239409685135, Validation loss: 0.28576332330703735
Epoch: 117/200 - Train loss: 0.1211768239736557, Validation loss: 0.2774726450443268
Epoch: 118/200 - Train loss: 0.12131614238023758, Validation loss: 0.2773713171482086
Epoch: 119/200 - Train loss: 0.12136427313089371, Validation loss: 0.27468737959861755
Epoch: 120/200 - Train loss: 0.12098850309848785, Validation loss: 0.27892136573791504
Epoch: 121/200 - Train loss: 0.12063229829072952, Validation loss: 0.2835640609264374
Epoch: 122/200 - Train loss: 0.12141118943691254, Validation loss: 0.2775246202945709
Epoch: 123/200 - Train loss: 0.12130139768123627, Validation loss: 0.2878447473049164
Epoch: 124/200 - Train loss: 0.12184493988752365, Validation loss: 0.2774525284767151
Epoch: 125/200 - Train loss: 0.12064949423074722, Validation loss: 0.2791133224964142
Epoch: 126/200 - Train loss: 0.1223430410027504, Validation loss: 0.28004246950149536
Epoch: 127/200 - Train loss: 0.12119565159082413, Validation loss: 0.27813485264778137
Epoch: 128/200 - Train loss: 0.11932287365198135, Validation loss: 0.2847488820552826
Epoch: 129/200 - Train loss: 0.12005560100078583, Validation loss: 0.28736162185668945
Epoch: 130/200 - Train loss: 0.12166811525821686, Validation loss: 0.280827134847641
Epoch: 131/200 - Train loss: 0.11986751109361649, Validation loss: 0.2832707166671753
Epoch: 132/200 - Train loss: 0.11973647773265839, Validation loss: 0.2792455554008484
Epoch: 133/200 - Train loss: 0.12017626315355301, Validation loss: 0.2808690667152405
Epoch: 134/200 - Train loss: 0.1188737228512764, Validation loss: 0.28210970759391785
Epoch: 135/200 - Train loss: 0.11995133757591248, Validation loss: 0.2826160192489624
Epoch: 136/200 - Train loss: 0.11938371509313583, Validation loss: 0.2837511897087097
Epoch: 137/200 - Train loss: 0.12024141848087311, Validation loss: 0.2818987965583801
Epoch: 138/200 - Train loss: 0.11907941102981567, Validation loss: 0.2818335294723511
Epoch: 139/200 - Train loss: 0.1195913702249527, Validation loss: 0.2895428240299225
Epoch: 140/200 - Train loss: 0.12066715955734253, Validation loss: 0.2871777415275574
Epoch: 141/200 - Train loss: 0.11966272443532944, Validation loss: 0.29236921668052673
Epoch: 142/200 - Train loss: 0.11828487366437912, Validation loss: 0.28425538539886475
Epoch: 143/200 - Train loss: 0.11939465254545212, Validation loss: 0.28758156299591064
Epoch: 144/200 - Train loss: 0.11884288489818573, Validation loss: 0.2840684950351715
Epoch: 145/200 - Train loss: 0.11789495497941971, Validation loss: 0.2894708216190338
Epoch: 146/200 - Train loss: 0.11913377046585083, Validation loss: 0.2867239713668823
Epoch: 147/200 - Train loss: 0.11844536662101746, Validation loss: 0.28756725788116455
Epoch: 148/200 - Train loss: 0.1181475818157196, Validation loss: 0.2863854467868805
Epoch: 149/200 - Train loss: 0.1190086081624031, Validation loss: 0.29403364658355713
Epoch: 150/200 - Train loss: 0.11927162855863571, Validation loss: 0.2850458323955536
Epoch: 151/200 - Train loss: 0.11903123557567596, Validation loss: 0.28564760088920593
Epoch: 152/200 - Train loss: 0.1194709911942482, Validation loss: 0.2920328974723816
Epoch: 153/200 - Train loss: 0.1184290274977684, Validation loss: 0.28425395488739014
Epoch: 154/200 - Train loss: 0.118055559694767, Validation loss: 0.28797268867492676
Epoch: 155/200 - Train loss: 0.11916251480579376, Validation loss: 0.2895154058933258
Epoch: 156/200 - Train loss: 0.1189446896314621, Validation loss: 0.2905506491661072
Epoch: 157/200 - Train loss: 0.11769747734069824, Validation loss: 0.2866758108139038
Epoch: 158/200 - Train loss: 0.11753090471029282, Validation loss: 0.28904539346694946
Epoch: 159/200 - Train loss: 0.11736473441123962, Validation loss: 0.2876583933830261
Epoch: 160/200 - Train loss: 0.11791995167732239, Validation loss: 0.2909363806247711
Epoch: 161/200 - Train loss: 0.11916365474462509, Validation loss: 0.2916646897792816
Epoch: 162/200 - Train loss: 0.11763790994882584, Validation loss: 0.290378600358963
Epoch: 163/200 - Train loss: 0.11776121705770493, Validation loss: 0.2887296974658966
