Epoch: 1/300 - Train loss: 0.7034443616867065, Validation loss: 0.7016683220863342
Epoch: 2/300 - Train loss: 0.6996861696243286, Validation loss: 0.698112428188324
Epoch: 3/300 - Train loss: 0.696105420589447, Validation loss: 0.694538950920105
Epoch: 4/300 - Train loss: 0.6926564574241638, Validation loss: 0.6912891268730164
Epoch: 5/300 - Train loss: 0.6892592310905457, Validation loss: 0.6877318024635315
Epoch: 6/300 - Train loss: 0.6858549118041992, Validation loss: 0.6842960715293884
Epoch: 7/300 - Train loss: 0.6823819875717163, Validation loss: 0.6807842254638672
Epoch: 8/300 - Train loss: 0.6787896156311035, Validation loss: 0.6769917607307434
Epoch: 9/300 - Train loss: 0.6750413179397583, Validation loss: 0.6730499863624573
Epoch: 10/300 - Train loss: 0.6711108088493347, Validation loss: 0.6691110730171204
Epoch: 11/300 - Train loss: 0.6669900417327881, Validation loss: 0.6644687056541443
Epoch: 12/300 - Train loss: 0.6626814603805542, Validation loss: 0.6599791049957275
Epoch: 13/300 - Train loss: 0.6581840515136719, Validation loss: 0.6555801033973694
Epoch: 14/300 - Train loss: 0.6535114049911499, Validation loss: 0.6507895588874817
Epoch: 15/300 - Train loss: 0.6486778259277344, Validation loss: 0.6458515524864197
Epoch: 16/300 - Train loss: 0.6436976194381714, Validation loss: 0.6407535076141357
Epoch: 17/300 - Train loss: 0.6385828256607056, Validation loss: 0.6356676816940308
Epoch: 18/300 - Train loss: 0.6333513855934143, Validation loss: 0.6303463578224182
Epoch: 19/300 - Train loss: 0.6280264258384705, Validation loss: 0.6253634691238403
Epoch: 20/300 - Train loss: 0.6226223111152649, Validation loss: 0.6197472214698792
Epoch: 21/300 - Train loss: 0.6171520352363586, Validation loss: 0.6144540309906006
Epoch: 22/300 - Train loss: 0.6116295456886292, Validation loss: 0.6088619232177734
Epoch: 23/300 - Train loss: 0.6060734987258911, Validation loss: 0.6034026741981506
Epoch: 24/300 - Train loss: 0.6004925966262817, Validation loss: 0.5979980826377869
Epoch: 25/300 - Train loss: 0.5948907732963562, Validation loss: 0.5925086736679077
Epoch: 26/300 - Train loss: 0.5892714262008667, Validation loss: 0.5871235132217407
Epoch: 27/300 - Train loss: 0.583636999130249, Validation loss: 0.5814995169639587
Epoch: 28/300 - Train loss: 0.5779926180839539, Validation loss: 0.5759211778640747
Epoch: 29/300 - Train loss: 0.5723392367362976, Validation loss: 0.5707630515098572
Epoch: 30/300 - Train loss: 0.5666775703430176, Validation loss: 0.5649906992912292
Epoch: 31/300 - Train loss: 0.5610112547874451, Validation loss: 0.5592563152313232
Epoch: 32/300 - Train loss: 0.5553445219993591, Validation loss: 0.5536686182022095
Epoch: 33/300 - Train loss: 0.5496811866760254, Validation loss: 0.5485108494758606
Epoch: 34/300 - Train loss: 0.5440255999565125, Validation loss: 0.5429752469062805
Epoch: 35/300 - Train loss: 0.5383850336074829, Validation loss: 0.5374624729156494
Epoch: 36/300 - Train loss: 0.5327640175819397, Validation loss: 0.5316734910011292
Epoch: 37/300 - Train loss: 0.5271687507629395, Validation loss: 0.5263046026229858
Epoch: 38/300 - Train loss: 0.5216048359870911, Validation loss: 0.5210344791412354
Epoch: 39/300 - Train loss: 0.5160771012306213, Validation loss: 0.5151858329772949
Epoch: 40/300 - Train loss: 0.5105905532836914, Validation loss: 0.5101020932197571
Epoch: 41/300 - Train loss: 0.5051493048667908, Validation loss: 0.5047532916069031
Epoch: 42/300 - Train loss: 0.4997568130493164, Validation loss: 0.49974459409713745
Epoch: 43/300 - Train loss: 0.49441665410995483, Validation loss: 0.49419689178466797
Epoch: 44/300 - Train loss: 0.48913219571113586, Validation loss: 0.48938044905662537
Epoch: 45/300 - Train loss: 0.48390644788742065, Validation loss: 0.48390159010887146
Epoch: 46/300 - Train loss: 0.4787430763244629, Validation loss: 0.4791344404220581
Epoch: 47/300 - Train loss: 0.4736449122428894, Validation loss: 0.4743398129940033
Epoch: 48/300 - Train loss: 0.4686148464679718, Validation loss: 0.4694627523422241
Epoch: 49/300 - Train loss: 0.46365591883659363, Validation loss: 0.46448782086372375
Epoch: 50/300 - Train loss: 0.4587708115577698, Validation loss: 0.4602140784263611
Epoch: 51/300 - Train loss: 0.453962117433548, Validation loss: 0.45544978976249695
Epoch: 52/300 - Train loss: 0.4492318630218506, Validation loss: 0.4510606825351715
Epoch: 53/300 - Train loss: 0.4445814788341522, Validation loss: 0.44625967741012573
Epoch: 54/300 - Train loss: 0.4400120973587036, Validation loss: 0.44214192032814026
Epoch: 55/300 - Train loss: 0.43552446365356445, Validation loss: 0.4382418394088745
Epoch: 56/300 - Train loss: 0.43111881613731384, Validation loss: 0.4337615966796875
Epoch: 57/300 - Train loss: 0.42679497599601746, Validation loss: 0.4295043647289276
Epoch: 58/300 - Train loss: 0.4225529432296753, Validation loss: 0.42546528577804565
Epoch: 59/300 - Train loss: 0.4183923304080963, Validation loss: 0.4219799041748047
Epoch: 60/300 - Train loss: 0.41431254148483276, Validation loss: 0.4177788496017456
Epoch: 61/300 - Train loss: 0.410313218832016, Validation loss: 0.41366997361183167
Epoch: 62/300 - Train loss: 0.40639352798461914, Validation loss: 0.4104817807674408
Epoch: 63/300 - Train loss: 0.4025529623031616, Validation loss: 0.4066879153251648
Epoch: 64/300 - Train loss: 0.39879071712493896, Validation loss: 0.40315186977386475
Epoch: 65/300 - Train loss: 0.39510539174079895, Validation loss: 0.3990820050239563
Epoch: 66/300 - Train loss: 0.3914962112903595, Validation loss: 0.39572420716285706
Epoch: 67/300 - Train loss: 0.3879619538784027, Validation loss: 0.3927968442440033
Epoch: 68/300 - Train loss: 0.38450124859809875, Validation loss: 0.3890957236289978
Epoch: 69/300 - Train loss: 0.38111260533332825, Validation loss: 0.3864205479621887
Epoch: 70/300 - Train loss: 0.37779465317726135, Validation loss: 0.3825382888317108
Epoch: 71/300 - Train loss: 0.37454602122306824, Validation loss: 0.37928369641304016
Epoch: 72/300 - Train loss: 0.3713654577732086, Validation loss: 0.37657293677330017
Epoch: 73/300 - Train loss: 0.3682512044906616, Validation loss: 0.37342333793640137
Epoch: 74/300 - Train loss: 0.3652017116546631, Validation loss: 0.369924396276474
Epoch: 75/300 - Train loss: 0.362215518951416, Validation loss: 0.3675205111503601
Epoch: 76/300 - Train loss: 0.3592911660671234, Validation loss: 0.3650677800178528
Epoch: 77/300 - Train loss: 0.35642752051353455, Validation loss: 0.3617123067378998
Epoch: 78/300 - Train loss: 0.3536228835582733, Validation loss: 0.3595799207687378
Epoch: 79/300 - Train loss: 0.3508760333061218, Validation loss: 0.3569616675376892
Epoch: 80/300 - Train loss: 0.3481854796409607, Validation loss: 0.354345440864563
Epoch: 81/300 - Train loss: 0.34554949402809143, Validation loss: 0.35186266899108887
Epoch: 82/300 - Train loss: 0.34296661615371704, Validation loss: 0.3497495949268341
Epoch: 83/300 - Train loss: 0.3404357135295868, Validation loss: 0.34674423933029175
Epoch: 84/300 - Train loss: 0.33795562386512756, Validation loss: 0.3446972668170929
Epoch: 85/300 - Train loss: 0.3355248272418976, Validation loss: 0.34219908714294434
Epoch: 86/300 - Train loss: 0.3331421911716461, Validation loss: 0.3398447632789612
Epoch: 87/300 - Train loss: 0.3308063745498657, Validation loss: 0.3376627266407013
Epoch: 88/300 - Train loss: 0.3285161852836609, Validation loss: 0.3347686231136322
Epoch: 89/300 - Train loss: 0.326270192861557, Validation loss: 0.3336152136325836
Epoch: 90/300 - Train loss: 0.32406726479530334, Validation loss: 0.3311963975429535
Epoch: 91/300 - Train loss: 0.32190611958503723, Validation loss: 0.32852041721343994
Epoch: 92/300 - Train loss: 0.3197857439517975, Validation loss: 0.3272971510887146
Epoch: 93/300 - Train loss: 0.31770503520965576, Validation loss: 0.3249795138835907
Epoch: 94/300 - Train loss: 0.31566300988197327, Validation loss: 0.3225715458393097
Epoch: 95/300 - Train loss: 0.31365859508514404, Validation loss: 0.32077863812446594
Epoch: 96/300 - Train loss: 0.3116908371448517, Validation loss: 0.3189658224582672
Epoch: 97/300 - Train loss: 0.3097587525844574, Validation loss: 0.3170854151248932
Epoch: 98/300 - Train loss: 0.307861328125, Validation loss: 0.3155166804790497
Epoch: 99/300 - Train loss: 0.30599743127822876, Validation loss: 0.3137966990470886
Epoch: 100/300 - Train loss: 0.3041662871837616, Validation loss: 0.31188729405403137
Epoch: 101/300 - Train loss: 0.30236703157424927, Validation loss: 0.30963167548179626
Epoch: 102/300 - Train loss: 0.30059880018234253, Validation loss: 0.3079277276992798
Epoch: 103/300 - Train loss: 0.29886066913604736, Validation loss: 0.3067857027053833
Epoch: 104/300 - Train loss: 0.29715198278427124, Validation loss: 0.30529269576072693
Epoch: 105/300 - Train loss: 0.29547199606895447, Validation loss: 0.30313003063201904
Epoch: 106/300 - Train loss: 0.2938198447227478, Validation loss: 0.30172139406204224
Epoch: 107/300 - Train loss: 0.29219478368759155, Validation loss: 0.29980263113975525
Epoch: 108/300 - Train loss: 0.2905961275100708, Validation loss: 0.29797977209091187
Epoch: 109/300 - Train loss: 0.289023220539093, Validation loss: 0.2963411211967468
Epoch: 110/300 - Train loss: 0.28747543692588806, Validation loss: 0.29561737179756165
Epoch: 111/300 - Train loss: 0.285952091217041, Validation loss: 0.2941754162311554
Epoch: 112/300 - Train loss: 0.2844526171684265, Validation loss: 0.29233869910240173
Epoch: 113/300 - Train loss: 0.28297632932662964, Validation loss: 0.2908654510974884
Epoch: 114/300 - Train loss: 0.2815227210521698, Validation loss: 0.2894427180290222
Epoch: 115/300 - Train loss: 0.280091255903244, Validation loss: 0.2874559462070465
Epoch: 116/300 - Train loss: 0.2786814272403717, Validation loss: 0.2863289713859558
Epoch: 117/300 - Train loss: 0.2772926688194275, Validation loss: 0.2852585017681122
Epoch: 118/300 - Train loss: 0.27592453360557556, Validation loss: 0.28382834792137146
Epoch: 119/300 - Train loss: 0.27457648515701294, Validation loss: 0.28232449293136597
Epoch: 120/300 - Train loss: 0.27324798703193665, Validation loss: 0.28179651498794556
Epoch: 121/300 - Train loss: 0.27193865180015564, Validation loss: 0.28011444211006165
Epoch: 122/300 - Train loss: 0.2706480920314789, Validation loss: 0.27896028757095337
Epoch: 123/300 - Train loss: 0.26937586069107056, Validation loss: 0.27762866020202637
Epoch: 124/300 - Train loss: 0.2681216597557068, Validation loss: 0.2766088545322418
Epoch: 125/300 - Train loss: 0.26688504219055176, Validation loss: 0.27497175335884094
Epoch: 126/300 - Train loss: 0.2656656801700592, Validation loss: 0.2740350067615509
Epoch: 127/300 - Train loss: 0.2644631266593933, Validation loss: 0.2727132737636566
Epoch: 128/300 - Train loss: 0.2632770836353302, Validation loss: 0.2722688615322113
Epoch: 129/300 - Train loss: 0.26210707426071167, Validation loss: 0.27036720514297485
Epoch: 130/300 - Train loss: 0.2609529197216034, Validation loss: 0.2697603106498718
Epoch: 131/300 - Train loss: 0.2598143219947815, Validation loss: 0.2681425213813782
Epoch: 132/300 - Train loss: 0.25869089365005493, Validation loss: 0.2675614655017853
Epoch: 133/300 - Train loss: 0.257582426071167, Validation loss: 0.2664797306060791
Epoch: 134/300 - Train loss: 0.2564886212348938, Validation loss: 0.26512664556503296
Epoch: 135/300 - Train loss: 0.2554091513156891, Validation loss: 0.26387134194374084
Epoch: 136/300 - Train loss: 0.2543438673019409, Validation loss: 0.26327192783355713
Epoch: 137/300 - Train loss: 0.2532924711704254, Validation loss: 0.262002170085907
Epoch: 138/300 - Train loss: 0.25225475430488586, Validation loss: 0.26084384322166443
Epoch: 139/300 - Train loss: 0.2512303292751312, Validation loss: 0.2596948742866516
Epoch: 140/300 - Train loss: 0.25021892786026, Validation loss: 0.2590225636959076
Epoch: 141/300 - Train loss: 0.24922038614749908, Validation loss: 0.25873279571533203
Epoch: 142/300 - Train loss: 0.24823449552059174, Validation loss: 0.2566293776035309
Epoch: 143/300 - Train loss: 0.24726103246212006, Validation loss: 0.25601354241371155
Epoch: 144/300 - Train loss: 0.24629981815814972, Validation loss: 0.25505363941192627
Epoch: 145/300 - Train loss: 0.24535062909126282, Validation loss: 0.2545633316040039
Epoch: 146/300 - Train loss: 0.24441327154636383, Validation loss: 0.25320911407470703
Epoch: 147/300 - Train loss: 0.24348758161067963, Validation loss: 0.2529265582561493
Epoch: 148/300 - Train loss: 0.2425733506679535, Validation loss: 0.2511393427848816
Epoch: 149/300 - Train loss: 0.2416703850030899, Validation loss: 0.2508346736431122
Epoch: 150/300 - Train loss: 0.24077850580215454, Validation loss: 0.24975475668907166
Epoch: 151/300 - Train loss: 0.23989754915237427, Validation loss: 0.24860332906246185
Epoch: 152/300 - Train loss: 0.23902727663516998, Validation loss: 0.24805104732513428
Epoch: 153/300 - Train loss: 0.23816756904125214, Validation loss: 0.24697352945804596
Epoch: 154/300 - Train loss: 0.2373182624578476, Validation loss: 0.246400386095047
Epoch: 155/300 - Train loss: 0.23647920787334442, Validation loss: 0.24621817469596863
Epoch: 156/300 - Train loss: 0.2356501817703247, Validation loss: 0.24484233558177948
Epoch: 157/300 - Train loss: 0.2348310947418213, Validation loss: 0.2446330487728119
Epoch: 158/300 - Train loss: 0.23402176797389984, Validation loss: 0.24288778007030487
Epoch: 159/300 - Train loss: 0.23322205245494843, Validation loss: 0.2422451674938202
Epoch: 160/300 - Train loss: 0.2324318140745163, Validation loss: 0.2417239099740982
Epoch: 161/300 - Train loss: 0.2316509634256363, Validation loss: 0.24116773903369904
Epoch: 162/300 - Train loss: 0.2308792918920517, Validation loss: 0.24013645946979523
Epoch: 163/300 - Train loss: 0.23011662065982819, Validation loss: 0.23916028439998627
Epoch: 164/300 - Train loss: 0.22936280071735382, Validation loss: 0.23845821619033813
Epoch: 165/300 - Train loss: 0.22861774265766144, Validation loss: 0.2379526048898697
Epoch: 166/300 - Train loss: 0.22788137197494507, Validation loss: 0.2376750409603119
Epoch: 167/300 - Train loss: 0.22715353965759277, Validation loss: 0.23654019832611084
Epoch: 168/300 - Train loss: 0.226434126496315, Validation loss: 0.2353094071149826
Epoch: 169/300 - Train loss: 0.2257230430841446, Validation loss: 0.2345193475484848
Epoch: 170/300 - Train loss: 0.22502008080482483, Validation loss: 0.23404884338378906
Epoch: 171/300 - Train loss: 0.22432512044906616, Validation loss: 0.23390288650989532
Epoch: 172/300 - Train loss: 0.223638117313385, Validation loss: 0.23262088000774384
Epoch: 173/300 - Train loss: 0.22295887768268585, Validation loss: 0.2323344200849533
Epoch: 174/300 - Train loss: 0.22228725254535675, Validation loss: 0.23137417435646057
Epoch: 175/300 - Train loss: 0.2216232419013977, Validation loss: 0.231304332613945
Epoch: 176/300 - Train loss: 0.2209666520357132, Validation loss: 0.23040476441383362
Epoch: 177/300 - Train loss: 0.22031742334365845, Validation loss: 0.22993195056915283
Epoch: 178/300 - Train loss: 0.21967542171478271, Validation loss: 0.2288707047700882
Epoch: 179/300 - Train loss: 0.21904055774211884, Validation loss: 0.2282514125108719
Epoch: 180/300 - Train loss: 0.21841268241405487, Validation loss: 0.22760537266731262
Epoch: 181/300 - Train loss: 0.21779172122478485, Validation loss: 0.22718507051467896
Epoch: 182/300 - Train loss: 0.2171776294708252, Validation loss: 0.22686858475208282
Epoch: 183/300 - Train loss: 0.21657031774520874, Validation loss: 0.2259068489074707
Epoch: 184/300 - Train loss: 0.21596965193748474, Validation loss: 0.22550415992736816
Epoch: 185/300 - Train loss: 0.21537555754184723, Validation loss: 0.22519972920417786
Epoch: 186/300 - Train loss: 0.21478794515132904, Validation loss: 0.22395814955234528
Epoch: 187/300 - Train loss: 0.21420666575431824, Validation loss: 0.22345928847789764
Epoch: 188/300 - Train loss: 0.21363170444965363, Validation loss: 0.22277481853961945
Epoch: 189/300 - Train loss: 0.21306297183036804, Validation loss: 0.22280170023441315
