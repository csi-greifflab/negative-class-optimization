Epoch: 1/200 - Train loss: 0.48110008239746094, Validation loss: 0.4015917181968689
Epoch: 2/200 - Train loss: 0.3571453392505646, Validation loss: 0.3489695191383362
Epoch: 3/200 - Train loss: 0.3097412586212158, Validation loss: 0.3259967863559723
Epoch: 4/200 - Train loss: 0.2764100432395935, Validation loss: 0.3070974349975586
Epoch: 5/200 - Train loss: 0.26247701048851013, Validation loss: 0.2901035249233246
Epoch: 6/200 - Train loss: 0.25105008482933044, Validation loss: 0.2908288240432739
Epoch: 7/200 - Train loss: 0.24538485705852509, Validation loss: 0.2799937129020691
Epoch: 8/200 - Train loss: 0.24038003385066986, Validation loss: 0.28758689761161804
Epoch: 9/200 - Train loss: 0.237613245844841, Validation loss: 0.27656257152557373
Epoch: 10/200 - Train loss: 0.23295484483242035, Validation loss: 0.2760663628578186
Epoch: 11/200 - Train loss: 0.23109039664268494, Validation loss: 0.2796371281147003
Epoch: 12/200 - Train loss: 0.228972390294075, Validation loss: 0.274605929851532
Epoch: 13/200 - Train loss: 0.226880744099617, Validation loss: 0.28146085143089294
Epoch: 14/200 - Train loss: 0.22798475623130798, Validation loss: 0.271309494972229
Epoch: 15/200 - Train loss: 0.2250138223171234, Validation loss: 0.2729274034500122
Epoch: 16/200 - Train loss: 0.2238726019859314, Validation loss: 0.271601140499115
Epoch: 17/200 - Train loss: 0.22177937626838684, Validation loss: 0.273044228553772
Epoch: 18/200 - Train loss: 0.22228704392910004, Validation loss: 0.28839221596717834
Epoch: 19/200 - Train loss: 0.2232266664505005, Validation loss: 0.2778310775756836
Epoch: 20/200 - Train loss: 0.22114351391792297, Validation loss: 0.279390424489975
Epoch: 21/200 - Train loss: 0.21985356509685516, Validation loss: 0.27770325541496277
Epoch: 22/200 - Train loss: 0.21999205648899078, Validation loss: 0.2739497423171997
Epoch: 23/200 - Train loss: 0.21821896731853485, Validation loss: 0.27272510528564453
Epoch: 24/200 - Train loss: 0.2198333889245987, Validation loss: 0.27793678641319275
Epoch: 25/200 - Train loss: 0.21753008663654327, Validation loss: 0.27668437361717224
Epoch: 26/200 - Train loss: 0.21805930137634277, Validation loss: 0.28096523880958557
Epoch: 27/200 - Train loss: 0.21663491427898407, Validation loss: 0.27362537384033203
Epoch: 28/200 - Train loss: 0.21533790230751038, Validation loss: 0.277356892824173
Epoch: 29/200 - Train loss: 0.21487127244472504, Validation loss: 0.2760921120643616
Epoch: 30/200 - Train loss: 0.21488334238529205, Validation loss: 0.27488675713539124
Epoch: 31/200 - Train loss: 0.21426256000995636, Validation loss: 0.27660083770751953
Epoch: 32/200 - Train loss: 0.2138376235961914, Validation loss: 0.2713509202003479
Epoch: 33/200 - Train loss: 0.2133082151412964, Validation loss: 0.27631762623786926
Epoch: 34/200 - Train loss: 0.2138601690530777, Validation loss: 0.2744685411453247
Epoch: 35/200 - Train loss: 0.21290437877178192, Validation loss: 0.26963338255882263
Epoch: 36/200 - Train loss: 0.21238374710083008, Validation loss: 0.27224719524383545
Epoch: 37/200 - Train loss: 0.212918221950531, Validation loss: 0.279441773891449
Epoch: 38/200 - Train loss: 0.2117435336112976, Validation loss: 0.27256593108177185
Epoch: 39/200 - Train loss: 0.21190398931503296, Validation loss: 0.27472880482673645
Epoch: 40/200 - Train loss: 0.21131113171577454, Validation loss: 0.2723211944103241
Epoch: 41/200 - Train loss: 0.2113579660654068, Validation loss: 0.2717607617378235
Epoch: 42/200 - Train loss: 0.21188148856163025, Validation loss: 0.2779948115348816
Epoch: 43/200 - Train loss: 0.2115614414215088, Validation loss: 0.27623921632766724
Epoch: 44/200 - Train loss: 0.21060645580291748, Validation loss: 0.2745790481567383
Epoch: 45/200 - Train loss: 0.2101801037788391, Validation loss: 0.28018197417259216
Epoch: 46/200 - Train loss: 0.2093094438314438, Validation loss: 0.271764874458313
Epoch: 47/200 - Train loss: 0.20964522659778595, Validation loss: 0.27257978916168213
Epoch: 48/200 - Train loss: 0.2098010629415512, Validation loss: 0.27239927649497986
Epoch: 49/200 - Train loss: 0.20886945724487305, Validation loss: 0.27314427495002747
Epoch: 50/200 - Train loss: 0.20936143398284912, Validation loss: 0.2769087255001068
Epoch: 51/200 - Train loss: 0.20900043845176697, Validation loss: 0.28145715594291687
Epoch: 52/200 - Train loss: 0.2093111127614975, Validation loss: 0.27434393763542175
Epoch: 53/200 - Train loss: 0.20906463265419006, Validation loss: 0.27577102184295654
Epoch: 54/200 - Train loss: 0.20859916508197784, Validation loss: 0.28235000371932983
Epoch: 55/200 - Train loss: 0.20877303183078766, Validation loss: 0.270998477935791
Epoch: 56/200 - Train loss: 0.2087876796722412, Validation loss: 0.2767305374145508
Epoch: 57/200 - Train loss: 0.20829997956752777, Validation loss: 0.2711256742477417
Epoch: 58/200 - Train loss: 0.207597017288208, Validation loss: 0.27341973781585693
Epoch: 59/200 - Train loss: 0.20867495238780975, Validation loss: 0.2821124196052551
Epoch: 60/200 - Train loss: 0.20852699875831604, Validation loss: 0.2767360806465149
Epoch: 61/200 - Train loss: 0.2082725465297699, Validation loss: 0.27393263578414917
Epoch: 62/200 - Train loss: 0.20786212384700775, Validation loss: 0.2787177860736847
Epoch: 63/200 - Train loss: 0.20810651779174805, Validation loss: 0.2814243733882904
Epoch: 64/200 - Train loss: 0.20783813297748566, Validation loss: 0.27467402815818787
Epoch: 65/200 - Train loss: 0.20695871114730835, Validation loss: 0.27729663252830505
Epoch: 66/200 - Train loss: 0.20678305625915527, Validation loss: 0.2742670178413391
Epoch: 67/200 - Train loss: 0.2063468098640442, Validation loss: 0.27200478315353394
Epoch: 68/200 - Train loss: 0.2085450440645218, Validation loss: 0.27316394448280334
Epoch: 69/200 - Train loss: 0.20698338747024536, Validation loss: 0.27397218346595764
Epoch: 70/200 - Train loss: 0.20697854459285736, Validation loss: 0.28128063678741455
Epoch: 71/200 - Train loss: 0.2074730545282364, Validation loss: 0.27393412590026855
Epoch: 72/200 - Train loss: 0.20671677589416504, Validation loss: 0.2774136960506439
Epoch: 73/200 - Train loss: 0.20594295859336853, Validation loss: 0.27390095591545105
Epoch: 74/200 - Train loss: 0.20671558380126953, Validation loss: 0.2804475724697113
Epoch: 75/200 - Train loss: 0.20579397678375244, Validation loss: 0.27332594990730286
Epoch: 76/200 - Train loss: 0.20616485178470612, Validation loss: 0.2747277021408081
Epoch: 77/200 - Train loss: 0.20613835752010345, Validation loss: 0.27170661091804504
Epoch: 78/200 - Train loss: 0.2060411423444748, Validation loss: 0.27901527285575867
Epoch: 79/200 - Train loss: 0.20621301233768463, Validation loss: 0.27260303497314453
Epoch: 80/200 - Train loss: 0.20622511208057404, Validation loss: 0.27363625168800354
Epoch: 81/200 - Train loss: 0.20565560460090637, Validation loss: 0.2801937460899353
Epoch: 82/200 - Train loss: 0.2064872533082962, Validation loss: 0.27161312103271484
Epoch: 83/200 - Train loss: 0.20521895587444305, Validation loss: 0.27661505341529846
Epoch: 84/200 - Train loss: 0.20626263320446014, Validation loss: 0.27621403336524963
Epoch: 85/200 - Train loss: 0.20570701360702515, Validation loss: 0.27585360407829285
Epoch: 86/200 - Train loss: 0.20530666410923004, Validation loss: 0.2756046652793884
Epoch: 87/200 - Train loss: 0.20524227619171143, Validation loss: 0.2765820324420929
Epoch: 88/200 - Train loss: 0.20637181401252747, Validation loss: 0.27737826108932495
Epoch: 89/200 - Train loss: 0.2059962898492813, Validation loss: 0.2775804102420807
Epoch: 90/200 - Train loss: 0.20549426972866058, Validation loss: 0.27588531374931335
Epoch: 91/200 - Train loss: 0.2057570070028305, Validation loss: 0.27358517050743103
Epoch: 92/200 - Train loss: 0.20532462000846863, Validation loss: 0.281450092792511
Epoch: 93/200 - Train loss: 0.20515267550945282, Validation loss: 0.27809619903564453
Epoch: 94/200 - Train loss: 0.20476803183555603, Validation loss: 0.2806571125984192
Epoch: 95/200 - Train loss: 0.2050675004720688, Validation loss: 0.2743750810623169
Epoch: 96/200 - Train loss: 0.20470722019672394, Validation loss: 0.27546626329421997
Epoch: 97/200 - Train loss: 0.20454725623130798, Validation loss: 0.28210410475730896
Epoch: 98/200 - Train loss: 0.2047645002603531, Validation loss: 0.27621975541114807
Epoch: 99/200 - Train loss: 0.20484048128128052, Validation loss: 0.2781497538089752
Epoch: 100/200 - Train loss: 0.20524491369724274, Validation loss: 0.27629372477531433
Epoch: 101/200 - Train loss: 0.2048959583044052, Validation loss: 0.2756049633026123
Epoch: 102/200 - Train loss: 0.20460109412670135, Validation loss: 0.27605926990509033
Epoch: 103/200 - Train loss: 0.2051945924758911, Validation loss: 0.2799392640590668
Epoch: 104/200 - Train loss: 0.20350615680217743, Validation loss: 0.2825086712837219
Epoch: 105/200 - Train loss: 0.20429019629955292, Validation loss: 0.2758450210094452
Epoch: 106/200 - Train loss: 0.20516259968280792, Validation loss: 0.2737405002117157
Epoch: 107/200 - Train loss: 0.20423659682273865, Validation loss: 0.27321138978004456
Epoch: 108/200 - Train loss: 0.2042669653892517, Validation loss: 0.2849784195423126
Epoch: 109/200 - Train loss: 0.2039039134979248, Validation loss: 0.2834111452102661
Epoch: 110/200 - Train loss: 0.20561830699443817, Validation loss: 0.27790698409080505
Epoch: 111/200 - Train loss: 0.20553605258464813, Validation loss: 0.2819289267063141
Epoch: 112/200 - Train loss: 0.20507347583770752, Validation loss: 0.2764139175415039
Epoch: 113/200 - Train loss: 0.20467588305473328, Validation loss: 0.2722817659378052
Epoch: 114/200 - Train loss: 0.20398329198360443, Validation loss: 0.27288496494293213
Epoch: 115/200 - Train loss: 0.20418116450309753, Validation loss: 0.27423667907714844
Epoch: 116/200 - Train loss: 0.20440731942653656, Validation loss: 0.27467772364616394
Epoch: 117/200 - Train loss: 0.20437727868556976, Validation loss: 0.2744884788990021
Epoch: 118/200 - Train loss: 0.20402278006076813, Validation loss: 0.27322423458099365
Epoch: 119/200 - Train loss: 0.2040349543094635, Validation loss: 0.2769046127796173
Epoch: 120/200 - Train loss: 0.20409993827342987, Validation loss: 0.27804893255233765
Epoch: 121/200 - Train loss: 0.20415203273296356, Validation loss: 0.274898886680603
Epoch: 122/200 - Train loss: 0.2037980854511261, Validation loss: 0.272030234336853
Epoch: 123/200 - Train loss: 0.20416174829006195, Validation loss: 0.2789468467235565
Epoch: 124/200 - Train loss: 0.20361490547657013, Validation loss: 0.28723642230033875
Epoch: 125/200 - Train loss: 0.20314191281795502, Validation loss: 0.2761029899120331
Epoch: 126/200 - Train loss: 0.20371687412261963, Validation loss: 0.2762366831302643
Epoch: 127/200 - Train loss: 0.2034311294555664, Validation loss: 0.2787337899208069
Epoch: 128/200 - Train loss: 0.2031305432319641, Validation loss: 0.2810647189617157
Epoch: 129/200 - Train loss: 0.20294883847236633, Validation loss: 0.27617427706718445
Epoch: 130/200 - Train loss: 0.2039809674024582, Validation loss: 0.27733081579208374
Epoch: 131/200 - Train loss: 0.20299118757247925, Validation loss: 0.2759489119052887
Epoch: 132/200 - Train loss: 0.20323969423770905, Validation loss: 0.2778758406639099
Epoch: 133/200 - Train loss: 0.20408640801906586, Validation loss: 0.2761847674846649
Epoch: 134/200 - Train loss: 0.20326119661331177, Validation loss: 0.27946388721466064
Epoch: 135/200 - Train loss: 0.20346549153327942, Validation loss: 0.2760332226753235
Epoch: 136/200 - Train loss: 0.20307298004627228, Validation loss: 0.27881360054016113
Epoch: 137/200 - Train loss: 0.20407825708389282, Validation loss: 0.27627453207969666
Epoch: 138/200 - Train loss: 0.2030544877052307, Validation loss: 0.27632957696914673
