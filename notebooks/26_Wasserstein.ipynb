{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB 26. Measuring distances among slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from NegativeClassOptimization import utils, config, preprocessing, visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/sources/eugen/negative-class-optimization/src/NegativeClassOptimization/NegativeClassOptimization/preprocessing.py:284: UserWarning: Not scaling X.\n",
      "  warnings.warn(\"Not scaling X.\")\n"
     ]
    }
   ],
   "source": [
    "df_p_train = pd.read_csv(\"../data/MiniAbsolut/3VRL/high_train_15000.tsv\", sep='\\t')\n",
    "df_p_test = pd.read_csv(\"../data/MiniAbsolut/3VRL/high_test_5000.tsv\", sep='\\t')\n",
    "\n",
    "df_n_train = pd.read_csv(\"../data/MiniAbsolut/1NSN/high_train_15000.tsv\", sep='\\t')\n",
    "df_n_test = pd.read_csv(\"../data/MiniAbsolut/1NSN/high_test_5000.tsv\", sep='\\t')\n",
    "\n",
    "df_train = pd.concat([df_p_train, df_n_train])\n",
    "df_test = pd.concat([df_p_test, df_n_test])\n",
    "\n",
    "(train_data, test_data, train_loader, test_loader) = preprocessing.preprocess_data_for_pytorch_binary(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    ag_pos=[\"3VRL\"],\n",
    "    scale_X=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_loader.dataset.df[\"X\"].values\n",
    "X_train = np.stack(X_train)\n",
    "y_train = train_loader.dataset.df[\"y\"].values\n",
    "\n",
    "X_test = test_loader.dataset.df[\"X\"].values\n",
    "X_test = np.stack(X_test)\n",
    "y_test = test_loader.dataset.df[\"y\"].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geomloss\n",
    "\n",
    "loss = geomloss.SamplesLoss(\"sinkhorn\", p=2, blur=0.05, scaling=0.9)\n",
    "# Blur for sinkhorn: The default value of .05 is sensible for input measures that lie in the unit square/cube.\n",
    "# scaling (float, default=.5) – If loss is \"sinkhorn\", specifies the ratio between successive values of in the -scaling descent. This parameter allows you to specify the trade-off between speed (scaling < .4) and accuracy (scaling > .9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand((\u001b[39m5000\u001b[39m, \u001b[39m220\u001b[39m))\n\u001b[1;32m      2\u001b[0m Y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand((\u001b[39m5000\u001b[39m, \u001b[39m220\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m loss(X, Y)\n",
      "File \u001b[0;32m~/miniconda3/envs/nco/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/nco/lib/python3.8/site-packages/geomloss/samples_loss.py:265\u001b[0m, in \u001b[0;36mSamplesLoss.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    262\u001b[0m     α, x, β, y \u001b[39m=\u001b[39m α\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), x\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), β\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), y\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    264\u001b[0m \u001b[39m# Run --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m values \u001b[39m=\u001b[39m routines[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss][backend](\n\u001b[1;32m    266\u001b[0m     α,\n\u001b[1;32m    267\u001b[0m     x,\n\u001b[1;32m    268\u001b[0m     β,\n\u001b[1;32m    269\u001b[0m     y,\n\u001b[1;32m    270\u001b[0m     p\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp,\n\u001b[1;32m    271\u001b[0m     blur\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblur,\n\u001b[1;32m    272\u001b[0m     reach\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreach,\n\u001b[1;32m    273\u001b[0m     diameter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdiameter,\n\u001b[1;32m    274\u001b[0m     scaling\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaling,\n\u001b[1;32m    275\u001b[0m     truncate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtruncate,\n\u001b[1;32m    276\u001b[0m     cost\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcost,\n\u001b[1;32m    277\u001b[0m     kernel\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel,\n\u001b[1;32m    278\u001b[0m     cluster_scale\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcluster_scale,\n\u001b[1;32m    279\u001b[0m     debias\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdebias,\n\u001b[1;32m    280\u001b[0m     potentials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpotentials,\n\u001b[1;32m    281\u001b[0m     labels_x\u001b[39m=\u001b[39;49ml_x,\n\u001b[1;32m    282\u001b[0m     labels_y\u001b[39m=\u001b[39;49ml_y,\n\u001b[1;32m    283\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    284\u001b[0m )\n\u001b[1;32m    286\u001b[0m \u001b[39m# Make sure that the output has the correct shape ------------------------------------\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    288\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpotentials\n\u001b[1;32m    289\u001b[0m ):  \u001b[39m# Return some dual potentials (= test functions) sampled on the input measures\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nco/lib/python3.8/site-packages/geomloss/sinkhorn_samples.py:196\u001b[0m, in \u001b[0;36msinkhorn_tensorized\u001b[0;34m(a, x, b, y, p, blur, reach, diameter, scaling, cost, debias, potentials, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m diameter, eps, eps_list, rho \u001b[39m=\u001b[39m scaling_parameters(\n\u001b[1;32m    192\u001b[0m     x, y, p, blur, reach, diameter, scaling\n\u001b[1;32m    193\u001b[0m )\n\u001b[1;32m    195\u001b[0m \u001b[39m# Use an optimal transport solver to retrieve the dual potentials:\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m f_aa, g_bb, g_ab, f_ba \u001b[39m=\u001b[39m sinkhorn_loop(\n\u001b[1;32m    197\u001b[0m     softmin_tensorized,\n\u001b[1;32m    198\u001b[0m     log_weights(a),\n\u001b[1;32m    199\u001b[0m     log_weights(b),\n\u001b[1;32m    200\u001b[0m     C_xx,\n\u001b[1;32m    201\u001b[0m     C_yy,\n\u001b[1;32m    202\u001b[0m     C_xy,\n\u001b[1;32m    203\u001b[0m     C_yx,\n\u001b[1;32m    204\u001b[0m     eps_list,\n\u001b[1;32m    205\u001b[0m     rho,\n\u001b[1;32m    206\u001b[0m     debias\u001b[39m=\u001b[39;49mdebias,\n\u001b[1;32m    207\u001b[0m )\n\u001b[1;32m    209\u001b[0m \u001b[39m# Optimal transport cost:\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39mreturn\u001b[39;00m sinkhorn_cost(\n\u001b[1;32m    211\u001b[0m     eps,\n\u001b[1;32m    212\u001b[0m     rho,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m     potentials\u001b[39m=\u001b[39mpotentials,\n\u001b[1;32m    222\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nco/lib/python3.8/site-packages/geomloss/sinkhorn_divergence.py:489\u001b[0m, in \u001b[0;36msinkhorn_loop\u001b[0;34m(softmin, a_logs, b_logs, C_xxs, C_yys, C_xys, C_yxs, eps_list, rho, jumps, kernel_truncation, truncate, cost, extrapolate, debias, last_extrapolation)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39m# See Fig. 3.21 in Jean Feydy's PhD thesis to see the importance\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[39m# of debiasing when the target \"blur\" or \"eps**(1/p)\" value is larger\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39m# than the average distance between samples x_i, y_j and their neighbours.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m debias:\n\u001b[0;32m--> 489\u001b[0m     ft_aa \u001b[39m=\u001b[39m damping \u001b[39m*\u001b[39m softmin(eps, C_xx, a_log \u001b[39m+\u001b[39;49m f_aa \u001b[39m/\u001b[39;49m eps)  \u001b[39m# a -> a\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     gt_bb \u001b[39m=\u001b[39m damping \u001b[39m*\u001b[39m softmin(eps, C_yy, b_log \u001b[39m+\u001b[39m g_bb \u001b[39m/\u001b[39m eps)  \u001b[39m# b -> b\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[39m# Symmetrized updates - see Fig. 3.24.b in Jean Feydy's PhD thesis:\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nco/lib/python3.8/site-packages/geomloss/sinkhorn_samples.py:72\u001b[0m, in \u001b[0;36msoftmin_tensorized\u001b[0;34m(eps, C_xy, h_y)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Soft-C-transform, implemented using dense torch Tensors.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[39mThis routine implements the (soft-)C-transform\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m        by the points :math:`x_i`.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m B \u001b[39m=\u001b[39m C_xy\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39meps \u001b[39m*\u001b[39m (h_y\u001b[39m.\u001b[39;49mview(B, \u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m-\u001b[39;49m C_xy \u001b[39m/\u001b[39;49m eps)\u001b[39m.\u001b[39;49mlogsumexp(\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39mview(B, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = torch.rand((5000, 220))\n",
    "Y = torch.rand((5000, 220))\n",
    "\n",
    "loss(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ec5334dbdc4a6f7f47854c251e8d2556e95e85daa09db51a6f2bda295b96836"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
