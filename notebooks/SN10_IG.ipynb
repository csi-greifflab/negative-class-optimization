{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SN10 classifier and integrated gradients attribution\n",
    "\n",
    "In this notebook, we develop the SN10 classifier used in `Absolut!` and the integrated-gradients method of attribution. We might also check other attribution methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugen/miniconda3/envs/ab-negative-training/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import NegativeClassOptimization.config as config\n",
    "import NegativeClassOptimization.preprocessing as preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data on which we are going to develop the binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_slide_Variant</th>\n",
       "      <th>CDR3</th>\n",
       "      <th>Best</th>\n",
       "      <th>Slide</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Structure</th>\n",
       "      <th>UID</th>\n",
       "      <th>Antigen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1873658_06a</td>\n",
       "      <td>CARPENLLLLLWYFDVW</td>\n",
       "      <td>True</td>\n",
       "      <td>LLLLLWYFDVW</td>\n",
       "      <td>-112.82</td>\n",
       "      <td>137442-BRDSLLUDLS</td>\n",
       "      <td>3VRL_1873658_06a</td>\n",
       "      <td>3VRL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7116990_04a</td>\n",
       "      <td>CARGLLLLLWYFDVW</td>\n",
       "      <td>True</td>\n",
       "      <td>LLLLLWYFDVW</td>\n",
       "      <td>-112.82</td>\n",
       "      <td>137442-BRDSLLUDLS</td>\n",
       "      <td>3VRL_7116990_04a</td>\n",
       "      <td>3VRL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_slide_Variant               CDR3  Best        Slide  Energy  \\\n",
       "0      1873658_06a  CARPENLLLLLWYFDVW  True  LLLLLWYFDVW -112.82   \n",
       "1      7116990_04a    CARGLLLLLWYFDVW  True  LLLLLWYFDVW -112.82   \n",
       "\n",
       "           Structure               UID Antigen  \n",
       "0  137442-BRDSLLUDLS  3VRL_1873658_06a    3VRL  \n",
       "1  137442-BRDSLLUDLS  3VRL_7116990_04a    3VRL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(config.DATA_SLACK_1_GLOBAL, sep='\\t')\n",
    "\n",
    "ag_pos = \"3VRL\"\n",
    "ag_neg = \"1ADQ\"\n",
    "df = df.loc[df[\"Antigen\"].isin([ag_pos, ag_neg])].copy()\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, handle duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Slide</th>\n",
       "      <th>Antigen</th>\n",
       "      <th>Slide_onehot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAELFWYFDVW</td>\n",
       "      <td>3VRL</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAFITTVGWYF</td>\n",
       "      <td>1ADQ</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAFYGRWYFDV</td>\n",
       "      <td>1ADQ</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Slide Antigen                                       Slide_onehot\n",
       "0  AAELFWYFDVW    3VRL  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1  AAFITTVGWYF    1ADQ  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2  AAFYGRWYFDV    1ADQ  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_data(df: pd.DataFrame, pos_ag: str) -> pd.DataFrame:\n",
    "    \"\"\"Prepare data for SN10 training and evaluation. \n",
    "    Most importantly - appropriately removes duplicates. \n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): typical dataframe used in the project\n",
    "        pos_ag (str): the antigen assuming the positive dataset role\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with new columns suitable for modelling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def infer_antigen_from_duplicate_list(antigens: List[str], pos_antigen: str):\n",
    "        assert len(antigens) <= 2, \">2 antigens not supported yet.\"\n",
    "        if len(antigens) == 1:\n",
    "            return antigens[0]\n",
    "        else:\n",
    "            if pos_antigen in antigens:\n",
    "                return pos_antigen\n",
    "            else:\n",
    "                return list(set(antigens) - set([pos_antigen]))[0]\n",
    "\n",
    "    df = df.groupby(\"Slide\").apply(\n",
    "        lambda df_: infer_antigen_from_duplicate_list(df_[\"Antigen\"].unique().tolist(), pos_antigen=ag_pos)\n",
    "    )\n",
    "    df = pd.DataFrame(data=df, columns=[\"Antigen\"])\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "df = prepare_data(df, ag_pos)\n",
    "\n",
    "preprocessing.onehot_encode_df(df);\n",
    "\n",
    "df[\"X\"] = df[\"Slide_onehot\"]\n",
    "df[\"y\"] = np.where(df[\"Antigen\"] == ag_pos, 1, 0)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.df.loc[idx, \"X\"]),\n",
    "            torch.tensor(self.df.loc[idx, \"y\"]), \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_frac = 0.8\n",
    "df = df.sample(frac=1).reset_index(drop=True)  # shuffle\n",
    "\n",
    "split_idx = int(df.shape[0] * train_frac)\n",
    "df_train = df.loc[:split_idx].copy()\n",
    "df_test = df.loc[split_idx:].copy()\n",
    "\n",
    "train_data = PairwiseDataset(df_train)\n",
    "test_data = PairwiseDataset(df_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/data/sources/eugen/negative-class-optimization/notebooks/SN10_IG.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.40.3.22/data/sources/eugen/negative-class-optimization/notebooks/SN10_IG.ipynb#ch0000024vscode-remote?line=0'>1</a>\u001b[0m model(train_data[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniconda3/envs/ab-negative-training/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/data/sources/eugen/negative-class-optimization/notebooks/SN10_IG.ipynb Cell 8\u001b[0m in \u001b[0;36mSN10.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.40.3.22/data/sources/eugen/negative-class-optimization/notebooks/SN10_IG.ipynb#ch0000024vscode-remote?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.40.3.22/data/sources/eugen/negative-class-optimization/notebooks/SN10_IG.ipynb#ch0000024vscode-remote?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.40.3.22/data/sources/eugen/negative-class-optimization/notebooks/SN10_IG.ipynb#ch0000024vscode-remote?line=13'>14</a>\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_relu_stack(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.40.3.22/data/sources/eugen/negative-class-optimization/notebooks/SN10_IG.ipynb#ch0000024vscode-remote?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/miniconda3/envs/ab-negative-training/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ab-negative-training/lib/python3.8/site-packages/torch/nn/modules/flatten.py:42\u001b[0m, in \u001b[0;36mFlatten.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mflatten(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mend_dim)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "model(train_data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "SN10(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=220, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SN10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SN10, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(11*20, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "model = SN10().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(loader, model, loss_fn, optimizer):\n",
    "    size = len(loader.dataset)\n",
    "    for batch, (X, y) in enumerate(loader):\n",
    "        print(X)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(loader, model, loss_fn):\n",
    "    size = len(loader.dataset)\n",
    "    num_batches = len(loader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            y_pred = model(X)\n",
    "            test_loss += loss_fn(y_pred, y).item()\n",
    "            correct += (round(y_pred) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ab-negative-training')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a66fbe740016efe90d6139b67c84a560168f2532ea3e3d46e49051f459208991"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
