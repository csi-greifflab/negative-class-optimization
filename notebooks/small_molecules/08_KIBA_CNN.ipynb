{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'captum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mNegativeClassOptimization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ml\n",
      "File \u001b[0;32m~/Music/nco/negative-class-optimization/src/NegativeClassOptimization/NegativeClassOptimization/ml.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepLift, IntegratedGradients\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rankdata\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# from sklearn.preprocessing import StandardScaler\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# from sklearn.preprocessing import LabelEncoder\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'captum'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from NegativeClassOptimization import ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv1_num_filters=5,\n",
    "        conv1_filter_size=3,\n",
    "        conv2_num_filters=3,\n",
    "        conv2_filter_size=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ConvNet Calculator\n",
    "        # https://madebyollin.github.io/convnet-calculator/\n",
    "\n",
    "        # input: 67(W) x 100(H) x 1(#C)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=conv1_num_filters,  # filter count\n",
    "            kernel_size=conv1_filter_size,  # filter size\n",
    "        )\n",
    "        conv1_out_w = math.floor((67 - conv1_filter_size) / 1 + 1)\n",
    "        conv1_out_h = math.floor((100 - conv1_filter_size) / 1 + 1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(\n",
    "            kernel_size=2,  # filter size\n",
    "            stride=2,\n",
    "        )\n",
    "        pool1_out_w = math.floor((conv1_out_w - 2) / 2 + 1)\n",
    "        pool1_out_h = math.floor((conv1_out_h - 2) / 2 + 1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=conv1_num_filters,\n",
    "            out_channels=conv2_num_filters,  # filter count\n",
    "            kernel_size=conv2_filter_size,\n",
    "        )\n",
    "        conv2_out_w = math.floor((pool1_out_w - conv2_filter_size) / 1 + 1)\n",
    "        conv2_out_h = math.floor((pool1_out_h - conv2_filter_size) / 1 + 1)\n",
    "\n",
    "        pool2_out_w = math.floor((conv2_out_w - 2) / 2 + 1)\n",
    "        pool2_out_h = math.floor((conv2_out_h - 2) / 2 + 1)\n",
    "        fc1_in_features = pool2_out_w * pool2_out_h * conv2_num_filters\n",
    "        self.fc1 = nn.Linear(fc1_in_features, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, forward_logits=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(F.relu(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(F.relu(x))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        if forward_logits:\n",
    "            return x\n",
    "        else:\n",
    "            x = self.sigmoid(x)\n",
    "            return x\n",
    "\n",
    "    def forward_logits(self, x):\n",
    "        return self.forward(x, forward_logits=True)\n",
    "\n",
    "    def compute_metrics_closed_testset(self, x_test, y_test):\n",
    "        x_test_cnn = x_test.reshape((-1, 1, 67, 100))\n",
    "        return ml.SN10.compute_metrics_closed_testset_static(self, x_test_cnn, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_optimizer(\n",
    "    optimizer_type,\n",
    "    learning_rate,\n",
    "    momentum,\n",
    "    weight_decay,\n",
    "    model,\n",
    ") -> torch.optim.Optimizer:\n",
    "    if optimizer_type == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            momentum=momentum,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "    elif optimizer_type == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(momentum, 0.999),  # beta1 ~ momentum\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"optimizer_type `{optimizer_type}` not recognized.\")\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(loader, model, loss_fn, optimizer):\n",
    "    \"\"\"Basic training loop for pytorch.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader)\n",
    "        model (nn.Model)\n",
    "        loss_fn (Callable)\n",
    "        optimizer\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    size = len(loader.dataset)\n",
    "    for batch, (X, y) in enumerate(loader):\n",
    "        X_pred = model(X.reshape(-1, 1, 67, 100))\n",
    "        loss = loss_fn(X_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            losses.append(loss)\n",
    "    return losses\n",
    "\n",
    "def train_for_ndb1(\n",
    "    epochs,\n",
    "    learning_rate,\n",
    "    train_loader,\n",
    "    model,\n",
    "    optimizer_type: str,\n",
    "    momentum: float = 0,\n",
    "    weight_decay: float = 0,\n",
    "    callback_on_model_end_epoch: callable = None,\n",
    "):\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = construct_optimizer(\n",
    "        optimizer_type, learning_rate, momentum, weight_decay, model\n",
    "    )\n",
    "\n",
    "\n",
    "    if callback_on_model_end_epoch is None:\n",
    "        callback_on_model_end_epoch = lambda x, t: None\n",
    "\n",
    "    online_metrics_per_epoch = []\n",
    "    for t in range(epochs):\n",
    "        losses = train_loop(train_loader, model, loss_fn, optimizer)\n",
    "        online_metrics_per_epoch.append(\n",
    "            {\n",
    "                \"train_losses\": losses,}\n",
    "        )\n",
    "\n",
    "        callback_on_model_end_epoch(model, t)\n",
    "\n",
    "    return online_metrics_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMISET = {\"C\": 67, \"l\": 1, \".\": 2, \"c\": 3, \"1\": 4, \"2\": 5, \"(\": 6,\n",
    "          \"N\": 7, \"=\": 8, \"3\": 9, \")\": 10, \"n\": 11, \"[\": 12, \"H\": 13,\n",
    "           \"]\": 14, \"O\": 15, \"@\": 16, \"s\": 17, \"+\": 18, \"/\": 19, \"S\": 20,\n",
    "            \"F\": 21, \"-\": 22, \"4\": 23, \"B\": 24, \"r\": 25, \"o\": 26, \"\\\\\": 27,\n",
    "             \"#\": 28, \"5\": 29, \"a\": 30, \"P\": 31, \"e\": 32, \"6\": 33, \"7\": 34,\n",
    "              \"I\": 35, \"A\": 36, \"i\": 37, \"8\": 38, \"9\": 39, \"Z\": 40, \"K\": 41,\n",
    "               \"L\": 42, \"%\": 43, \"0\": 44, \"T\": 45, \"g\": 46, \"G\": 47, \"d\": 48,\n",
    "                \"M\": 49, \"b\": 50, \"u\": 51, \"t\": 52, \"R\": 53, \"p\": 54, \"m\": 55,\n",
    "                 \"W\": 56, \"Y\": 57, \"V\": 58, \"~\": 59, \"U\": 60, \"E\": 61, \"f\": 62,\n",
    "                  \"X\": 63, \"D\": 64, \"y\": 65, \"h\": 66}\n",
    "\n",
    "def one_hot_matrix_smiles(line, MAX_SMI_LEN=100):\n",
    "    \"\"\"\n",
    "    Converts a SMILES string into a one-hot matrix 67*100. If smile is shorter than MAX_SMI_LEN, it is padded with zeros.\n",
    "    \"\"\"\n",
    "    X = np.zeros((MAX_SMI_LEN, len(SMISET)))  # +1\n",
    "\n",
    "    if type(line)!=str:\n",
    "        print('SMILE format is not str!')\n",
    "    for i, ch in enumerate(line[:MAX_SMI_LEN]):\n",
    "        tmp=SMISET.get(ch)\n",
    "        if tmp:\n",
    "            X[i, tmp - 1] = 1\n",
    "        else:\n",
    "            print(line,'exits not in SMISET character',ch)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_data(df, X_col='X', y_col='Y_binary'):\n",
    "\n",
    "    X = np.array(df[X_col].tolist())\n",
    "    y = np.array(df[y_col].tolist())\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try with one dataset\n",
    "target = \"P06239\"\n",
    "task = \"vs_Weak\"\n",
    "split = 0\n",
    "\n",
    "path_to_target = Path('./data/processed') / target\n",
    "path_vs_task_split = path_to_target / task / f'split_{split}'\n",
    "# Loading data\n",
    "path_to_train = path_vs_task_split / 'train.pkl'\n",
    "path_to_test = path_vs_task_split / 'test.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(path_to_train)\n",
    "df_test = pd.read_pickle(path_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"X_matrix\"] = df_train.Drug.apply(one_hot_matrix_smiles)\n",
    "df_test[\"X_matrix\"] = df_test.Drug.apply(one_hot_matrix_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_custom_data(df_train, X_col=\"X_matrix\")\n",
    "dataset_test = load_custom_data(df_test, X_col=\"X_matrix\")\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =20\n",
    "learning_rate = 0.001\n",
    "optimizer_type = \"Adam\"\n",
    "momentum = 0.9\n",
    "model = CNN()\n",
    "\n",
    "train_output = train_for_ndb1(\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        train_loader=train_loader,\n",
    "        model=model,\n",
    "        optimizer_type=optimizer_type,\n",
    "        momentum=momentum,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_losses': [0.6677110195159912]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(loader, model, loss_fn) -> dict:\n",
    "    \"\"\"Basic test loop for pytorch.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader)\n",
    "        model (nn.Model)\n",
    "        loss_fn (Callable)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    test_loss = compute_avg_test_loss(loader, model, loss_fn)\n",
    "\n",
    "    loop_metrics = {\n",
    "        \"test_loss\": test_loss,\n",
    "    }\n",
    "\n",
    "    x_test, y_test = Xy_from_loader(loader=loader)\n",
    "    closed_metrics: dict = compute_metrics_closed_testset(model, x_test, y_test)\n",
    "\n",
    "    acc_closed = closed_metrics.get(\"acc_closed\", np.nan)\n",
    "    #print(f\"Test Error: \\n Acc: {100*acc_closed:.1f} Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return {\n",
    "        **loop_metrics,\n",
    "        **closed_metrics,\n",
    "    }\n",
    "\n",
    "def compute_avg_test_loss(loader, model, loss_fn):\n",
    "    num_batches = len(loader)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            test_loss += compute_loss(model, loss_fn, X, y).item()\n",
    "    test_loss /= num_batches\n",
    "    return test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
