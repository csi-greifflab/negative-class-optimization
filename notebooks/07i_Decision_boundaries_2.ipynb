{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision boundaries usign Supervised decision boundary maps (SDBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugen/miniconda3/envs/nco/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/home/eugen/miniconda3/envs/nco/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from sklearn.utils.extmath import cartesian\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from NegativeClassOptimization import decision_boundaries\n",
    "from NegativeClassOptimization import utils\n",
    "from NegativeClassOptimization import preprocessing\n",
    "from NegativeClassOptimization import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_from_task(task):\n",
    "    if type(task.model) == torch.optim.swa_utils.AveragedModel:\n",
    "    # Unwrap the SWA model. We need a module class,\n",
    "    # that has updated weights, but still has other\n",
    "    # module funcs, such as forward_logits.\n",
    "    # Note: swa_model.module has same weights as swa_model.state_dict().\n",
    "        return task.model.module\n",
    "    else:\n",
    "        return task.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = datasets.FrozenMiniAbsolutMLLoader(\n",
    "    data_dir=Path(\"../data/Frozen_MiniAbsolut_ML/\")\n",
    ")\n",
    "\n",
    "task = datasets.ClassificationTask(\n",
    "        task_type=datasets.ClassificationTaskType.HIGH_VS_LOOSER,\n",
    "        ag_pos=\"5E94\",\n",
    "        ag_neg=\"auto\",\n",
    "        seed_id=0,\n",
    "        split_id=42,\n",
    "    )\n",
    "            \n",
    "task = loader.load(task, attributions_toload=\"v2.0-2\", load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.test_dataset = preprocessing.onehot_encode_df(task.test_dataset)\n",
    "task.test_dataset[\"X\"] = task.test_dataset[\"Slide_onehot\"]\n",
    "\n",
    "X = np.stack(task.test_dataset[\"X\"])\n",
    "y = task.test_dataset[\"y\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 15:41:07.553236: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-03-21 15:41:07.553450: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 15:41:07.554899: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Based on experiment_blobs.py\n",
    "\n",
    "output_dir = Path(\"07i_decision_boundaries\")\n",
    "dataset_name = \"test_1adq\"\n",
    "epochs = 30  # 200\n",
    "patience = 5\n",
    "verbose = False\n",
    "\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "X_ssnpgt_proj_file = f'X_SSNP_{dataset_name}.npy'\n",
    "name_projector_ssnp = f\"{dataset_name}_ssnp\"\n",
    "\n",
    "ssnpgt = decision_boundaries.SSNP(\n",
    "    epochs=epochs, \n",
    "    verbose=verbose, \n",
    "    patience=patience, \n",
    "    opt='adam', \n",
    "    bottleneck_activation='linear'\n",
    ")\n",
    "\n",
    "if (output_dir / name_projector_ssnp).exists():\n",
    "    ssnpgt.load_model(output_dir / name_projector_ssnp)\n",
    "else: #otherwise it will be fitted\n",
    "    ssnpgt.fit(X, y)\n",
    "    ssnpgt.save_model(output_dir / name_projector_ssnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected SSNP points found! 07i_decision_boundaries/X_SSNP_test_1adq.npy\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(output_dir, X_ssnpgt_proj_file)):\n",
    "    print(\n",
    "        f\"Projected SSNP points found! {os.path.join(output_dir,X_ssnpgt_proj_file)}\"\n",
    "    )\n",
    "    X_ssnpgt = np.load(os.path.join(output_dir, X_ssnpgt_proj_file))\n",
    "else:\n",
    "    print(\"Projected SSNP points not found! Transforming...\")\n",
    "    X_ssnpgt = ssnpgt.transform(X)\n",
    "    np.save(os.path.join(output_dir, X_ssnpgt_proj_file), X_ssnpgt)\n",
    "    print(f\"Projected points ({dataset_name}) saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vanilla. 300x300 - test_1adq - SN10\n",
      "Saving alpha. 300x300 - test_1adq - SN10\n",
      "Saving hsv. 300x300 - test_1adq - SN10\n"
     ]
    }
   ],
   "source": [
    "clf = get_model_from_task(task)\n",
    "clf_name = \"SN10\"\n",
    "grid_size = 300\n",
    "\n",
    "\n",
    "ssnp_done = False\n",
    "out_name = f\"{clf_name}_{grid_size}x{grid_size}_{dataset_name}\"\n",
    "out_file = os.path.join(output_dir, out_name + \"_ssnp.npy\")\n",
    "\n",
    "if os.path.exists(out_file):\n",
    "    img_grid_ssnp = np.load(\n",
    "        os.path.join(output_dir, out_name + \"_ssnp.npy\")\n",
    "    )\n",
    "    prob_grid_ssnp = np.load(\n",
    "        os.path.join(output_dir, out_name + \"_ssnp_prob\" + \".npy\")\n",
    "    )\n",
    "    prob_grid_ssnp = prob_grid_ssnp.clip(max=0.8)\n",
    "\n",
    "    # Background mode\n",
    "    normalized = None\n",
    "    suffix = \"ssnp_background\"\n",
    "\n",
    "    decision_boundaries.results_to_png(\n",
    "        np_matrix=img_grid_ssnp,\n",
    "        prob_matrix=prob_grid_ssnp,\n",
    "        grid_size=grid_size,\n",
    "        n_classes=n_classes,\n",
    "        real_points=normalized,\n",
    "        max_value_hsv=0.8,\n",
    "        dataset_name=dataset_name,\n",
    "        classifier_name=clf_name,\n",
    "        suffix=suffix,\n",
    "        output_dir=output_dir,\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Defining grid around projected 2D points.\")\n",
    "    xmin_ssnp = np.min(X_ssnpgt[:, 0])\n",
    "    xmax_ssnp = np.max(X_ssnpgt[:, 0])\n",
    "    ymin_ssnp = np.min(X_ssnpgt[:, 1])\n",
    "    ymax_ssnp = np.max(X_ssnpgt[:, 1])\n",
    "\n",
    "    x_intrvls_ssnp = np.linspace(xmin_ssnp, xmax_ssnp, num=grid_size)\n",
    "    y_intrvls_ssnp = np.linspace(ymin_ssnp, ymax_ssnp, num=grid_size)\n",
    "\n",
    "    x_grid = np.linspace(0, grid_size - 1, num=grid_size)\n",
    "    y_grid = np.linspace(0, grid_size - 1, num=grid_size)\n",
    "\n",
    "    pts_ssnp = cartesian((x_intrvls_ssnp, y_intrvls_ssnp))\n",
    "    pts_grid = cartesian((x_grid, y_grid))\n",
    "    pts_grid = pts_grid.astype(int)\n",
    "\n",
    "    batch_size = min(grid_size**2, 10000)\n",
    "\n",
    "    # Can probably be moved lower, here not used\n",
    "    img_grid_ssnp = np.zeros((grid_size, grid_size))\n",
    "    prob_grid_ssnp = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    pbar = tqdm.tqdm(total=len(pts_ssnp))\n",
    "    position = 0\n",
    "\n",
    "    # Iterate over all points in the 2D-grid \n",
    "    while True:\n",
    "        if position >= len(pts_ssnp):\n",
    "            break\n",
    "\n",
    "        pts_batch_ssnp = pts_ssnp[position : position + batch_size]\n",
    "        image_batch_ssnp = ssnpgt.inverse_transform(pts_batch_ssnp)\n",
    "\n",
    "        probs_ssnp = clf.predict(torch.tensor(image_batch_ssnp)).detach().numpy()\n",
    "        alpha_ssnp = np.amax(probs_ssnp, axis=1)\n",
    "        labels_ssnp = probs_ssnp.argmax(axis=1)\n",
    "\n",
    "        pts_grid_batch = pts_grid[position : position + batch_size]\n",
    "\n",
    "        img_grid_ssnp[\n",
    "            pts_grid_batch[:, 0],  # First column\n",
    "            pts_grid_batch[:, 1],  # Second column\n",
    "        ] = labels_ssnp\n",
    "\n",
    "        position += batch_size\n",
    "\n",
    "        prob_grid_ssnp[\n",
    "            pts_grid_batch[:, 0],  # First column\n",
    "            pts_grid_batch[:, 1],  # Second column\n",
    "        ] = alpha_ssnp\n",
    "\n",
    "        pbar.update(batch_size)\n",
    "\n",
    "    pbar.close()\n",
    "    np.save(os.path.join(output_dir, f\"{out_name}_ssnp.npy\"), img_grid_ssnp)\n",
    "    np.save(\n",
    "        os.path.join(output_dir, f\"{out_name}_ssnp_prob.npy\"), prob_grid_ssnp\n",
    "    )\n",
    "\n",
    "    prob_grid_ssnp = prob_grid_ssnp.clip(max=0.8)\n",
    "\n",
    "    # Background mode\n",
    "    normalized = None\n",
    "    suffix = \"ssnp_background\"\n",
    "\n",
    "    decision_boundaries.results_to_png(\n",
    "        np_matrix=img_grid_ssnp,\n",
    "        prob_matrix=prob_grid_ssnp,\n",
    "        grid_size=grid_size,\n",
    "        n_classes=n_classes,\n",
    "        real_points=normalized,\n",
    "        max_value_hsv=0.8,\n",
    "        dataset_name=dataset_name,\n",
    "        classifier_name=clf_name,\n",
    "        suffix=suffix,\n",
    "        output_dir=output_dir,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
